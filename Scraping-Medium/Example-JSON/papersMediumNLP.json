[
    {
        "link": "https://medium.com/@venelinvalkov/autogen-build-powerful-ai-agents-with-chatgpt-gpt-4-426cc50ef720?source=list-2eb23a991a63--------7-------0a856388a93a---------------------",
        "title": "AutoGen — Build Powerful AI Agents with ChatGPT/GPT-4",
        "subtitle": "Explore AutoGen, a Microsoft library that lets you create LLM applications with agents. These agents can communicate and help you solve complex tasks.",
        "autorName": "Venelin Valkov",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/2*OoQjeo1aWgiGKub_5QxwvA.jpeg",
        "clap": "264",
        "response": "1",
        "timeForRead": "5 min read",
        "dateCreate": "Oct 17",
        "text": [
            "We’ll begin with an introduction to AutoGen and its benefits. Then, we’ll kick off with a basic example of building a single agent for analyzing stock price trends. Afterward, we’ll delve into a more advanced demonstration, using four agents to construct a cryptocurrency indicator, drawing insights from historical prices and news.\n",
            "AutoGen is like having a bunch of smart friends who work together to get things done, and it’s made with help from top-notch researchers.\n",
            "You can install AutoGen with pip:\n",
            "Let’s add the required libraries:\n",
            "Next, you need to enter your API key for OpenAI (get yours from https://platform.openai.com/account/api-keys(opens in a new tab)):\n"
        ]
    },
    {
        "link": "https://medium.com/@pedroalvarad0/dall-e-2-the-ai-that-create-images-from-descriptions-in-natural-language-887c82195e1e?source=list-ce6aa401ab97--------18-------0c347d204c53---------------------",
        "title": "DALL·E 2: the AI that create images from descriptions in natural language",
        "subtitle": "false",
        "autorName": "Pedro Alvarado",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*Mj9ep9dXtDnTE44-Z6XCww.png",
        "clap": "55",
        "response": "1",
        "timeForRead": "3 min read",
        "dateCreate": "Apr 8, 2022",
        "text": [
            "OpenAI’s new AI system is strikingly, since it can’t only create realistic images and art, but also it can make edits to an existent image and create new variations inspired by an original image, all this from a description in natural language.\n",
            "DALL·E 2 is the DALL·E 1 successor. In January 2021, OpenAI announced DALLE·E 1, and a year later introduced DALL·E 2. Unlike its predecessor, DALL·E 2 generates more realistic and accurate images with 4x greater resolution.\n",
            "This AI model is based on a neural network. The neural network was trained with pairs of images and their corresponding descriptions, that is, the images were accurately labeled. This is important, because if you labeled an image incorrectly, and train the model with that data, the model may output an image which has nothing to do with the description. This is a limitation due to it is based on supervised learning.\n",
            "Thanks to deep learning, the model doesn’t only understand individual objects but also the relation between them. Due to this the AI can take what it learned from other images and apply all that knowledge in a new image.\n",
            "For example, you may saw a teddy bear (maybe played with one). You may have the idea of a mad scientist (maybe you saw one on television). Now, if I tell you to imagine “Teddy bears mixing sparkling chemicals as mad scientists” you could do it, because you have the knowledge of two separate ideas and mix them in a picture imagined in your mind. DALL·E 2 can also make this.\n",
            "To avoid improper use of this technology, DALL-E 2 cannot generate violent or explicit images, nor can it generate portraits that can be assimilated to real people, which is why it tends to create more generic images.\n",
            "I have been thinking about the possible uses to this AI. For example one use could be that instead of searching images that were created or taken by people, we could use images generate by this AI. The advantage is that the images will be more specific.\n",
            "Also this AI could potentially be use to create NFTs and digital art in general (I’m not going to expand on this idea because I don’t know much about NFTs and digital art, but it is a possibility).\n",
            "It was though that AI would first replace the jobs and tasks that could be automated. But surprise! Here is this AI performing tasks that we (humans) consider highly creative. Who would have thought that the work of the illustrator (creative work) would be among the first to potentially be automated?\n",
            "This new AI make me think that a wonderful future awaits AI, and that an uncertain future awaits humans. This is not an inherently bad thing. This just shows that one of the most important skills at the XXI century is to “never stop learning”.\n",
            "Thank you for reading me. See you next!\n",
            "The information of this article was taken from the OpenAI site.\n"
        ]
    },
    {
        "link": "https://medium.com/@bratanic-tomaz/competition-for-optimizing-cypher-based-rag-b4d3ad2ad3f0?source=list-2eb23a991a63--------246-------0a856388a93a---------------------",
        "title": "Competition for Optimizing Cypher-based RAG",
        "subtitle": "LLMs occasionally mess up the relationship direction in Cypher statements, but we can fix that",
        "autorName": "Tomaz Bratanic",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*SnWQP0l4Vg9577WAErbjfw.jpeg",
        "clap": "16",
        "response": "3",
        "timeForRead": "2 min read",
        "dateCreate": "Aug 14",
        "text": [
            "In the time of LLMs, it is becoming increasingly popular to implement Retrieval-Augmented Generation (RAG) applications, where you feed additional information to an LLM at query time to increase its ability to generate valid, accurate, and up-to-date answers. This new RAG paradigm is bringing in the revolution in data accessibility, as it means that anybody, even non-technical users, can now ask questions about information stored in the database without requiring them to learn a database query language.\n",
            "For example, when you want the users to be able to ask questions about the information stored in Neo4j, a graph database, you need to implement a so-called text2cypher module that takes natural language as input and produces Cypher statements as output. State-of-the-art LLMs are pretty good at generating Cypher statements. However, most of them share a common flaw: they can sometimes mess up the direction of the relationship in the generated Cypher statement, which can cause significant dissatisfaction among users.\n",
            "I believe that the direction of the relationship can be deterministically after the LLM generates a Cypher statement based on the provided schema. Therefore, I am hosting this competition to help us find the best implementation of validating and fixing relationship directions in Cypher statements as accurately and fast as possible.\n",
            "The competition has a prize pool of 2500€, and we are accepting applications until Friday, 17th September 2023, 23.59 CEST.\n",
            "The idea is to use the winner’s code and add it to LLM libraries like LangChain, LlamaIndex, and others. By applying to this competition, you are allowing me, or others, to re-use the provided code in any commercial or non-commercial application with appropriate attribution.\n",
            "Looking forward to your applications!\n"
        ]
    },
    {
        "link": "https://medium.com/@jaypozo/stream-from-firestore-query-to-firebase-storage-228ba5bc4280?source=list-9f88f190fa7--------39-------64d2b10e1db0---------------------",
        "title": "Stream OpenAI Data From Firestore Query to Firebase Storage",
        "subtitle": "false",
        "autorName": "Jay Pozo",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*Fe4iD4UcCkjdaUmgjAEUjA.jpeg",
        "clap": "4",
        "response": "1",
        "timeForRead": "5 min read",
        "dateCreate": "Dec 28, 2021",
        "text": [
            "NOTE: I begin with how Syrano uses OpenAI and prompts to add context to my code solution for saving data from Firestore to Firebase Storage. If you would rather, you can jump directly to that code.\n",
            "During the last quarter of 2021, I have been focused on building Syrano - a tool that helps inspire writing for online dating apps. Syrano uses OpenAI’s GPT-3 language model to generate text that people can use to create headlines, opening lines, or to write a full dating profile. I’ve been using NextJS, Tailwind CSS and Firebase to build Syrano (and loving it).\n",
            "This post describes a data problem I encountered, plus its solution, which streams the results of a large Firebase Firestore query to a file on Google Cloud Storage so that it could be downloaded. But first, some context.\n",
            "Syrano has been running in beta for a few weeks now, with a couple of users helping by generating headlines. How headlines in Syrano work: you provide one of your interests as input, and Syrano creates a headline. Simple. Out of the box, OpenAI’s completion API is able to provide pretty decent results for the headlines. For example, one of my favourite completions is for the input “Back to the Future”.\n",
            "Those familiar with the 1985 film will recognize the reference to the 1985 film that stars Michael J. Fox, and the most important 88 mph speed at which the Delorean time machine must go to achieve time travel. If you haven’t seen this movie yet, stop reading and please go see it. It is truly entertaining and an important piece of western pop culture.\n",
            "When Syrano spat out this headline, I cheered aloud and was giddy for the rest of the day. The more I have been working with Syrano though, the more I am learning that the quality of this completion is not as common as I would like it to be, for a product that I hope will be useful enough for people to use regularly.\n"
        ]
    },
    {
        "link": "https://medium.com/@paul.k.pallaghy/chatgpt-the-hard-part-of-agi-is-now-done-3179d31a7277?source=list-9f88f190fa7--------25-------64d2b10e1db0---------------------",
        "title": "ChatGPT: AGI by 2024, the hard part is now done",
        "subtitle": "false",
        "autorName": "Paul Pallaghy, PhD",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*vO0seLpXCosFXnSF_OSTqA.png",
        "clap": "265",
        "response": "10",
        "timeForRead": "9 min read",
        "dateCreate": "Dec 25, 2022",
        "text": [
            "ChatGPT is not yet AGI, so-called, artificial general intelligence. And it may have some fundamental limitations. But here I argue we might get early GPT-based AGI in 2023 and decent ones by 2024. Let’s talk about how that might happen.\n",
            "Here’s a few quick assumptions I make:\n",
            "Such a nascent AGI could soon aid and manage our projects and even businesses, at least in shadow mode or for short periods.\n",
            "Additionally, we will assume that present LLMs (large language models) like GPT-3 / ChatGPT do at least deliver apparent human-level understanding reasonably reliably. To engage on that debate, including whether LLM understanding is genuine or not, or even possible, visit these Medium links: 1, 2, 3, 4 and 5. For thoughts on consciousness & AI see 6.\n",
            "Otherwise, stay here to ponder all things specifically AGI.\n",
            "Natural language understanding (NLU) of any sort — real or apparent or hallucinatory — is, as I will argue, the critical first step towards AGI that we’ve been waiting for.\n",
            "ChatGPT and the underlying GPT-3 is a sufficient successful starting point to serve as the basis for AGI. NLU, including language generation, in the context of the user’s input and much of human common sense and typical human world knowledge has been, until LLMs, completely elusory.\n",
            "Getting the generation of grammar right has been enough of a challenge, let alone understanding…\n"
        ]
    },
    {
        "link": "https://medium.com/@eldatero/master-the-perfect-chatgpt-prompt-formula-c776adae8f19?source=list-2eb23a991a63--------79-------0a856388a93a---------------------",
        "title": "Master the Perfect ChatGPT Prompt Formula",
        "subtitle": "false",
        "autorName": "DatHero",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*kEW4xpwqEIc9Dp3KNoa6Bw.jpeg",
        "clap": "1.3K",
        "response": "16",
        "timeForRead": "9 min read",
        "dateCreate": "Sep 5",
        "text": [
            "If you’ve been involved and constantly learning about AIs for some time now, you understand that prompt management is a very important skill. However, sometimes you don’t know what the best way to construct them is, which results in the responses generated by various AIs tending to be very long and general.\n",
            "Because of this, I have spent hundreds of hours in various prompt engineering courses and constantly testing different possible structures to build a formula that can bring me closer to high-quality responses from AIs (ChatGPT, Google Bard, Claude, etc).\n",
            "In this article, I will share with you 6 basic elements that a good prompt should have, and you will see for yourself the significant difference that exists if you include these elements in your next query.\n",
            "The first thing we need to understand is that the following elements are listed in descending order of importance: task, context, exemplar, persona, format, and tone. These elements are ranked in descending order based on their importance.\n",
            "Let’s look at it with a simple example.\n",
            "The first half of the entire example is the context, and the remainder is the task, where the latter is the most important (more than the context).\n",
            "Thus, if we only input tasks individually into ChatGPT, we will still get some meaningful results. Let’s look at the following example:\n",
            "The opposite happens if we only provide the context to ChatGPT, where we simply don’t get significant information for our purpose. Let’s look at the following example:\n",
            "So, we can say that every time we need to build a prompt, it is almost a duty to include a task, which should be complemented with context and examples. The persona, format, and tone are like the ornaments on the Christmas tree (of course they are important, but without the tree to support them, they wouldn’t be as relevant).\n",
            "When you build your next prompt, I recommend that you create a mental checklist, which would be as follows:\n",
            "You can also review the formula to include the outlined points or directions. Likewise, include sufficient relevant information. Later in this article, we will see that a good prompt doesn’t necessarily have to include these 6 elements.\n",
            "Now let’s start analyzing each of these six elements in detail.\n",
            "The key principle we should keep in mind is to always start the task sentence with an action verb (generate, drive, write, analyze, etc.) and clearly state your end goal.\n",
            "For example, we can generate a 3-month training plan or complex 3-step tasks (which include analysis, summary, and categorization).\n",
            "The second element is perhaps the most challenging question, because technically the information you can provide is infinite. I later found that asking these 3 questions is useful and practical.\n",
            "Always remember to provide ChatGPT with enough information so that it can produce high-quality results.\n",
            "Now let’s go back to the example mentioned at the beginning of this article, where I provide my background (1), write what I want to achieve as success (2), and finally describe the environment where all of this will happen (3).\n",
            "The key to ensuring efficient use of ChatGPT and Bard is to provide them with enough information to limit their infinite potential responses.\n",
            "Basically, studies conducted on all major language models have shown that including examples in prompts will yield better-quality answers.\n",
            "Let’s look at the following example, where I’ve taken an excerpt from a resume and now have ChatGPT rewrite all this information through a structure of achievement, measurement, and impact (Task). I also provide an explicit and detailed example to give it more information on how it should proceed.\n",
            "Now let’s look at a job interview example. Based on my resume, I can ask ChatGPT to use the STAR method to identify my weaknesses.\n",
            "Now, if you want to write a job description for a position within your team, you could initially give ChatGPT a description of the situation (context), and then you can provide a reference based on an already prepared description (example).\n",
            "In this case, you could use a LinkedIn role description, as ChatGPT will generate a response imitating its format and expression of professional language, consistent with the tone of an HR specialist.\n",
            "We will definitely save a lot of time!\n",
            "Here’s a thought about all the above:\n",
            "This is about the role you would like the AI (ChatGPT or Bard) to take on. Imagine you have a problem and would like to consult a specialist to address these doubts. For example, if you injure yourself while exercising, the person you would go to is a rehabilitation therapist who has years of experience treating athletes. If you’re looking for a job, the person you’d ask for advice would be a recruiter.\n",
            "You can also specify a particular persona, but only when it’s sufficiently famous. The results will be even better.\n",
            "Let’s look at the following example, where I ask ChatGPT to take on the personality of Batman when drafting an email.\n",
            "Close your eyes and visualize in your mind what you want to achieve, personal or group goals, landing your dream job, becoming a great speaker, etc. All of these are scattered ideas, but with the help of ChatGPT, you could organize them into a specific format, like a table.\n",
            "In this case, we’re going to analyze the prices of typical dishes for an upcoming trip to Peru, and I ask it to show me a table containing columns for places to visit, typical dishes, and their prices.\n",
            "However, we can also obtain formats like emails, bullet points, code blocks. These formats will also be useful for our work, such as paragraphs and markdown.\n",
            "Let’s look at an email example where we give it a context, a task, and also ask ChatGPT to use a header format.\n",
            "The last element is easy to understand if we correctly gauge the type of tone to use.\n",
            "Now, as an example, I’ll share a professional skill with you and tell ChatGPT the feeling you’re aiming for.\n",
            "We can write our next email in clear and concise language. You can also use a professional or polite tone.\n",
            "Now let’s combine all the elements seen through a practical example. We find ourselves in a situation where we need to communicate an email with content that includes relevant and impactful information.\n",
            "Keep in mind that if I have an existing email as a reference, we can add it through this new prompt and remove the element of exemplars.\n",
            "Considering the previous example, we have considered several elements for constructing the prompt, which definitely ensures that ChatGPT, or any AI we use, will provide us with high-quality answers. However, in the following image, we can see the contrast with a simpler prompt, which is more likely to yield very general or not so close-to-expected answers.\n",
            "In terms of specificity and usability, there are huge differences!\n",
            "If you liked this story, I invite you to check out the next one:\n",
            "See you in a future story. Hugs and blessings.\n"
        ]
    },
    {
        "link": "https://medium.com/@ignacio.de.gregorio.noblejas/ai-more-impressive-than-chatgpt-4cc9cf343185?source=list-9f88f190fa7--------41-------64d2b10e1db0---------------------",
        "title": "An AI more impressive than ChatGPT is here",
        "subtitle": "Action Transformers are the next leap for AI",
        "autorName": "Ignacio de Gregorio",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*p6kCCpNZARkVEYv4OCH7GQ@2x.jpeg",
        "clap": "2.5K",
        "response": "40",
        "timeForRead": "7 min read",
        "dateCreate": "Jan 28",
        "text": [
            "Few things have the potential to change that much in our daily life. Or in our work.\n",
            "And although you may very well be tempted to see the title as pure sensationalism, I can assure you that by the end of the article, you will think otherwise.\n",
            "What if I told you that there’s an even bigger use case for AI than ChatGPT, a use case that can completely change the way we interact with our phones, our tablets, or our computers to use them in ways thought impossible?\n",
            "This technology exists, and it’s sneaking up on you.\n",
            "But let me put a disclaimer; the extent of how excited or scared you’ll be after reading this article will depend entirely on you, not on me.\n",
            "That is the extent to how disruptive and transformative Action Transformers can be to your future.\n",
            "Generative AI represents the first time that this decades-long promise that AI was, has become a reality that can be appreciated by even the less techie part of society.\n",
            "Even though AI is already everywhere, until now AI models have been used as predictors; decision-makers for very tailored and specific use cases.\n",
            "Weirdly enough, the most successful — economically speaking — field of AI has been Online Advertising, allowing companies like Google or Meta to build literal empires out of the simple concept of certainty.\n",
            "That is, providing humans with the empirical, data-driven assurance that the outcome of a certain action, more often than not, would be profitable.\n",
            "Thanks to AI, Google and Meta guaranteed advertisers results by ensuring that their marketing campaigns would reach the desired customer personas, transforming the marketing industry from the historical “hit-and-miss” to something much more streamlined.\n",
            "But this amazing success required important investments, making AI a prohibited technology for the majority.\n"
        ]
    },
    {
        "link": "https://medium.com/@sarang0909.bds/nlp-knowledge-graph-e6e82ebef98d?source=list-6a12672b898d--------39-------54fdf6aa16d2---------------------",
        "title": "NLP-Knowledge Graph",
        "subtitle": "false",
        "autorName": "Sarang Mete",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*fUIbZ3v8nD68RkJpNJdKOg.png",
        "clap": "141",
        "response": "1",
        "timeForRead": "3 min read",
        "dateCreate": "Nov 1, 2022",
        "text": [
            "Explore different libraries and create production ready code\n",
            "Knowledge Graphs(KG) are one of the most important NLP tasks. KG is nothing but way of representing information extraction/relationship(subject,object,relation) from text.\n",
            "In this article, we’ll explore a process to create KG.\n",
            "Steps in creation of Knowledge Graph:\n",
            "We’ll use following Input Text to create KG\n",
            "Convert pronouns to their original nouns. You can read about it more in my project.\n",
            "Coreference resolution Output: Text in Bold is resolved\n",
            "2.Named Entity Recognition(NER)\n",
            "We can skip this step and just get all relationships extracted. However, sometimes you ‘ll need only certain entities types and their relationships. We can extract default entities like NAME,PERSON etc from many available libraries or we can also build our own NER model. I’ve created a project to build custom NER-PERSON,ORG,PLACE,ROLE. But for knowledge graph,I am getting all relationships.Refer my Custom NER project.\n",
            "Output of custom NER\n",
            "3.Entity Linking/Entity Disambiguation\n",
            "We can get different words/nouns for same entity. Example, U.S,United States of America,America. All these should be considered as one entity. We can achieve this by getting their root id if we have some knowledge base. Here, we are going to use Wikipedia knowledge. So, many time entity linking is also called as wikification.\n",
            "4.Relationship Extraction\n",
            "It means fetching relationship in text.\n",
            "I’ve explored couple of libraries- Stanford Open IE and rebel libraries. Please check notebook.\n",
            "I selected rebel for my final implementation because Stanford Open IE output was little redundant and it is slow.\n",
            "Output of rebel relationship extraction:\n",
            "5. Knowledge Graph Creation\n",
            "I’ve explored neo4j python wrapper py2neo and networkx in a notebook and selected networkx just because ease of use for visualization. We should go for more powerful neo4j if want to use graph databases and perform further analysis but we are not doing that here.\n",
            "Output of networkx:\n",
            "sample output of py2neo for different text:\n",
            "I’ve created a complete end to end project for Knowledge Graph creation to deployment. The project is production ready. You can refer it here.\n",
            "The main challenges I’ve solved in this project:\n",
            "If you liked the article or have any suggestions/comments, please share them below!\n",
            "Let’s connect and discuss on LinkedIn\n"
        ]
    },
    {
        "link": "https://medium.com/@angelina-yang/how-to-measure-prompts-performance-c5c3c7796a0c?source=list-2eb23a991a63--------222-------0a856388a93a---------------------",
        "title": "How to Measure Prompts Performance?",
        "subtitle": "false",
        "autorName": "Angelina Yang",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*vDrkdkPsVBmL9qi9vQ84BQ.jpeg",
        "clap": "27",
        "response": "1",
        "timeForRead": "2 min read",
        "dateCreate": "Aug 17",
        "text": [
            "There are a lot of explanations elsewhere, here I’d like to share some example questions and potential answers in an interview setting.\n",
            "Here are some tips for readers’ reference:\n",
            "Introducing new prompts often leads to varied outcomes across different scenarios. The conventional approach to evaluating a model’s success, typical in traditional machine learning, doesn’t directly align with the nature of generative models. Metrics like accuracy (or the ones we talked about last week) might not seamlessly apply, as determining correctness can be subjective and challenging to quantify.\n",
            "At a broader level, there are two key focal points to consider:\n",
            "This question as a whole is a complex topic and I’ll write a more detailed post about it next time!\n",
            "Some key points about why this matters:\n",
            "You can find explanation by Josh Tobin from my original post here!\n",
            "Check Josh Tobin’s explanation!\n",
            "Thanks for reading my newsletter. You can follow me on Linkedin or Twitter @Angelina_Magr! You can find the original post and references here.\n",
            "Note: There are different angles to answer an interview question. The author of this newsletter does not try to find a reference that answers a question exhaustively. Rather, the author would like to share some quick insights and help the readers to think, practice and do further research as necessary.\n"
        ]
    },
    {
        "link": "https://medium.com/@cobusgreyling/least-to-most-prompting-b37ed2e19859?source=list-2eb23a991a63--------306-------0a856388a93a---------------------",
        "title": "Least To Most Prompting",
        "subtitle": "Least To Most Prompting for Large Language Models (LLMs) enables the LLM to handle complex reasoning.",
        "autorName": "Cobus Greyling",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*nzfAEuujMN0s-aK6R7RcNg.jpeg",
        "clap": "22",
        "response": "1",
        "timeForRead": "6 min read",
        "dateCreate": "Jul 27",
        "text": [
            "I’m currently the Chief Evangelist @ HumanFirst. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n",
            "The process of inference is reaching a conclusion based on evidence and reasoning. And in turn reasoning can be engendered with LLMs by providing the LLM with a few examples on how to reason and use evidence.\n",
            "In the case of Chain Of Thought Reasoning (CoT), via a few-shot prompt engineering approach, the LLM can be taught how to decompose the challenge or question into a reasoning pattern. In essence decompose the question, and the LLM is taught that via a few examples.\n",
            "The Self-Ask approach not only guides the LLM to follow a chain of thought while reasoning, but aims to have the LLM ask itself questions to reach the final conclusion.\n",
            "The basic premise of Self-Ask is that even if the LLM does not have an explicit answer to a specific question, the LLM does have enough supporting information. This supporting information to sub-questions can be used to reach a final conclusion.\n",
            "Hence a novel prompting strategy was developed, named least-to-most prompting. This method is underpinned by the following strategy:\n",
            "Solving each subproblem is facilitated by the answers to previously solved subproblems.\n",
            "Hence least to most prompting is a technique of using a progressive sequence of prompts to reach a final conclusion.\n",
            "Here is a practical example prompt:\n",
            "text-davinci-003 returns the wrong answer as seen below, when prompted directly:\n",
            "Below the least to most prompting approach is followed and illustrated within the OpenAI playground.\n",
            "The sequence followed is to first-off ask the model (for consistency we again use text-davinci-003) the following question:\n",
            "What subproblems must be solved before answering the inquiry?\n",
            "Subsequently the LLM generates 4 subproblems which needs to be solved in order for the instruction to be completed.\n",
            "Below the Least To Most prompting approach is followed, the LLM is prompted with the four subproblems or tasks it identified.\n",
            "However, the subproblems are posed one at a time, and the previous problem and answer is included in the prompt. Hence building the prompt out one Q/A or subproblem at a time.\n",
            "The previous prompts are included as a reference and serves a few-shot training purpose.\n",
            "The scenario presented to the LLM can be seen as ambiguous, but the LLM does a stellar job in following the sequence and reaching the correct answer.\n",
            "The correct answer to this prompt is:\n",
            "Both Chain-Of-Thought prompting and Self-Ask prompting via a playground environment supplies the LLM with a single example to follow in one prompt.\n",
            "However, the principle behind Least-To-Most is to get a breakdown from the LLM, and then in a sequential fashion step through the questions and answers.\n",
            "Three caveats to add to this are:\n",
            "⭐️ Follow me on LinkedIn for updates on Large Language Models ⭐️\n",
            "I’m currently the Chief Evangelist @ HumanFirst. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"
        ]
    },
    {
        "link": "https://medium.com/@logankilpatrick/why-is-everyone-in-ai-talking-about-llamas-ca776ed93bab?source=list-2eb23a991a63--------265-------0a856388a93a---------------------",
        "title": "Why is everyone in AI talking about Llamas?",
        "subtitle": "Meta’s Llama model, Llama Index, and more Llama named things for developers",
        "autorName": "Logan Kilpatrick",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*uE-kx1RATLXacyr5_r95qA.jpeg",
        "clap": "133",
        "response": "2",
        "timeForRead": "6 min read",
        "dateCreate": "Jul 28",
        "text": [
            "Someone please remind me why we are talking about Llama’s?\n",
            "Edit: This story was updated on July 28th, 2023 to reflect that Llama Index was the first to use this naming before Meta.\n",
            "Keeping up with the pace of new generative AI research, news, products, and open source projects can be pretty exhausting. Everyday there is some new thing that begs the attention of those in the generative AI space.\n",
            "If you have been following the news closely at all, you have likely heard the word Llama a lot more than any other time in your life. And no, it’s not because the new San Francisco trend is to own your own Llama, it is because of AI products named around this.\n",
            "In this post we will explore some of the many Llama related generative AI projects, where the naming comes from, and more.\n",
            "As always, you are reading my personal blog so you guessed it, these are my personal views. Let’s dive in!\n",
            "Back in February of 2023, Meta (facebook) released the now famous LLaMA foundation model in response to the excitement spurred by ChatGPT a few months prior. At the time, this was one of the largest models which had been released to data so the developer community was very excited.\n",
            "Let’s get it out of the way to begin with, why did Meta decide to name it Llama? Is it because their ML model has a thick coat of fur or a oddly long neck? Sadly, the real reason is a boring acronym, Large Language Model Meta AI.\n",
            "Taking a small step back, Meta releasing Llama is actually, in my opinion, a very positive thing. I think their commitment to open science is admirable and something I am happy to see. For those who don’t know, I am an advisor at NASA supporting the open science initiative there which is now adopted by over 10 US government agencies so it is a topic which in many contexts is deeply aligned with my world view.\n",
            "Much of the initial excitement about Llama was hampered by the non commercial license which prevented companies from building products with it. This was until the weights (the brain of a deep learning model) were leaked online. Another aspect that seemed to get developers excited was the differing sizes of the model. For many, being able to fine-tune a smaller model is critical for their use case and just having a large 75 billion parameter model would likely cost too much to use in practice:\n",
            "More broadly speaking, my impression of the excitement around Llama was that developers were happy Meta was taking an open source approach to developing large language models, but the current versions did not help much given the licensing and infrastructure required to run the model on your own.\n",
            "Since the initial release, developers have been waiting to see when Meta would release the next version. To many people’s surprise, the 2nd iteration of Llama came on July 18th 2023 with an unexpected parter: Microsoft.\n",
            "Llama 2 solves the two main problems that the original Llama model fell short of:\n",
            "Both of these are generally wins for the developer ecosystem. I will say that historically Meta has had a dicey track record and is the target of frequent attacks for privacy and other short comings. But there are also a lot of really smart people working hard to make these models work so I wish them all well!\n",
            "Edit: This section has been updated to reflect that Llama Index was the first to use the Llama name, here’s a quote from Llama Index founder Jerry Liu:\n",
            "I can only imagine the validation and marketing + SEO boost Llama index got from Meta planting the flag of Llama in the AI ecosystem. I expect that these two early adopters will spawn a whole new ecosystem of tools based around this brand.\n",
            "My general impression is that like GPT (generative pre-trained transformer) has become a popular term in the AI space, Llama is filling a much needed gap to give people some naming optionality while still making it clear you are in the generative AI space.\n",
            "There was a single moment in my mind the crystalized Llama as a term, and it was Hugging Faces AI meetup in SF where they had a real Llama present:\n",
            "Generally speaking, the Llama naming schema passes my viral product checklist simply because it has an available emoji, a critical angle in todays product space.\n",
            "Besides the Llama foundation model, Llama index is probably the 2nd most popular Llama project out there. It is designed as a data framework for large language models that allows you to seamlessly connect different data stores (like a database, your email, etc) to a large language model. This is incredibly useful as you are building a project because it means you don’t need to build something from scratch to connect with all these sources yourself.\n",
            "Llama index also connects with other tools like LangChain which serve more as the application layer than the data layer. I wrote up some thoughts on LangChain in another post:\n",
            "Llama index actually does many similar things as Langchain with support for agents, chat bots, data sources, and more tooling to make working with large language models easier.\n",
            "For embeddings, there are a bunch of helpful tools to enable changing the batch size of the embeddings, switch vector database provider, and more.\n",
            "The above code is a simple example of using the ChromaDB provider right inside of Llama Index. This is helpful as you can test multiple providers (as long as they support the embeddings format you are using) without needing to worry about writing code for all of them.\n",
            "I am working on a more in-depth article on Llama index so I will save the rest of this for later!\n",
            "There are so many interesting projects around the Llama models, a few are as follows:\n",
            "And many others which port the Llama model to different languages or support different training architectures that the original model did not.\n",
            "In general, simply from a naming perceptive, I am glad that Llama seems to be helping push people away from calling everything GPT. While I think GPT has the added boost of relating to ChatGPT, it is a bit overwhelming that it’s used so frequently.\n",
            "I hope that this post was useful to get just a little more context about why everyone is talking about Llamas. I will be interested to see how Meta’s approach to releasing large language models changes over time as the competition heats up. 🦙\n"
        ]
    },
    {
        "link": "https://medium.com/@cobusgreyling/automatic-prompt-engineering-907e230ece0?source=list-2eb23a991a63--------85-------0a856388a93a---------------------",
        "title": "Automatic Prompt Engineering",
        "subtitle": "Automatic Prompt Engineering (APE) generates optimised prompts for text generation, based on three inputs; the expected input data, the desired output & a prompt template.",
        "autorName": "Cobus Greyling",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*nzfAEuujMN0s-aK6R7RcNg.jpeg",
        "clap": "74",
        "response": "1",
        "timeForRead": "5 min read",
        "dateCreate": "Sep 27",
        "text": [
            "I’m currently the Chief Evangelist @ HumanFirst. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n",
            "This study from March 2023 takes a simple yet novel approach to prompt engineering by automatically generating prompts based on the desired input and output.\n",
            "In a recent article I considered the future of prompt engineering, and the possibility of soft prompts (prompt tuning). I argued that user context, ambiguity and user intent all play an important role in any conversational UI.\n",
            "The question is, can this approach accelerate the process where manually wording prompts fade into the background and interaction with the LLM is based on contextual example input and output datasets?\n",
            "What I like about this approach, is that context, and user intent can be mapped, while also taking into consideration possible ambiguity.\n",
            "Yet manually crafting prompts is tedious in the sense of trying to word a prompt in such a way to engender a desired response from the LLM. Focussing on prompt engineering also does not take into consideration an array of possible user inputs.\n",
            "APE offers an alternative approach to prompt engineering, where via input and matching output examples, prompts can be generated on the fly.\n",
            "The basic notebook below shows how Automatic Prompt Engineering (APE) can be used to generate prompts based on a small input data set, a list of expected outputs and a prompt template.\n",
            "APE performs this in two steps:\n",
            "Below is a complete notebook example to run APE, all you will need to run this, is your own OpenAI API key. The input dataset is defined as words, and the output dataset is defined as antonyms. The template is defined as eval_template.\n",
            "And below is the list of possible prompts listed according to a score.\n",
            "The code below can be used to score a human or manually entered prompt.\n",
            "And the score of the human entered prompt.\n",
            "APE is an approach where the LLM is given the desired input and output, and the prompt is generated from these examples.\n",
            "This approach reduces the human effort involved in creating and validating prompts. This algorithm uses LLMs to generate and select prompts automatically.\n",
            "⭐️ Follow me on LinkedIn for updates on Large Language Models ⭐️\n",
            "I’m currently the Chief Evangelist @ HumanFirst. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"
        ]
    },
    {
        "link": "https://medium.com/@odsc/google-ai-proposes-new-method-to-reduce-burden-on-llms-pairwise-ranking-prompting-e411ecb0a76b?source=list-2eb23a991a63--------276-------0a856388a93a---------------------",
        "title": "Google AI Proposes New Method to Reduce Burden on LLMs: Pairwise Ranking Prompting",
        "subtitle": "false",
        "autorName": "ODSC - Open Data Science",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/2*xmanz0nHjUWhZDxHWKKaYg.png",
        "clap": "67",
        "response": "2",
        "timeForRead": "3 min read",
        "dateCreate": "Jul 28",
        "text": [
            "Google AI researchers have released a new paper proposing a new approach called Pairwise Ranking Prompting, or PRP for short. The goal is to alleviate the challenges faced by Large Language Models in solving text ranking problems. LLMs, such as GPT-3 and PaLM, have demonstrated remarkable performance on natural language tasks, even in zero-shot settings.\n",
            "But, when it comes to text ranking, existing methods tend to fall short compared to trained baseline rankers, with the exception of black box systems like GPT-4. In the paper, the team acknowledges the value of black box systems, they also emphasize the constraints faced by academic researchers, including cost and access limitations.\n",
            "So in their study, they delve into the reasons why LLMs struggle with ranking problems using the current pointwise and listwise approaches. According to the team, they found that generating calibrated prediction probabilities for pointwise techniques proves to be exceedingly challenging for LLMs.\n",
            "Listwise techniques, on the other hand, result in inconsistent or irrelevant outputs, indicating a lack of ranking awareness in current LLM pre-training and fine-tuning techniques. So to compensate for this limitation and reduce issues related to task complexity, the researchers proposed the PRP paradigm.\n",
            "This method utilizes a simple prompt architecture, employing a query and a pair of documents as the prompt for ranking tasks. Unlike existing methods, PRP offers both generation and scoring LLM APIs by default, addressing the calibration issue. Several PRP variations are discussed to ensure efficiency and effectiveness.\n",
            "They went on to evaluate PRP using moderate-sized, open-sourced LLMs on traditional benchmark datasets. The results paid off as they surpassed previous methods based on the black box commercial GPT-4 with significantly larger model sizes.\n",
            "One example of this was on the TREC-DL2020 dataset. The PRP based on the 20B parameter FLAN-UL2 model achieved a more than 5% improvement at NDCG@1 compared to the prior best method. On TREC-DL2019, PRP outperformed existing solutions such as InstructGPT by over 10% on most ranking measures, with slight performance degradation in NDCG@5 and NDCG@10 metrics compared to GPT-4.\n",
            "Overall, the PRP exhibits several advantages, including its support for LLM APIs for scoring and generation, and its insensitivity to input orders. This work presents three major contributions. First, it demonstrates effective zero-shot ranking using moderate-sized, open-sourced LLMs. Next, the achievement of state-of-the-art ranking performance through straightforward prompting and scoring mechanisms.\n",
            "And finally, the exploration of efficiency enhancements while maintaining good empirical performance.\n",
            "Originally posted on OpenDataScience.com\n",
            "Read more data science articles on OpenDataScience.com, including tutorials and guides from beginner to advanced levels! Subscribe to our weekly newsletter here and receive the latest news every Thursday. You can also get data science training on-demand wherever you are with our Ai+ Training platform. Subscribe to our fast-growing Medium Publication too, the ODSC Journal, and inquire about becoming a writer.\n"
        ]
    },
    {
        "link": "https://medium.com/@ankushmulkar/top-most-ten-nlp-techniques-used-in-the-industry-34570a29f2f?source=list-cbb1022c4bbb--------6-------5fec4a91bed0---------------------",
        "title": "Top Most Ten NLP Techniques Used In The Industry",
        "subtitle": "false",
        "autorName": "Ankush Mulkar",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ngV6B3hxzwq2WJ_OuyiW7A.jpeg",
        "clap": "171",
        "response": "1",
        "timeForRead": "10 min read",
        "dateCreate": "Jan 25",
        "text": [
            "Sentiment analysis is the process of determining the emotional tone behind a piece of text, such as a tweet, a product review, or customer feedback.\n",
            "The goal of sentiment analysis is to classify the text as positive, negative, or neutral. For example, if a customer writes a review of a product saying, “I love this product, it’s amazing”, the sentiment analysis algorithm would classify the text as positive. Sentiment analysis is widely used in industries such as e-commerce, social media, and customer service to gain insights into customer opinions and preferences.\n",
            "One way to perform sentiment analysis is by using a pre-trained model such as the one provided by the nltk library in python. Here's an example of how to use the nltk library to classify the sentiment of a piece of text as positive, negative, or neutral\n",
            "This example uses the SentimentIntensityAnalyzer class from the nltk.sentiment module to analyze the sentiment of the text \"I love this product, it's amazing\". The polarity_scores() method returns a dictionary containing the sentiment scores for the text, with the 'compound' score is a value between -1 and 1, where -1 is negative, 1 is positive and 0 is neutral. Based on the compound score, we can classify the sentiment as positive, negative, or neutral.\n",
            "Note that this is a simple example and in practice, Sentiment analysis is an area that require a lot of fine-tuning and adjusting to achieve good results. A pre-trained model might not perform well on certain types of texts (e.g. sarcasm) and might require additional fine-tuning or pre-processing steps to improve its performance.\n",
            "Named Entity Recognition (NER) is a technique used to extract entities such as people, organizations, and locations from unstructured text. One way to perform NER is by using pre-trained models, such as the one provided by the spacy library in Python. Here's an example of how to use the spacy library to extract named entities from a piece of text\n",
            "example uses the en_core_web_sm model from spacy to analyze the text \"Barack Obama visited the White House today\". The ents attribute of the processed text returns an iterator of named entities, and each entity has the attributes text and label_ that represent the text and the label of the entity respectively. In this example, the output will be\n",
            "It shows that “Barack Obama” is a person, and “White House” is a facility.\n",
            "There are multiple pre-trained models available in spacy for different languages, and some of them are more accurate than others. In addition, Named Entity Recognition is an area that require a lot of fine-tuning and adjusting to achieve good results. A pre-trained model might not perform well on certain types of texts (e.g. technical texts) and might require additional fine-tuning or pre-processing steps to improve its performance.\n",
            "Text classification is the process of automatically categorizing text into predefined classes or categories. For example, a text classification algorithm might be used to classify emails as spam or not spam, or to categorize news articles by topic. Text classification is used in a variety of applications, including natural language processing, information retrieval, and machine learning.\n",
            "Here is an example of text classification using the Python library scikit-learn. This example uses the 20 Newsgroups dataset, which contains texts from 20 different newsgroups. The goal is to train a classifier to predict the newsgroup a text belongs to based on its content.\n",
            "This code will load the 20 Newsgroups dataset and split it into training and test sets. Then it will transform the texts into numerical representation using TfidfVectorizer and train a Multinomial Naive Bayes classifier using the training set. Finally, it will use the trained classifier to predict the newsgroup of the test texts and evaluate the classifier’s accuracy.\n",
            "Machine translation is the process of automatically translating text from one language to another. For example, a machine translation algorithm might translate a news article from Spanish to English. Machine translation is used in a variety of industries, including e-commerce, international business, and government.\n",
            "Here is an example of using the OpenNMT library to translate text from English to French:\n",
            "This code will output: “Bonjour, comment vas-tu?”\n",
            "Please note that this is a very simple example and will not work out of the box as it requires a pre-trained model to be loaded. Also, this example uses a small dataset as input, and a pre-trained model might not be available for the specific case. for more about machine learning follow here\n",
            "Text summarization is the process of automatically generating a condensed version of a longer piece of text. For example, a text summarization algorithm might take a long news article and generate a shorter summary of the main points. Text summarization is used in a variety of applications, including natural language processing, information retrieval, and machine learning.\n",
            "Please note that this is a very simple example and will not work out of the box as it requires a pre-trained model to be loaded. Also, this example uses a small dataset as input, and a pre-trained model might not be available for the specific case.\n",
            "This code will output a summarized version of the text, keeping only the 20% most important sentences: “Some tools specifically avoid removing these stop words to support phrase search.”\n",
            "You can adjust the ratio parameter to change the amount of text that is summarized, or use the word_count parameter to specify the number of words to include in the summary.\n",
            "Information extraction is the process of extracting structured data from unstructured text. For example, an information extraction algorithm might extract product information, such as price and availability, from an e-commerce website. Information extraction is used in a variety of industries, including e-commerce, finance, and healthcare, to extract structured data from unstructured text.\n",
            "Here is an example of information extraction using Python and the Natural Language Toolkit (NLTK) library:\n",
            "The above code first tokenizes the text into individual words, then performs POS tagging to identify the part of speech for each word, and finally performs named entity recognition to identify entities such as people, organizations, and locations.\n",
            "The output of the ne_chunk function is a tree structure that can be further processed to extract the entities of interest.\n",
            "Text generation is the process of automatically generating text, such as creating product descriptions or writing news articles. For example, a text generation algorithm might take a product image as input and generate a product description. Text generation is used in a variety of industries, including e-commerce, marketing, and content creation.\n",
            "Here is an example of text generation using the GPT-2 model in the Python library, Hugging Face’s transformers:\n",
            "This code will generate text based on the provided prompt “Once upon a time in a land far, far away” using the GPT-2 model. The generated text will be printed on the console.\n",
            "Please note that you may require internet connection to download the pre-trained model and also a powerful GPU to generate the text.\n",
            "Text clustering is the process of grouping similar text documents together. For example, a text clustering algorithm might take a collection of news articles and group them into categories such as “sports”, “politics”, and “entertainment”. Text clustering is used in a variety of applications, including natural language processing, information retrieval, and machine learning.\n",
            "The above code first tokenizes the text into individual words, then performs POS tagging to identify the part of speech for each word, and finally performs named entity recognition to identify entities such as people, organizations, and locations.\n",
            "The output of the ne_chunk function is a tree structure that can be further processed to extract the entities of interest.\n",
            "Speech recognition is the process of converting spoken words into written text. For example, a speech recognition algorithm might be used in a voice-controlled system, such as a virtual assistant, to transcribe spoken commands into text that can be understood by a computer. Speech recognition is used in a variety of industries, including healthcare, finance, and customer service.\n",
            "There are many libraries and frameworks available for speech recognition in various programming languages. Here is an example of how to use the Speech Recognition library in Python to transcribe speech from a microphone:\n",
            "This example uses the recognize_google() function, which utilizes the Google Web Speech API to transcribe speech. Other options for transcribing speech include using the recognize_sphinx() function (which uses the CMU Sphinx engine) or the recognize_wit() function (which uses the Wit.ai API).\n",
            "You can also use this library to recognize speech from a file:\n",
            "Note that you need to have internet connection to use Google Web Speech API, and you may need to setup the credentials and install some additional package depend on the transcribing engine you choose.\n",
            "Text-to-speech (TTS) is a technology that converts written text into spoken words. It is commonly used in applications such as speech synthesis for the visually impaired, voice assistants, and automated customer service systems.\n",
            "TTS systems use a combination of techniques, such as natural language processing and machine learning, to produce realistic-sounding speech. Some examples of TTS software include Google Text-to-Speech, Amazon Polly, and Apple’s Siri.\n",
            "Here is an example of using the gTTS (Google Text-to-Speech) library in Python to convert text to speech:\n",
            "This code uses the gTTS library to convert the text “Hello, this is an example of text to speech using the gTTS library in Python.” to speech and save it to an mp3 file called “welcome.mp3”.\n",
            "The last line os.system(“mpg321 welcome.mp3”) plays the mp3 file using the command line tool mpg321. If you don’t have mpg321 installed in your system, you could use other player to play the mp3 file.\n",
            "Follow to given link for advance NLP AnkushMulkar/Natural-Language-processing (github.com)\n",
            "Click below links to know more about “Ankush Mulkar”\n"
        ]
    },
    {
        "link": "https://medium.com/@cobusgreyling/automatic-prompt-engineer-ui-aa7dd0770dd1?source=list-e28f6edecf84--------7-------7b153c9756d3---------------------",
        "title": "Automatic Prompt Engineer UI",
        "subtitle": "The Automatic Prompt Engineer (APE) UI demonstrates how prompts can be optimised for text generation.",
        "autorName": "Cobus Greyling",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*nzfAEuujMN0s-aK6R7RcNg.jpeg",
        "clap": "33",
        "response": "1",
        "timeForRead": "4 min read",
        "dateCreate": "Sep 29",
        "text": [
            "I’m currently the Chief Evangelist @ HumanFirst. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n",
            "APE takes as input a dataset with a list of inputs and outputs and a prompt template.\n",
            "Then APE uses a language model to generate a set of possible prompts. APE also has a prompt evaluation function to evaluate the quality of each generated prompt.\n",
            "APE shows promise and the concept is novel, and can expand, but the following needs to be noted:\n",
            "Considering the image below, when you click on tasks, different datasets are loaded for prompt generation and scoring.\n",
            "After the dataset is loaded, an estimate of cost can be generated prior to committing the evaluation.\n",
            "Under configuration there are basic and advanced configuration tabs.\n",
            "After running the APE functionality, the results are shown below, with the scores.\n",
            "In conclusion, the APE GUI is a convenient way to play around with datasets to create and tweak prompts.\n",
            "However, in a follow-up post I will be considering a recent study, which highlighted the challenges and complexities of prompt engineering. And how an AI accelerated latent-space can assist with that.\n",
            "⭐️ Follow me on LinkedIn for updates on Large Language Models ⭐️\n",
            "I’m currently the Chief Evangelist @ HumanFirst. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"
        ]
    },
    {
        "link": "https://medium.com/@keerthanasathish/find-and-replace-regex-531933ce77ca?source=list-cf9917645e65--------2-------e3327a426a29---------------------",
        "title": "Find and Replace: Regex",
        "subtitle": "false",
        "autorName": "Keerthana Sathish",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*w5ougHR7_PB2wKTte48F6w.jpeg",
        "clap": "1",
        "response": "1",
        "timeForRead": "3 min read",
        "dateCreate": "Jun 5",
        "text": [
            "We all perform find/ replace in our day-today tasks. It makes our work easier. To know more about how it works read until the end.\n",
            "Regex also called as Regular Expression is used to find patterns in a sentence. Common applications are web scraping, data validation, data wrangling and many other task.\n",
            "Before we get into the examples, let us first understand the metacharacters.\n",
            "To practice regex the link mentioned below can be used:\n",
            "https://regex101.com/\n",
            "[abcdefgh] is equivalent to [a-h].\n",
            "[^ ] — excludes the characters in the [ ] and includes other characters.\n",
            "In the below example characters after d is included.\n",
            "[ ^] — excludes the characters not mentioned within [ ].\n",
            "Characters from a-d is only included.\n",
            "We can also use numbers as characters.\n",
            "Alphabets from a-d and numbers from 0–5 are only included.\n",
            "The below example groups the character ‘a’ into three. There are 6 groups in total.\n",
            "It matches the string starting from 9 and takes 10 characters from 9\n",
            "[89] — Starting from 8 or 9\n",
            "[0–9] — Number is in the range from [0–9]\n",
            "{9} — Range from {0–9}\n",
            "2. Gmail\n",
            "3. Color/ Colour\n",
            "The character preceding ‘?’ is optional to be included.\n",
            "Regex has various applications like advanced search filters, web scraping, content filtering, search query validation. This is one of the important topics in natural language processing.\n"
        ]
    },
    {
        "link": "https://medium.com/@jrodthoughts/meet-lmql-an-open-source-query-language-for-llms-85b7bb5217f2?source=list-e28f6edecf84--------186-------7b153c9756d3---------------------",
        "title": "Meet LMQL: An Open Source Query Language for LLMs",
        "subtitle": "Developed by ETH Zurich, the language explores new paradigms for LLM programming.",
        "autorName": "Jesus Rodriguez",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*9hmRpqiPP9vEjlGS2AJnaw.jpeg",
        "clap": "232",
        "response": "3",
        "timeForRead": "6 min read",
        "dateCreate": "Jun 14",
        "text": [
            "In the realm of technology, large language models(LLMs) have exhibited exceptional capabilities across diverse tasks, including question answering and code generation. At its core, a LLM excels in automatically generating coherent sequences based on given inputs, relying on statistical likelihood. Leveraging this ability, users can prompt these models with language instructions or examples, enabling the execution of various downstream tasks. The advanced techniques of prompting can even facilitate interactions involving the language model, users, and external tools like calculators. However, achieving state-of-the-art performance or tailoring language models to specific tasks often necessitates implementing complex, task-specific programs, which may still rely on ad-hoc interactions.\n",
            "Language Model Programming (LMP) is an emerging discipline that is gaining traction to tackle those challenges. LMP represents a significant advancement in language model prompting, transitioning from pure text prompts to an intuitive combination of text prompting and scripting. Furthermore, LMP empowers users to specify constraints on the language model’s output, facilitating effortless adaptation to a multitude of tasks while abstracting the inner workings of the language model and providing high-level semantics. Recently, researchers from the prestigious ETH Zurich released LMQL, a query language for LLMs that builds on the principles of LMP.\n",
            "Conceptually, combines declarative SQL-like elements with an imperative scripting syntax. By leveraging…\n"
        ]
    },
    {
        "link": "https://medium.com/@Coursesteach/natural-language-processing-part-14-d3de0382ba09?source=list-660438a01f7f--------1-------dbbdeca8bd9e---------------------",
        "title": "Natural Language Processing(Part 14)-Probability and Bayes’ Rule",
        "subtitle": "false",
        "autorName": "Coursesteach",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg",
        "clap": "3",
        "response": "2",
        "timeForRead": "4 min read",
        "dateCreate": "Oct 15",
        "text": [
            "Probability is fundamental to many applications in NLP. You’ll see how you can use it to help classify whether a tweet is positive or negative. Let’s get started.\n",
            "To start, we are going to first review what’s probabilities and conditional probabilities are, how they operate, and how they can be expressed mathematically. Then I’ll go over how to derive Bayes rule from the definition of conditional probabilities. Bayes rule is applied in many different fields, ranging from medicine to education and is used extensively in NLP. Once you understand the theory behind Bayes rule, you can use it to perform sentiment analysis on tweets,\n",
            "Imagine you have an extensive corpus of tweets that can be categorized as either positive or negative sentiment, but not both. Within that corpus, the word happy is sometimes being labeled positive and sometimes negative.\n",
            "Let’s explore why this situation is occurring.\n",
            "One way to think about probabilities is by counting how frequently events occur. Suppose you define event A as a tweets being labeled positive, then the probability of event A, shown as B of A here, is calculated as the ratio between the counts of positive tweets in the corpus divided by the total number of tweets in the corpus.\n",
            "In this example, that number comes out to 13 over 20, or 0.65. You could also express this value as a percentage, 65 percent positive. It’s worth noting that the complimentary probability here, which is the probability of the tweets expressing a negative sentiment is just equal to one minus the probability of a positive sentiment.\n",
            "Note that for this to be true, all tweets must be categorized as either positive or negative but not both. Let’s define Event B in a similar way by counting tweets containing the word happy. In this case, the total number of tweets containing the word happy, shown here as N-happy is 4.\n",
            "Here’s another way of looking at it. Take a look at the section of the diagram were tweets are labeled positive and also contain the word happy. In the context of this diagram, the probability that a tweet is labeled positive and also contains the word happy is just the ratio of the area of the intersection divided by the area of the entire corpus.\n",
            "In other words, if there were 20 tweets in the corpus, and three of them are labeled positive and also contain the word happy, then the associated probability is 3 divided by 20 or 0.15. You now know how to calculate the probability of an intersection. You know how to calculate the probability of a word, namely happy with the probability of being positive. In the next Tutorial, we willtalk about Naive Bayes.\n",
            "Please Follow coursesteach to see latest updates on this story\n",
            "If you want to learn more about these topics: Python, Machine Learning Data Science, Statistic For Machine learning, Linear Algebra for Machine learning Computer Vision and Research\n",
            "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n",
            "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n",
            "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others!💻✌️\n",
            "Note:if you are a NLP export and have some good suggestions to improve this blog to share, you write comments and contribute.\n",
            "if you need more update about NLP and want to contribute then following and enroll in following\n",
            "👉Course: Natural Language Processing (NLP)\n",
            "👉📚GitHub Repository\n",
            "👉 📝Notebook\n",
            "Do you want to get into data science and AI and need help figuring out how? I can offer you research supervision and long-term career mentoring.Skype: themushtaq48, email:mushtaqmsit@gmail.com\n",
            "Contribution: We would love your help in making coursesteach community even better! If you want to contribute in some courses , or if you have any suggestions for improvement in any coursesteach content, feel free to contact and follow.\n",
            "Together, let’s make this the best AI learning Community! 🚀\n",
            "👉WhatsApp\n",
            "👉 Facebook\n",
            "👉Github\n",
            "👉LinkedIn\n",
            "👉Youtube\n",
            "👉Twitter\n",
            "1- Natural Language Processing with Classification and Vector Spaces\n"
        ]
    }
]
