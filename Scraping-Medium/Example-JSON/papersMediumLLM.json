[
    {
        "link": "https://medium.com/@tiro2000/chat-with-your-rag-97a142663d16?source=list-499d6e31470a--------3-------aa9880ad746d---------------------",
        "title": "Chat with your RAG",
        "subtitle": "false",
        "autorName": "Tarek AbdELKhalek",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/0*lo5QiV00jJfPa0na",
        "clap": "6",
        "response": "9",
        "timeForRead": "4 min read",
        "dateCreate": "Oct 9",
        "text": [
            "🔍 What’s Retrieval Augmented Generation (RAG) in AI?\n",
            "Retrieval Augmented Generation (RAG), the superhero of Chat with your docs, bridging the worlds of information retrieval and text generation! It’s like having Sherlock Holmes and Shakespeare team up to tackle complex tasks that require loads of knowledge. RAG swoops in, grabs the most relevant information from external sources like Wikipedia or company knowledge bases, mixes it with the input, and voila! It generates a comprehensive output with references .\n",
            "🔍 Unraveling RAG: RAG seamlessly combines the prowess of retrieving information from vast databases with the art of generating human-like text. It’s like having a librarian fetching the right book and a storyteller narrating it perfectly! RAG adds an intermediate information retrieval step before the final generation process.\n",
            "🛠 Harnessing RAG with Langchain and GPT4ALL: A Deep Dive:\n",
            "source\n",
            "🌟 The RAG Advantage:\n",
            "source\n",
            "Source\n",
            "🌐 Local or Cloud?: The beauty of RAG is its flexibility. Whether you’re using a local LLM, or cloud like OpenAI. A local vector database like chroma, or a cloud one like Pincone, the underlying magic remains the RAG model.\n",
            "🎉 Why It’s Exciting?: Imagine chatting with your documents, unlocking new possibilities, and streamlining your interactions. From professionals to students, the applications are endless!\n",
            "Picture this: businesses seeing a 50% surge in productivity for knowledge tasks, all thanks to RAG! This isn’t just about companies optimizing resources; it’s about a potential 7% GDP growth, as projected by Goldman Sachs. The horizon is thrilling with RAG ushering in the era of Knowledge Assistants. Imagine an AI ally streamlining corporate data, liaising with enterprise systems, and boosting team morale. RAG and Large Language Models (LLMs) are the dynamic duo propelling us into the future, we’re speeding into an AI-driven future. Ready to be part of this revolution? Jump aboard the RAG express!\n",
            "Resources :\n",
            "https://arxiv.org/abs/1909.01066\n",
            "https://ai.stanford.edu/blog/retrieval-based-NLP/\n",
            "https://python.langchain.com/docs/use_cases/question_answering\n",
            "https://huggingface.co/docs/transformers/model_doc/rag\n",
            "https://ai.stanford.edu/blog/retrieval-based-NLP/\n",
            "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n"
        ]
    },
    {
        "link": "https://medium.com/@abonia/best-llm-and-llmops-resources-for-2023-75e96ac37feb?source=list-46b5cb511765--------1-------ecb349332558---------------------",
        "title": "Best LLM and LLMOps Resources for 2023",
        "subtitle": "Curated list of best courses, books, resources on large language model",
        "autorName": "Abonia Sojasingarayar",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*r2D-WwsMud1wf4mraB6NFA.jpeg",
        "clap": "85",
        "response": "1",
        "timeForRead": "3 min read",
        "dateCreate": "May 19",
        "text": [
            "👉 Generative AI learning path: https://www.cloudskillsboost.google/paths/118\n",
            "👉 Full Stack LLM Bootcamp : https://lnkd.in/eH8VXpwB\n",
            "👉 LLM University to learn about LLMs and NLP — Cohere : https://lnkd.in/etKAjaYg\n",
            "👉 Deploying GPT and Large Language Models — Oreilly : https://lnkd.in/eDDivjB6\n",
            "👉 Professional Certificate in Large Language Models- edX : https://lnkd.in/edg3gPzQ\n",
            "👉 Understanding Large Language Models — Princeton University : https://lnkd.in/eE44cmza\n",
            "👉 Natural Language Processing Specialization — coursera : https://lnkd.in/eNGDYGeA\n",
            "👉 Large Language Models — The Stanford CS324 : https://lnkd.in/eJKfDTHK\n",
            "👉 Transformers Course — HuggingFace : https://lnkd.in/eY2-NdGG\n",
            "👉 Large Language Models — Class Central : https://lnkd.in/exVh6g-K\n",
            "👉 Large Language Models — Rycolab : https://lnkd.in/eRR_EzGW\n",
            "💥 Curated List: Elevate your Generative AI expertise with a carefully curated selection of Andrew Ng’s GenAI courses. Expand your skill set through focused modules offered by DeepLearning.AI\n",
            "👉 ChatGPT Prompt Engineering for Developers:Go beyond the chat box. Use API access to leverage LLMs into your own applications, and learn to build a custom chatbot.Level : Beginner to Advanced📌 Link : https://lnkd.in/gWZUEK2A\n",
            "👉 Building Systems with the ChatGPT APILevel up your use of LLMs. Learn to break down complex tasks, automate workflows, chain LLM calls, and get better outputs.Level : Beginner to Advanced📌 Link : https://lnkd.in/gq9kjmQf\n",
            "👉 LangChain for LLM Application DevelopmentThe framework to take LLMs out of the box. Learn to use LangChain to call LLMs into new environments, and use memories, chains, and agents to take on new and complex tasks.Level : Beginner📌 Link: https://lnkd.in/ggpgxHm7\n",
            "👉 LangChain: Chat with Your DataCreate a chatbot to interface with your private data and documents using LangChain.Level : Beginner📌 Link : https://lnkd.in/gUZrfksz\n",
            "👉 Building Generative AI Applications with GradioCreate and demo machine learning applications quickly. Share your app with the world on Hugging Face Spaces.Level : Beginner📌 Link : https://lnkd.in/gCHRS7nv\n",
            "👉 Evaluating and Debugging Generative AILearn MLOps tools for managing, versioning, debugging and experimenting in your ML workflow.Level : Intermediate📌 Link : https://lnkd.in/gdZd-prA\n",
            "👉 How Diffusion Models WorkLearn and build diffusion models from the ground up. Start with an image of pure noise, and arrive at a final image, learning and building intuition at each step along the way.Level : Intermediate📌 Link : https://lnkd.in/g7ajmY4X\n",
            "👉 Finetuning Large Language ModelsLearn to finetune an LLM in minutes and specialize it to use your own dataLevel : Intermediate📌 Link : https://lnkd.in/ghxMEpCX\n",
            "👉 Practical Natural Language Processing — Oreilly : https://lnkd.in/eKBHvdzM\n",
            "👉 Natural Language Processing with Transformers — Oreilly : https://lnkd.in/ehazWcMY\n",
            "👉 Transformers for Natural Language Processing — Packt : https://lnkd.in/e_Y5cX6c\n",
            "👉 GPT-3: Building Innovative NLP Products Using Large Language Models : https://lnkd.in/eSpvDErp\n",
            "👉 Hands-On Generative AI with Transformers and Diffusion Models: https://www.oreilly.com/library/view/hands-on-generative-ai/9781098149239/\n",
            "👉 Quick Start Guide to Large Language Models: Strategies and Best Practices for using ChatGPT and Other LLMs : https://lnkd.in/erbhdEjU\n",
            "👉 Understanding Large Language Models-A Transformative Reading List : https://lnkd.in/eEv2vi2w\n",
            "👉 Practical Deep Learning for Coders : https://course.fast.ai\n",
            "👉 The Annotated Transformer : https://lnkd.in/et883Grd\n",
            "👉 The Illustrated Transformer : https://lnkd.in/ehc9Bpk7\n",
            "👉 Langchain Demo : https://lnkd.in/eWyWzZrb\n",
            "👉 State of GPT | BRK216HFS : https://www.youtube.com/watch?v=bZQun8Y4L2A&themeRefresh=1\n",
            "👉 Awsome LLMOps — GitHub : https://lnkd.in/e3KNspKi\n",
            "👉 Awsome LLM Repo — GitHub : https://lnkd.in/eR2JwbPV\n",
            "👉 LLM Cheatsheet — Github : https://github.com/Abonia1/CheatSheet-LLM\n",
            "👉 LLMOps — https://vinija.ai/concepts/LLMOps/\n",
            "👉 OpenAI Cookbook — https://github.com/openai/openai-cookbook\n",
            "I hope you found this compilation of resources on large language models to be beneficial. We have included a variety of courses, books, reading lists, and other valuable resources and frameworks that can assist you in constructing your own impactful applications based on LLMs. Stay tune for another intresting article on LLM.\n",
            "Keep an eye out for future updates to this list, and stay tuned for another intriguing article on LLM.\n"
        ]
    },
    {
        "link": "https://medium.com/@jeremy-k/exploring-llama2-large-language-model-setup-utilization-and-prompt-engineering-986e9d338ee3?source=list-412323769000--------10-------07f9e4e23822---------------------",
        "title": "Exploring Llama2 Large Language Model: Setup, Utilization, and Prompt Engineering",
        "subtitle": "false",
        "autorName": "JeremyK",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*v75NZj-ChX0AadDzxUNE8g.png",
        "clap": "27",
        "response": "3",
        "timeForRead": "6 min read",
        "dateCreate": "Aug 25",
        "text": [
            "Since the public release and subsequent popularity of ChatGPT towards the end of 2022, Large Language Models (LLMs) have emerged as a significant advancement in the AI field. Following this trend, various other LLMs like Bard or Prometheus have also been introduced.\n",
            "In July 2023, MetaAI made the announcement of open-sourcing the latest iteration of their LLM, named Llama2. With seamless integration into the Hugging Face transformers ecosystem, utilizing, and even fine-tuning LLMs has become remarkably accessible to a wide range of users.\n",
            "In this article, I will guide you through the process of using Llama2, covering everything from downloading the model and running it on your laptop to initiating prompt engineering.\n",
            "For additional resources, please visit Huggingface’s official website: https://huggingface.co/blog/llama2\n",
            "Llama2 is available through 3 different models:\n",
            "While the first one can run smoothly on a laptop with one GPU, the other two require more robust hardware, with the 70b variant ideally supported by two GPUs.\n",
            "Additionally, each version includes a chat variant (e.g. Llama-2–70b-chat-hf) that was further trained with human annotations. This helps improve its ability to address human queries and provide helpful responses.\n",
            "Assuming you want to use LLama-2 via the transformers framework, which I recommend, it’s imperative to follow these two key steps:\n",
            "The email address you use on both sites must be the same. Once your request is approved (took less than one hour in my case), you should be able to see the model card on HuggingFace. You are now ready to move on to the next step.\n",
            "To run Llama-2, minor requirements must be met. Using a virtual environment is recommended to isolate the packages downloaded.\n",
            "You can reuse the script provided by Huggingface.\n",
            "It is first required to log in to Hugging Face through the terminal in order to access the model.\n",
            "If you don’t have a token yet, you can generate one here: https://huggingface.co/settings/tokens .\n",
            "Subsequently, the script can be executed:\n",
            "This process involves loading the 7 billion parameter model through the pipeline function and generating text based on a given prompt.\n",
            "Pretty easy, isn’t it? However, you may not want to use a LLM through via a Python script.\n",
            "The good news is you can use Gradio to quickly and easily set up a chatbot using Llama-2. The associated code is readily accessible on 🤗Huggingface.\n",
            "This space implement the 7B model in a chat app: https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat\n",
            "By cloning the repository to your machine, you can seamlessly set up and engage with the model through the interface.\n",
            "Then:\n",
            "Connect to http://127.0.0.1:7860/ and start interacting with the Llama-2 model.\n",
            "Although the model downloaded via Hugging Face is stored in ~/.cache/huggingface/hub, saving it locally can be advantageous for potential deployment on another system. The following code snippets illustrate the process:\n",
            "Then, loading the local version can be done as follows:\n",
            "If you deploy the model, do not forget to include Meta’s license and acceptable use policy.\n",
            "This segment explores basic prompt engineering and the configuration of the model behavior.\n",
            "If you toggle the advanced options button on the gradio app, you will see several parameters you can tune:\n",
            "The “system prompt” parameter is by default set to instruct the model to be helpful and friendly but not to disclose any harmful content.\n",
            "With the normal behavior, let’s ask: What is the capital of France?\n",
            "If you modify the system prompt by “do not answer any questions”, this is what you will get. In line with our instructions but still a bit surprising.\n",
            "Using the 7b model, we type different prompts to explore how Llama-2 responds.\n",
            "Despite its overall commendable performance, Llama-2 may occasionally exhibit unusual behaviors. For instance, it might decline to address certain inquiries, such as requests involving coding to delete files.\n",
            "Nevertheless, when altering the model’s instructions to “Always provide a positive answer.”, it yields a distinct outcome:\n",
            "The release of Llama2 by MetaAI marks a milestone in the realm of Large Language Models. Its integration with the Hugging Face transformers ecosystem empowers users to not only run Llama2 effortlessly but also fine-tune it.\n",
            "Whether implemented through Python scripts or integrated into web interfaces, Llama2’s capabilities remain impressive, even if occasional surprising behaviors may appear. Considering the results achieved using the 7B model, one can expected even enhanced performance from the 70b version.\n",
            "#AI #LLM #NLP #LLAMA2\n"
        ]
    },
    {
        "link": "https://medium.com/@vi.ai_/fine-tuning-llama-v2-7b-on-google-colab-unleashing-the-full-potential-of-language-modeling-9b9f05c3be35?source=list-412323769000--------8-------07f9e4e23822---------------------",
        "title": "Fine-Tuning LLaMA-v2–7B on Google Colab: Unleashing the Full Potential of Language Modeling",
        "subtitle": "false",
        "autorName": "vignesh yaadav",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*BVyuyXkv9sHhVycc1mW04w@2x.jpeg",
        "clap": "94",
        "response": "2",
        "timeForRead": "5 min read",
        "dateCreate": "Jul 21",
        "text": [
            "Fine-tuning a pre-existing language model is an exciting prospect as a Machine Learning Engineer, especially for unique use-cases or datasets. The practice becomes increasingly efficient and affordable because you’re not training a model from scratch but tweaking an existing one that has already been trained on a significant amount of data. Today, we’ll jump into the nitty-gritty aspects of fine-tuning the powerful LLaMA-v2–7B model on Google Colab.\n",
            "In the rapidly evolving world of Natural Language Processing (NLP), the introduction of LLaMA-v2 marks a significant milestone. This state-of-the-art language model, developed by Meta AI, is a testament to the incredible advancements in machine learning and artificial intelligence.\n",
            "LLaMA-v2, an acronym for Language Model from Meta AI version 2, is a large-scale transformer-based model that has been trained on a vast corpus of text data. It’s a powerful tool capable of understanding and generating human-like text, opening up a plethora of possibilities in various domains such as content creation, sentiment analysis, language translation, and much more.\n",
            "The model’s ability to comprehend context, generate coherent responses, and even exhibit a degree of creativity is truly remarkable. It’s like having a virtual assistant that not only understands your instructions but also adds value with its insights.\n",
            "But what makes LLaMA-v2 truly stand out is its improved performance and efficiency. The model has been fine-tuned to deliver high-quality results while minimizing computational resources, making it a practical choice for real-world applications.\n",
            "As we delve deeper into the capabilities of LLaMA-v2, we embark on an exciting journey to explore how this cutting-edge technology can revolutionize the way we interact with digital platforms. Whether you’re a seasoned AI practitioner or a curious enthusiast, the advent of LLaMA-v2 promises a fascinating exploration into the future of language models. So, let’s dive in and unravel the power of LLaMA-v2!\n",
            "Google Colab is a powerful tool offering free GPU resources. Before diving into the programming aspect, you should ensure that your notebook settings are configured for GPU usage. You can do this by navigating to ‘Runtime’ > ‘Change runtime type’ > ‘Hardware accelerator’ option at the top of your Google Colab notebook. Then set the ‘Hardware accelerator’ as ‘GPU’.\n",
            "Next, you need to install the necessary dependencies. Add code snippets to install any required packages, like so:\n",
            "With the environment setup, we can load our LLaMA v2–7B model from Huggingface’s model hub:\n",
            "If you’re using a private model, replace ‘use_auth_token=True’ with your actual Huggingface token.\n",
            "Now for the fun part: Actual fine-tuning. There are several steps to this: Defining the training arguments, creating a training dataset, and lastly, fine-tuning the model.\n",
            "These arguments give you control over how the training will proceed. You can define the learning rate, the number of epochs, etc.\n",
            "Creating a Training Dataset:\n",
            "You can load your training dataset using the HuggingFace ‘Datasets’ library.\n",
            "Replace ‘your_dataset_name_here’ with the name of your dataset. If you’re using a private dataset, replace ‘use_auth_token=True’ with your actual Huggingface token.\n",
            "Fine-tuning the Model:\n",
            "Now we create an instance of the Trainer class, pass it our model, our modified TrainingArguments, and the dataset, and then call the train method.\n",
            "Post fine-tuning, we can save the model for future use:\n",
            "you can also push your model to hugging facehub\n",
            "Fine-tuning the LLaMA-v2–7B model on Google Colab is a straightforward yet enriching project, involving several steps of model loading, defining training arguments, fine-tuning, and saving the improved model. The process boosts the prowess of pre-trained models for use-cases specific to our requirements.\n",
            "Approach 2 use TRL :\n",
            "Unleashing the Power of Fine-Tuning with TRL: Taking Language Modeling to New Heights\n",
            "TRL, developed by Hugging Face, is a cutting-edge library designed to simplify and streamline the fine-tuning process for language models. With its intuitive interface and extensive functionality, TRL empowers researchers and practitioners to fine-tune large language models like LLaMA-v2–7B with ease and efficiency.\n",
            "By leveraging TRL, we can unlock the full potential of language modeling. It provides a comprehensive set of tools and techniques for various NLP tasks, including text classification, named entity recognition, sentiment analysis, and much more. With TRL, fine-tuning LLaMA-v2–7B becomes an accessible and seamless process, enabling us to tailor the model’s capabilities to our specific needs.\n",
            "In this article, we will explore the ins and outs of the TRL library and delve into the fascinating world of fine-tuning LLaMA-v2–7B. We will uncover the key concepts, walk through the implementation steps, and showcase the remarkable results that can be achieved through this powerful combination.\n",
            "So, fasten your seatbelts as we embark on a journey to unlock the true potential of language modeling through the TRL library. Get ready to witness the transformative impact of fine-tuning LLaMA-v2–7B and take your NLP projects to new heights of performance and accuracy. Let’s dive in and explore the limitless possibilities that await us!\n",
            "Yes it’s that ez\n",
            "Note : You can use free google colab version for finetuning 7B parameter, in case of 70B you need a A100 GPU Instance\n",
            "Therefore, as a Machine Learning Engineer, embracing the fine-tuning of models is a necessary skill in today’s data-driven world. And with this guide, you’re well on your way to becoming a fine-tuning wizard! Here’s wishing you luck on your journey! Keep experimenting and happy coding!\n",
            "Remember to always experiment with different settings and datasets to achieve the best results. Happy fine-tuning!\n",
            "Disclaimer: This tutorial is for educational purposes only. Always ensure that you have the necessary permissions and resources before fine-tuning a large model like LLaMA-v2–7B.\n"
        ]
    },
    {
        "link": "https://medium.com/@gathnex/fine-tuning-llama-2-llm-on-google-colab-a-step-by-step-guide-dd79a788ac16?source=list-412323769000--------7-------07f9e4e23822---------------------",
        "title": "Fine-Tuning Llama-2 LLM on Google Colab: A Step-by-Step Guide.",
        "subtitle": "false",
        "autorName": "Gathnex",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/0*oHgD_0zXXL0OKuRY",
        "clap": "186",
        "response": "2",
        "timeForRead": "12 min read",
        "dateCreate": "Sep 18",
        "text": [
            "Llama, Llama, Llama: 🦙 A Highly Speakable Model in Recent Times. 🗣️ Llama 2: 🌟 It’s like the rockstar of language models, developed by the brilliant minds over at Meta. But what makes it so special? Well, it’s not just one model; it’s a family of models, ranging from 7 billion to 70 billion parameters. Think of it as the Swiss Army knife of AI language models. While it’s built on the trusted Google transformer architecture, Meta added their own secret sauce to make it even more powerful. Whether you’re looking to chat with an AI assistant or need a language model for a specific task, Llama 2 has you covered.\n",
            "Now, let’s talk about Llama 2’s journey to greatness. It started with some serious training — we’re talking a massive dataset of text and code, including books, articles, code repositories, and more. But what truly sets it apart is the fine-tuning process, where it learned from over 1 million human annotations. This is where it honed its skills, becoming incredibly accurate and fluent. And guess what? Llama 2 doesn’t just shine in the lab; it outperforms other open-source language models in various real-world tests. The best part is that you can use it for research and commercial applications, making it a versatile tool with boundless potential. So, buckle up, because Llama 2 is on a mission to redefine the AI landscape.\n",
            "Let’s understand the LLM’s Training process.\n",
            "There is mainly 2 steps:-\n",
            "Pre-training: It’s like teaching a language model the ABCs of language by exposing it to a massive amount of text from the 🌐 internet. Think of it as giving the model a broad understanding of grammar 📝, vocabulary, and common patterns in language . During this phase, the model learns to predict what comes next in a sentence 🤖, helping it grasp the structure of language 🧠. It’s like teaching a student the ABCs before moving on to reading books 📖.\n",
            "Fine-tuning ✨: Fine-tuning on the other hand, is where the magic happens. After the model has a general understanding of language from pre-training, fine-tuning narrows its focus. It’s like taking that well-rounded student and giving them specific lessons for a particular task. For example, you might fine-tune the model to be an expert in answering questions or generating code. It’s like guiding that student to excel in a specific subject in school. Fine-tuning adapts the general language knowledge gained during pre-training to perform specific tasks accurately and effectively.\n",
            "After Fine-tuning, the model still we had a problem. These include occasional generation of incorrect or nonsensical information, sensitivity to input phrasing, susceptibility to bias present in the fine-tuning data, and difficulty handling nuanced context in complex conversations. Additionally, models can struggle with generating coherent long-form content, which can affect their suitability for certain applications like content generation and chatbots. These limitations highlight the need for ongoing research and development efforts to refine fine-tuned models and address these issues for more reliable and ethical AI applications.\n",
            "Responsible AI is our goal🎯 not only a Fine-tuned model.\n",
            "Reinforcement Learning from Human Feedback : RLHF is like giving your language model a tutor🎓. After pre-training and fine-tuning, RLHF steps in to provide additional training. It’s a bit like having a teacher review and grade the model’s responses to improve them further. Human feedback, in the form of evaluations and corrections✅, helps the model learn from its mistakes and refine its language skills. Just as students benefit from feedback to excel in their studies, RLHF helps language models become even better at specific tasks by learning from human guidance.\n",
            "so look like lot of hard work needed for RLHF. so new buddy entering the game🎮.\n",
            "DPO Direct Preference Optimization is a new technique🤝 designed to address the limitations of Reinforcement Learning from Human Feedback (RLHF) in fine-tuning large language models (LLMs). Unlike RLHF, which relies on complex reward function learning, DPO simplifies the process by treating it as a classification problem based on human preference data.\n",
            "The Biggest Misconception: Fine-Tuning LLM on Google Colab 🧐.\n",
            "Let’s debunk a common myth 🌟: Yes, you can fine-tune a Language Model (LLM) on the free version of Google Colab, but with a catch! 🙅‍♂️\n",
            "Here’s the scoop: Google Colab offers a free environment, but there are time limits ⏳. You get a generous 12-hour window to run your code continuously after that it automatically disconnected , but here’s the twist — if there’s no activity, it disconnects after just 15–30 minutes of inactivity ⏱️. Colab also has a GPU limitation; you can only use GPUs for around 12 hours/day.\n",
            "Fine-tuning a large LLM on Google Colab’s free version? Not the easiest feat! 🤯 Due to these constraints, you might find yourself limited to fine-tuning smaller LLMs with smaller datasets, often maxing out at around 2 epochs ⚙️ with 10k samples will be difficult. So, while it’s possible, it can be quite challenging to fine-tune a substantial LLM using Google Colab’s free tier. 🚀\n",
            "We are going to use 🦙Llama-2–7B-HF, a pre-trained small model in the Llama-2 family, for fine-tuning with Qlora technique.\n",
            "QLoRA (Quantized Low-Rank Adaptation) is an extension of LoRA (Low-Rank Adapters) that uses quantization to improve parameter efficiency during fine-tuning. QLoRA is more memory efficient than LoRA because it loads the pretrained model to GPU memory as 4-bit weights, compared to 8-bits in LoRA. This reduces memory demands and speeds up calculations.\n",
            "In simple terms, we’re not going to train the entire model. 🚂 Instead, we’ll add an adapter in between the model and train only that adapter. 🧩 This way, we can fine-tune the LLM on the consumer GPU, 🎮 and it’s also a faster training process. ⏩\n",
            "The system setup we used to fine-tune a model included a Tesla V100 32GB GPU, and it ran on an Ubuntu VM. If you want to set up a similar VM for training LLMs, feel free to reach out to us Email : gathnexorg@gmail.com📧.\n",
            "Install required packages\n",
            "Import libraries\n",
            "Check system spec\n",
            "output:-\n",
            "Setting the model variable\n",
            "Log into hugging face hub\n",
            "Note : You need to enter the access token, before that you need to apply for access the llama-2 model in Meta website.\n",
            "Monitoring\n",
            "Apart from training, monitoring is a crucial part we need to consider in LLM training🚧.\n",
            "To get started, create a WandB account. Click here to log in🔗. After creating your account, enter the authorization token🔑 here.\n",
            "Load dataset\n",
            "We are utilizing the pre-processed dataset vicgalle/alpaca-gpt4 from Hugging Face.\n",
            "Loading the model and tokenizer\n",
            "We are going to load a Llama-2–7B-HF pre-trained model with 4-bit quantization, and the computed data type will be BFloat16.\n",
            "Lora config\n",
            "Training arguments\n",
            "SFTT Trainer arguments\n",
            "We’re all set to begin the training process.\n",
            "You can now monitor various training metrics, including loss, GPU usage, RAM usage, and more, directly on the WandB website. The link will be provided when you initiate above code.\n",
            "The UI look like,\n",
            "Now, at this crucial phase, it’s imperative to closely monitor the training loss. If the loss starts to exhibit unusual behaviour or anomalies🚨, it’s a signal to consider stopping the training. Overfitting is a common concern in such cases, and it may be necessary to fine-tune hyperparameters and retry to achieve the best results📉.\n",
            "Good training loss\n",
            "This is an image depicting our training loss, showcasing favourable trends. 📈While there may be occasional spikes, we have effectively applied exponential moving average to mitigate them. 🧮The total number of epochs used for training was 5. So the primary goal is being the gradual reduction of the loss curve over time🎯.\n",
            "Bad Training loss\n",
            "This training graph indicates overfitting as the training loss oscillates between 2 and 3. 🔴This could be due to issues such as inadequately pre-processed data or suboptimal hyperparameter settings🛠️ etc.\n",
            "In many articles📝, we often come across training loss curves that appear unusual and resemble overfitting, yet they are included in the blog posts🌐. This highlights the importance of exercising caution when utilizing code from the internet.\n",
            "What after training ?\n",
            "So, after training is completed, we need to save the model for testing. It only saves the trained adapter.\n",
            "Let’s test the model\n",
            "Output:-\n",
            "🤖 The results we’ve obtained 📊 reflect the model’s performance during testing. However, one of the primary challenges we’re facing with this model pertains to the stopping criteria used during its training 🛑. We’re committed to addressing this issue through ongoing research and development efforts 🔍, with the aim of identifying and implementing more optimal parameters ⚙️. While the model has demonstrated promise 🤝, it’s important to acknowledge that there is room for improvement in its performance 📈.\n",
            "Upload a model to hugging face repository\n",
            "Step 1 : Once you are finished training your model, you can use the code you provided to free this memory. This is important because it can help to prevent your computer from running out of memory, and it can also improve the performance of other programs that you are running.\n",
            "Step 2: Merging the adapter with model.\n",
            "Step 2 : Pushing the merged model to hugging face hub\n",
            "Conclusion\n",
            "In conclusion, our assessment indicates that the model’s performance is promising but falls short of being outstanding. Recognizing this, our team remains dedicated to continuous Research and Development (R&D) efforts to craft a superior model. 🌟 We are committed to providing more effective solutions for Language Models (LLMs) that cater to the needs of AI enthusiasts and practitioners. 🧠\n",
            "It’s essential to highlight that fine-tuning a model on platforms like Google Colab comes with its set of challenges. 🤯 The time limitations and resource constraints can make this task a formidable one. However, our team is actively exploring ways to navigate these difficulties, aiming to make fine-tuning on such platforms more accessible and efficient for everyone. 🕒📈\n",
            "In essence, our journey in the world of LLMs continues, fueled by the desire to deliver superior models and streamline the fine-tuning process. 💡 Stay tuned for more exciting updates! 📢\n",
            "Thanks & Regards to Gathnex team🎉.\n",
            "Additionally, we’d like to clarify that we’ve utilized certain images from the internet to enhance our explanations for the audience’s better understanding. We want to extend our credits and appreciation🙏 to the original owners of these images🖼️.\n",
            "Google colab\n",
            "Reference : Huggingface\n"
        ]
    },
    {
        "link": "https://medium.com/@bryan.mckenney/teaching-llms-to-think-and-act-react-prompt-engineering-eef278555a2e?source=list-1d38901d87ed--------0-------9653b1e942ef---------------------",
        "title": "Teaching LLMs to Think and Act: ReAct Prompt Engineering",
        "subtitle": "false",
        "autorName": "Bryan McKenney",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/0*Z_yqtikKbvwQPzeK",
        "clap": "99",
        "response": "1",
        "timeForRead": "10 min read",
        "dateCreate": "Jun 9",
        "text": [
            "Professors at Princeton University and Google researchers recently published a paper describing a novel prompt engineering method which enables large language models (think ChatGPT) to reason and act intelligently within a simulated environment. This ReAct method mimics how humans operate in the real world, as we reason verbally and can take actions to gain information. ReAct is found to perform well against other prompt engineering (and imitation learning) approaches in a variety of domains. This marks an important step towards Artificial General Intelligence (AGI) and embodied language models (robots that think like humans). Information about the paper and a link to it are below:\n",
            "In this section, I will discuss large language models, prompt engineering, and chain-of-thought reasoning.\n",
            "A large language model (LLM) is a machine learning Transformer model that has been trained on a huge corpus, or data set of texts, such as most of the webpages on the Internet. During training, which takes a lot of time (and/or GPUs), energy, and water (for cooling), gradient descent is employed to optimize the model’s parameters so that it becomes good at predicting the training data. In essence, a LLM learns to predict the most probable next word given a sequence of previous words. This can be used to perform inference — finding the likelihood that some text would be generated by the model — or text generation, which LLMs like ChatGPT use to converse with people. Once a LLM is done training, it is frozen, which means that its parameters are saved and it does not add inputs to its training data or retrain — doing so would be infeasible, and as we’ve learned from Microsoft’s Tay chatbot becoming a Nazi, it is probably better not to learn from users anyway. It is important to note that LLMs still learn bias from their training data, and OpenAI, the company behind ChatGPT, had to add safeguards — using reinforcement learning from human feedback (RLHF) — to try to prevent the model from generating problematic content. Also, since LLMs by default just generate the most likely next word based on what they’ve seen without doing any kind of fact-checking or reasoning, they are prone to hallucination, or making up facts, and reasoning errors (such as when doing simple mathematics).\n",
            "LLMs have been all the rage ever since the public release of ChatGPT took the world by storm. The emergent intelligence of these models and their applications to so many aspects of our lives have made them an incredibly popular tool, with every company wanting a piece of the action. Besides chat bots and coding and writing assistants, LLMs are being used to create agents that interact with simulated environments, including the Internet. ReAct is an example of how to turn a LLM into one such agent.\n",
            "If you’ve experimented with ChatGPT, you’ll know that sometimes it refuses to answer a question or answers it poorly, but if you rephrase the question you may get a better result. This is the art of prompt engineering — getting a LLM to respond in the way you want it to by modifying your input. The thought is that LLMs have been trained on so much human-generated data that they can almost be treated as a human — instead of training a new model on a specific problem domain, one can just try to elicit the proper response from an existing frozen LLM by bringing up some facts to “jog its memory” or telling it about a new domain. This is known as in-context learning, and there are two main types: zero-shot learning and few-shot learning. Zero-shot learning gives the LLM a prompt that could include some background information before the question/command to help the LLM find a good response. Few-shot learning gives the LLM a few examples of prompts and desirable responses and then poses a new prompt that the LLM will then respond to in the format of the examples.\n",
            "Prompt engineering is the future of Natural Language Processing (NLP). The field is shifting from customized models to customized prompts because LLMs are so much better than what anyone can make on their own without an incredible amount of time and energy. When a LLM is paired with the right prompt engineering technique, more often than not, it can do anything that a specialized model can.\n",
            "Chain-of-thought reasoning is a popular prompt engineering technique that is meant to combat reasoning errors. It involves giving the LLM one or more examples (few-shot learning) of how to verbally reason through a problem and then giving it a different problem to solve in that way. This can help with reasoning errors, but it still suffers from hallucination, and hallucinated “facts” can propagate through the reasoning, causing the model to come to the wrong conclusion regardless.\n",
            "In the image from the ReAct paper below, standard prompting — just asking a question — is compared to chain-of-thought (CoT) prompting (although the additional inputs are not shown) on a question that requires multiple steps of reasoning to figure out. The LLM with standard prompting guesses iPod, which is incorrect. The LLM with CoT prompting has a much more convincing response, but it is still wrong. Despite having flawless reasoning, the LLM hallucinates that the Apple Remote was originally designed to work with the Apple TV (it was actually designed for the Front Row program), which leads it to the wrong conclusion.\n",
            "Because of the issue of hallucination, CoT reasoning is unreliable. If LLMs are to be a useful tool, they cannot be making up facts left and right, because then we can never trust them and are better off just doing the research ourselves. ReAct aims to solve this issue by allowing the LLM to take actions such as searching Wikipedia so that it can find facts and reason from those.\n",
            "Like chain-of-thought reasoning, ReAct is a prompt engineering method that uses few-shot learning to teach the model how to solve problems. CoT is supposed to imitate how humans think about problems, and ReAct also includes this reasoning element, but it goes further by allowing the agent text actions that let it interact with its environment as well. Humans use verbal reasoning (speaking or thinking) to help us strategize and remember things, but we can also take actions to gain more information and achieve our goals. This is the foundation of ReAct. A ReAct prompt includes examples with actions, the observations gained by taking those actions, and the transcribed thoughts (reasoning strategies) of the human at various steps in the process. The LLM learns to emulate this approach of interleaved thinking and acting, making it an agent in its environment. Below is an illustration of how a ReAct agent operates, with a tragic example (in Thoughts -> Action -> Observation order) shown in monospaced font.\n",
            "It is important to remember that the observations are not generated by the LLM but by the environment, which is a separate module that the LLM can interact with only through specific text actions. Therefore, in order to implement ReAct, you need:\n",
            "The number of examples and the details of them is up to you. The beginning of an example used in a ReAct prompt is shown below.\n",
            "Here you can see that the thoughts, actions, and observations are clearly labeled as such and that the actions use a special format — with the query in brackets — so that the agent will learn to write them in this way and the output parser can then easily extract the query.\n",
            "For their frozen LLM, Yao et al. (2023) use PaLM-540B. They test ReAct prompting with this LLM on two knowledge-intensive reasoning tasks and two decision-making tasks. I will discuss each in turn.\n",
            "The two domains used in this task area are HotPotQA, which is multi-hop question answering using Wikipedia passages, and FEVER, which is fact verification. The agent is given the ability to interact with a purposefully simple Wikipedia API using the following actions:\n",
            "In these domains, ReAct is compared to the following techniques:\n",
            "Success is measured by accuracy in FEVER and EM in HotPotQA. The plots below show the results in each domain as a function of the number of sampled responses for CoT-SC.\n",
            "ReAct did poorly in HotPotQA but outperformed CoT in FEVER. ReAct is much less prone to hallucination than CoT, but has a higher reasoning error rate. Although ReAct does have this shortcoming, the ReAct -> CoT-SC and CoT-SC -> ReAct methods are the most successful of the bunch. Below is the same question from the beginning of this article with ReAct’s response, which is correct.\n",
            "The two domains used in this task area are ALFWorld and WebShop. I shall explain each domain individually.\n",
            "ALFWorld is a text-based game with realistic environments. It has text actions for moving around in and interacting with the simulated world, such as “Open drawer 1.” A goal given to the agent might be to find a specific object in a house, and therefore common-sense reasoning is helpful to know where such an object would commonly be found. The baselines ReAct is compared to in this domain are:\n",
            "The measure of success is the percentage of trials where the goal was reached. ReAct outperforms the baselines.\n",
            "WebShop is a simulated online shopping website with data crawled from Amazon. It is a challenging domain because it has a large number of actions for navigating the website and searching for products. The goal is to find an item that matches a user’s specifications. The baselines ReAct is compared to in this domain are:\n",
            "The measure of success is how close the chosen item is to the hidden one that the user had in mind. ReAct outperforms the baselines.\n",
            "ReAct, although not perfect by itself due to its reasoning errors, is nonetheless a strong prompt engineering approach that overcomes the fact hallucination issue of chain-of-thought reasoning and also allows the LLM to become an agent that can interact with its environment. In addition, it is a very interpretable method, since the agent outputs its thought process as it acts.\n",
            "I believe that ReAct is a step towards Artificial General Intelligence (AGI) and embodied language models (robots that think like humans). If a robot had a method of modeling a foreign environment based on familiar features and creating prompts using that model, it could (at least attempt to) act by itself in a large variety of domains without the need for human-crafted examples. It would also need a memory of some sort, or the ability to learn from its experiences, to make it even more human-like. It is unclear at this point whether the creation of AGI would help or hurt humanity, but robots with common-sense knowledge, as long as bugs like reasoning errors and hallucinations are worked out, could be a great help to us (as firefighters, for instance).\n",
            "LLM agents have already been commercialized and are being used for a variety of tasks, from creating websites to ordering pizza. There are also non-commercial applications, such as destroying humanity. I only hope that these tools can be used for good as well. An agent with the goal of figuring out how to solve the world’s problems might be nice.\n"
        ]
    },
    {
        "link": "https://medium.com/@ogbanugot/notes-on-fine-tuning-llama-2-using-qlora-a-detailed-breakdown-370be42ccca1?source=list-ce01ef2736c2--------5-------847f40a2cdd4---------------------",
        "title": "Notes on fine-tuning Llama 2 using QLoRA: A detailed breakdown",
        "subtitle": "false",
        "autorName": "Ogban Ugot",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*dlbCP8yafJEVrzsWs071qQ.jpeg",
        "clap": "1",
        "response": "3",
        "timeForRead": "23 min read",
        "dateCreate": "Sep 18",
        "text": [
            "I’ve been following recent developments around LLMs but not really tinkering with any of the open-source models, libraries, etc. And if you have been following then you know there has been a lot of stuff coming out lately.\n",
            "While trying to play with this code I realized that there is so much to keep up with, I found myself asking a lot of questions, when did this library come out? what more does it offer? What technique does this class implement?\n",
            "In this article, I will attempt to present some answers to these questions in the style of detailed notes. The questions are specific to the code, therefore, most of the notes are specific to questions regarding the open-source libraries involved and the methods and classes used. I will discuss some of the theoretical aspects of fine-tuning an LLM, but I won’t dive too deep, just summaries. While presenting these notes I will share many links to detailed discussions about topics like weight quantization, parameter-efficient fine-tuning techniques, library documentation, papers, etc. Be sure to check out all the links, if you cannot read them immediately bookmark them for later.\n",
            "At the end of the article, if you were like me feeling a bit lost with all the recent developments in the fine-tuning-open-source-LLMs space, you’ll probably have a better understanding of how everything fits together.\n",
            "Let me first summarize what exactly the code is about. It shows us how to fine-tune Llama 2–7B (you can learn more about Llama 2 here) on a small dataset using a finetuning technique called QLoRA, this is done on Google Colab Notebook with a T4 GPU. Both the model and the dataset are available from HuggingFace. I’ll take the code in sections and then present important notes on the relevant lines in the code as well as any other context needed.\n",
            "The first thing I noticed was that there were some unfamiliar Python libraries required.\n",
            "As you can see above we have to first install accelerate, peft, bitsandbytes, transformers, and trl. Honestly, the only library I was familiar with here was transformers, so I’m going to provide some notes on each of these libraries and what they do.\n",
            "transformers: The transformers library is definitely the oldest library here with the earliest version (2.0.0) on PyPI dating back to 2019. It’s a huggingface library for quickly accessing (downloading from hugging’s API) machine-learning models, for text, image, and audio. It also provides functions for training or fine-tuning models and sharing these models on the HuggingFace model hub. The library doesn’t have abstraction layers and modules for building neural networks from scratch like Pytorch or Tensorflow. Instead, it provides training and inference APIs that are optimized specifically for the models provided by the library. The repository has more info on why you should or shouldn’t use transformers for your ML project. You can also see a list of the available models provided by transformers here. It is safe to say that transformers is one of the key Python libraries for LLM finetuning simply because of its easy toolbox for accessing open-source models. Indeed the huggingface community (and the company) should be given much credit for the work they are doing as one of the major players in democratizing open-source machine learning. I love this post that says that by sorting the models on HuggingFace by downloads, you can get a pretty good idea of which models are easiest to work with.\n",
            "bitsandbytes: bitsandbytes is a relatively newer library with the earliest release on PyPI dating back to 2021. It is a lightweight wrapper around CUDA custom functions, specifically designed for 8-bit optimizers, matrix multiplication, and quantization. It provides functionality for optimizing and quantizing models, particularly for LLMs and transformers in general. It also offers features such as 8-bit Adam/AdamW, SGD momentum, LARS, LAMB, and more. I believe that the goal for bitsandbytes is to make LLMs more accessible by enabling efficient computation and memory usage through 8-bit operations. By leveraging 8-bit optimization and quantization techniques we improve the performance and efficiency of models. If you have been following the conversation around open-source LLMs, you probably know that there is a memory bottleneck concerning running LLMs on smaller-sized consumer GPUs such as the RTX 3090. Therefore there has been a surge in interest in weight quantization techniques that attempt to reduce the memory requirements for running LLMs. The idea is to quantize the floating point precision of the weights of the model from larger precision points like FP32 to a smaller precision like Int8 (a 4x4 Float16). There are techniques to quantize say an FP32 to Int8 including absmax and zero-point quantization but due to limitations with these techniques, the creator of the bitsandbytes library coauthored the LLM.int8() paper as well as 8-bit Optimizers to provide efficient quantization methods for LLMs. The bitsandbytes library thus provides these quantization techniques as an open-source library. Earlier I said that we’ll be running the code for fine-tuning Llama 2 which is a 7B parameter model on a free Colab T4 GPU. This is largely possible due to quantization techniques provided by the bitsandbytes library. Here is a great article on weight quantization and another from HuggingFace.\n",
            "Peft: Weight quantization will allow us to reduce the memory requirements of loading an LLM (or parts of it) into working memory for fine-tuning. However, there is still the problem of efficiently fine-tuning LLMs. Unlike transfer learning techniques with smaller deep learning models, where you simply had to freeze the lower layers of a neural network like AlexNet and then fully fine-tune the classification layers on a new task, with LLMs there are huge prohibitive costs to performing such full fine-tuning. Parameter Efficient Fine-Tuning (PEFT) methods are a set of methods for adapting LLMs for downstream tasks such as summarization or question-answering on memory-constrained devices such as the T4 GPU (the T4 provides 16GB VRAM). The motivation for these methods is that we can fine-tune efficiently, parts of the LLM and still achieve comparable results than if we do a full fine-tuning. There are a few of these methods such as LoRA and Prefix Tuning that are quite successful and widely used in the literature. The peft library is a HuggingFace library that provides these fine-tuning methods, it’s a new library dating back to January 2023. In this tutorial, we’ll be using the QLoRA technique which is a low-rank adaptation or fine-tuning technique for quantized LLMs.\n",
            "trl: Another HuggingFace library, trl has been around (since 2021) but active development peaked in January 2023 and has continued to peak. TRL stands for Transformer Reinforcement Learning and is a library that provides implementations of different algorithms in the various steps for training and fine-tuning an LLM. Including the Supervised Fine-tuning step (SFT), Reward Modeling step (RM), and the Proximal Policy Optimization (PPO) step. trl also has peft as a dependency so that you can for instance use an SFT Trainer with a PEFT method such as LoRA see here for example. The utility of using TRL (as described in the repository) is that you can perform the full end-to-end training of an LLM using the trainers provided.\n",
            "datasets: Finally, although not included in our list of installed packages from earlier (it comes installed with transformers), the datasets library is another one from the huggingface ecosystem. The library is important for one-line dataloaders for many public datasets hosted on the HuggingFace dataset hub as well as for efficient data pre-processing.\n",
            "These libraries complement each other and are certainly crucial for any sort of work with LLMs. I have made a simple image to summarize how these libraries fit together below.\n",
            "You can access all the HuggingFace library documentation from here.\n",
            "Next, let’s take a look at the imports\n",
            "Judging from some of the names of the imports and the context of what each of the libraries does, you might have an idea of what we’ll be using these imports for. But to be sure, let’s take some notes on each import, we’ll keep it simple for now focusing only on what they do. We’ll dive deep into details about the initialization parameters later on.\n",
            "torch: I’m sure you are familiar with the Pytorch machine learning library. Usually, you’d import torch to build neural networks and train them using torch’s data classes and optimizers. Here, however, we won’t need those kinds of low-level functionality from torch because we are primarily using other libraries like transformers and trl for that. We’ll be using torch here to get dtypes (data-types) like torch.float16 and also to check device capability (more on this later). That’s all.\n",
            "load_dataset: load_dataset does exactly what the name implies, but one detail is that it loads our dataset from the HuggingFace dataset hub here. So it’s an online loader, but it’s efficient and simple, requiring just one line of code.\n",
            "AutoModelForCausalLM: Recall that we are accessing the Llama 2–7b model that we’ll be fine-tuning from the HuggingFace model hub using the transformers library. More specifically we’ll be accessing this particular model here. The transformers library provides a set of classes called Auto Classes that given the name/path of the pre-trained model, can infer the correct architecture and retrieve the relevant model. This AutoModelForCausalLM is a generic Auto Class for loading models for causal language modeling. Note that there are two types of language modeling, causal and masked. Causal language models include; GPT-3 and Llama, these models predict the next token in a sequence of tokens to generate semantically similar text to the input data. The AutoModelForCausalLM class will retrieve the causal model from the model hub and load the model weights thus initializing the model. The from_pretrained() method does this for us.\n",
            "AutoTokenizer: The AutoTokenizer allows for easy tokenization of text data. It specifically provides a convenient way to initialize and use tokenizers for different models without needing to specify the tokenizer class explicitly. Since it is also a generic Auto Class it can automatically select the appropriate tokenizer based on the model name or path provided. The tokenizer converts input text into tokens, which are the basic units of text used by NLP models. It also provides additional features like padding, truncation, and attention masks. Overall, the AutoTokenizer simplifies the process of tokenizing text data for NLP tasks using transformer models. We can see how we initialize the AutoTokenizer below, later on, we’ll see how the SFTTrainer takes the initialized AutoTokenizer as a parameter.\n",
            "BitsAndBytesConfig: As you know by now, we’ll be using bitsandbytes for quantization. The transformers library recently added full support for bitsandbytes so using the BitsandBytesConfig you can configure any of the quantization methods that bitsandbytes offers such LLM.int8, FP4, and NF4. The way this works is that you pass a quantization configuration to your AutoModelForCausalLM initializer so that it makes use of the configured quantization method to load the model weights.\n",
            "TrainingArguments: TrainingArguments’s utility is pretty straightforward. It is a data class for storing all the training arguments for the SFTTrainer. The SFFTrainer takes different types of arguments that are not necessarily specific to training. So, TrainingArguments helps us to organize all the related training arguments into a single data class and keeps the code clean and organized. Also, there are a bunch of nice utilities that can be used with TrainingArguments, for instance using HfArgumentParser we can create an argument parser for TrainingArguments that is useful for CLI applications. In the code below, we’ll pass training_arguments to SFFTrainer later.\n",
            "pipeline: We’ll be using pipeline for inference after we are done with fine-tuning. I think the pipeline is a great utility, there is a list of various pipeline tasks (see here) you can choose from like, “Image Classification”, “Text Summarization” etc. You can also select a model to use for the task, but you can also choose not to and the pipeline will use a default model for the task. You can add an argument that does some form of preprocessing like tokenization or feature extraction. For reference here is how we initialize a pipeline for inference later on:\n",
            "logging: The final import from transformers is logging. It’s a centralized logging system to set up the verbosity of the library. There are various levels you can set like CRITICAL, ERROR, INFO etc. It’s a global logging system that is very useful when debugging your transformers code.\n",
            "LoraConfig: From the peft library we import the LoraConfig data class. LoraConfig is a configuration class to store configurations required to initialize the LoraModel which is an instance of a PeftTuner. We’ll then pass this config to the SFTTrainer it will use the config to initialize the appropriate Tunerwhich again, in this case, is the LoraModel.\n",
            "PeftModel: Once we fine-tune a pre-trained transformer using one of the peft methods such as LoRA, we can save the LoRA adapter weights to disk as well as load them back into memory. PS: Adapters are basically the weights that PEFT modules fine-tune, these are separate from the base-model weights. Using PeftModel, you also have the option of loading the adapter weights into memory and then merging (adapting) the base_model weights with the newly fine-tuned adapter weights. This is precisely what we’ll use PeftModel for, we’ll use PeftModel.from_pretrained()to load the adapter weights from memory and merge them with the base_model using merge_and_unload().Here is what this looks like in the code.\n",
            "SFTTrainer: Finally, the last import, and arguably the most important is the SFTTrainer from trl. The SFTTrainer is a subclass of transformers Trainer class. Trainer is a feature-complete training API for transformer models. SFTTrainer builds on this with added support for parameter-efficient fine-tuning. The supervised fine-tuning step is a key step in training causal language models like Llama for downstream tasks like instruction-following. For example, the dataset for this tutorial is a small instruction-following dataset with 1k examples. The key idea behind supervised fine-tuning is that the model is trained on a set of validated responses that the model can emulate, that is a set of input-output pairs. Again recall that SFTTrainer supports PEFT, so we will use the SFTTrainer with LoRA. SFTTrainer will then perform the supervised fine-tuning using LoRA. We can then run the trainer (train()) and save the weights as well (save_pretrained()).\n",
            "Okay, so we now know what libraries we need to fine-tune Llama 2 (or any LLM really), we know the classes we need from these libraries and we have context about what those classes do. Next, let’s look at the parameters that go into all those classes we imported earlier. This will also shed more light on what they do.\n",
            "Model and dataset names:\n",
            "First, in lines 2, 5, and 8 we define the model_name , the dataset_name and the new_model . These names follow the format of the HuggingFace model and dataset names on their hub. So for example given a name like “NousResearch/Llama-2–7b-chat-hf” the first part of the name NousResearch is a research organization that has a HuggingFace account where they upload open-source models. Anyone can have an account, and anyone can upload a model. In the last part of this tutorial, you upload the fine-tuned model to HuggingFace under your own account. The second part of the name is the model name Llama-2–7b-chat-hf. Here it is good to give your models descriptive names that include useful info like the distinct model name (Llama-2), key parameter info (7b), and some other useful info about how the model works (chat-hf). We see the same thing with the new_model name llama-2–7b-miniguanaco this is the name we assign to the fine-tuned model. Here we append the name of the dataset that we fine-tuned on miniguanaco . When you upload the fine-tuned model to HuggingFace the name will look like <your-account-name>/llama-2–7b-miniguanaco . Together, the account name and the model are referred to as the path . The dataset is from the path mlabonne/guanaco-llama2–1knotice that the dataset path follows the same format as the model path.\n",
            "QLoRA Parameters:\n",
            "In the code there are brief comments about what these parameters are but why do we set those particular values? The QLoRA parameters go into your LoraConfig , there are other parameters not used here so be sure to click the link and check them out in the docs. The parameters we’ll make use of are r (lora_r) , lora_alpha and lora_dropout . These parameters are the most essential for LoRA , to understand why, we’ll have to dive into the LoRA paper here. Let me try to summarize. In neural networks, the backpropagation algorithm calculates the error between the expected value and the actual value, this error eis then used to calculate the delta which is the contribution to e from the weights in the neural network. So if you have the initial weights of a neural network W0 then with respect to the error e we calculate delta_W0 = ∆W . You then use ∆W to update the weight W0 + ∆W in order to reduce the error e . LoRA proposes that ∆W can be decomposed into two sets of Low-Rank matrices A and B such that W0 + ∆W = W0 + BA . Instead of using the full ∆W update, we use the smaller low-rank update matrix BA , this is how we achieve efficiency and lower computational requirements. If the size of ∆W is (d x k) (the size of W0 ) then we decompose ∆W into two matrices: B and A, with dimensions (d x r) and (r x k) , where r is the rank. The parameter r (lora_r)in LoraConfig is the rank that determines the shape of the update matrices BA . According to the paper, you can set a small rank and still get excellent results. When we update W0 we can control the impact of BA by using a scaling factor α , this scaling factor acts as a learning rate. The scaling factor is our second parameter (lora_alpha) . Finally, we set our lora_droput , which is a typical dropout rate for regularization.\n",
            "BitsandBytes Parameters:\n",
            "The bitsandbytes parameters go into the BitsandBytesConfig. Recall that we are using a quantized version of LoRA known as QLoRA you can read more about QLoRA in the paper here. What this means is that we want to use quantization for our LoRA fine-tuning, applying quantization to (amongst other things) the update weights we spoke about earlier. To understand more about floating point precision and the difference between the floating point quantization types FP4 (floating point 4) and NF4 (normalized float 4) read these articles here and here. We set the param use_4bit (line 6)to True to use high-fidelity 4-bit fine-tuning, this was later introduced in the QLoRA paper to achieve even lower memory constraints than the 8-bit quantization introduced in the LLM.int8 paper. We set the bnb_4bit_compute_dtype (line 9)which is the data type (float16) that computation is performed with. That is while 4-bit quantization stores the weights in 4-bit to reduce memory usage, the computation happens in 16 or 32-bit and any combination can be chosen (float16, bfloat16, float32, etc.). With bnb_4bit_quant_type (line 12)we set nf4 which has shown better theoretical and empirical performance according to the QLoRA paper. We set the final parameter defined on line 15 use_nested_quant to False and pass it to bnb_4bit_use_double_quant . Setting this parameter to True will enable a second quantization after the first one to save an additional 0.4 bits per parameter. This is useful if you have serious memory problems. In our case for this tutorial, after we choose NF4 quantization with FP16 (float16) precision for computation we should have no memory constraints on the Colab T4 GPU (16 GB VRAM). To see how this works, if we use Llama-2–7B (7 billion params) with FP16 (no quantization) we get 7B × 2 bytes = 14 GB (VRAM required) . Using 4-bit quantization we get 7B × 0.5 bytes = ~ 4 GB (VRAM required) .\n",
            "Training Arguments:\n",
            "The training arguments contain the bulk of arguments defined. We’ll take brief notes on these arguments one after the other:\n",
            "output_dir (line 6): here we set where the model predictions and checkpoints are stored. If you use a visualization tool like tensorboard, this is where training logs are stored and retrieved for visualization.\n",
            "num_train_epochs (line 9): We can set a training epoch of 1 because our dataset is not very large (1K samples). Each epoch contains 250 steps.\n",
            "fp16 and bf16 (lines 12 & 13): We set both of these to false as we won’t be using mixed-precision training (read about mixed-precision training here) to reduce memory requirements. We have QLoRA for that already.\n",
            "per_device_train_batch_size & per_device_eval_batch_size (lines 16 & 19): We set both of these to 4. Usually, you can set a higher batch size (>8) if you have enough memory, this will speed up training.\n",
            "gradient_accumulation_steps (line 22): “gradient accumulation steps” refers to the number of forward and backward passes (update steps) you perform before actually updating the model weights. During each of these forward and backward passes, gradients are computed and accumulated over a batch of data. After accumulating gradients for the specified number of steps, you then perform a backward pass that computes the average gradient over those steps and updates the model weights accordingly. This approach helps in effectively simulating a larger batch size for gradient updates, which can be beneficial when you have GPU memory constraints or want to stabilize training. It reduces the memory requirements for each individual forward and backward pass while still allowing you to update the model’s weights. The default value is 1, but you can play with higher steps to see how this improves training performance.\n",
            "max_gradient_norm (line 25): Gradient clipping involves scaling down the gradients if their norm (magnitude) exceeds a certain threshold, which is specified by the max_grad_norm parameter. If the gradient norm is greater than max_grad_norm, the gradients are scaled down so that their norm becomes equal to max_grad_norm. If the gradient norm is already below max_grad_norm, no scaling is applied. It’s advisable to start with a higher value for max_grad_norm and then slowly scale it down over multiple training iterations to see how this affects performance. However, for more conservative control over the training loss (especially since we have a small dataset), we can start with a lower threshold like 0.3.\n",
            "learning_rate (line 28): The learning rate for AdamW . AdamW is a variant of the popular Adam optimizer. It combines techniques from both the Adam optimizer and weight decay regularization. The learning rate is used by the AdamW optimizer to determine the step size for updating the model’s parameters during training.\n",
            "weight_decay (line 31): Weight decay, also known as L2 regularization or weight regularization, is a regularization technique commonly used in machine learning and deep learning to prevent overfitting of a model to the training data. It works by adding a penalty term to the loss function that encourages the model’s weights to be small. It makes sense that we are using AdamW and weight decay because weight decay can be especially useful during fine-tuning because it helps prevent overfitting and ensures that the model adapts to the new task while retaining some of the knowledge from the pre-training. However, the value for our weight decay (0.001), imposes a relatively mild penalty to the loss function, larger values would impose a stronger penalty.\n",
            "optim (line 34): Although by default we use the AdamW optimizer, here we specify a variant of the optimizer to use. We use the “paged_adamw_32bit” optimizer but unfortunately, I couldn’t find detailed information specifically about the Paged AdamW 32-bit optimizer itself. It seems to be a specific implementation or variant of the AdamW optimizer so if you have info on this please leave it in the comments, thanks!\n",
            "lr_scheduler_type (line 37): Typically we use a learning rate scheduler during the training of deep learning models, to adjust the learning rate over time. The comment (line 36) says constant is a bit better than cosine why? A constant learning rate remains the same throughout the training process while a cosine learning rate is usually used with a cyclical learning rate and oscillates between a maximum and minimum value. You should vary the training between the two types to see which one performs better.\n",
            "warmup_ratio (line 40): Here we set the “warmup_ratio” to 0.03. Since each epoch has 250 training steps, the warm-up phase will last for the first ~8 steps (3% of 250), during which the learning rate will linearly increase from 0 to the specified initial value 2e-4 . Warm-up phases are often used to stabilize training, prevent gradient explosions, and allow the model to start learning effectively.\n",
            "group_by_length (line 44): We set this parameter to True and the comment says it speeds up training considerably, why? When group_by_length is set to True, it groups together samples of roughly the same length from the training dataset into the same batch. This means that sequences with similar lengths are grouped together, reducing the amount of padding required. In other words, batches will have sequences of more similar lengths, which minimizes the amount of padding applied. I suspect that the way this can improve training speed is that it enables efficient GPU utilization because GPU processing is typically more efficient when batches have consistent sizes because it allows for parallelism. When batches are more uniform in length due to grouping by length, the GPU can process them more efficiently, leading to faster training times.\n",
            "save_steps and logging_steps (lines 47 and 50): Here we set both params to 25 to control the interval steps at which to log training information and save checkpoints.\n",
            "SFTTrainer Parameters:\n",
            "The final batch of parameters is specific to the SFTTrainer.\n",
            "max_seq_length: Setting max_seq_length to None allows us not to impose a maximum sequence length limit since our dataset contains sequences of different lengths. In this case, we don’t want to truncate or pad them to a fixed length so setting max_seq_length to None allows us to work with the full range of sequence lengths present in the data.\n",
            "packing: According to the docs this parameter is used by the ConstantLengthDataset to pack the sequences of the dataset. Setting packing to False in the context of the ConstantLengthDataset can increase efficiency when dealing with multiple short examples, which is the case with our dataset. When packing is set to True, the ConstantLengthDataset packs sequences in a way that each input sequence contains a single example. This means that each input sequence corresponds to one example from the dataset, and these sequences are often padded to a fixed length if they are shorter than the specified maximum sequence length (max_seq_length). By setting packing to False, we allow the ConstantLengthDataset to pack multiple short examples into a single input sequence, effectively combining them. This reduces the need for extensive padding and increases the efficiency of memory usage and computation.\n",
            "That’s it for the needed parameters. Seeing as we have already dived into the various imports earlier (and what they do) and we just covered the parameters required to initialize them. I believe we have covered the bulk of the work. Next, we’ll see how everything fits together over the next few sections.\n",
            "We’ll start by loading the dataset on line 5. Then on line 9, we use the getttr function to set compute_dtype to torch.float16 . On line 10 we initialize our BitsandBytesConfig .\n",
            "On line 17 we check the compatibility of the GPU with bfloat16 using the function torch.cuda.get_device_capability() . The function returns the compute capability of a CUDA-enabled GPU device. The compute capability represents the version and features supported by the GPU. The function returns a tuple of two integers, (major, minor) which represent the major and minor compute capability scores of the GPU. The major score indicates the major version of the compute capability, while the minor score indicates the minor version. For example, if the function returns (8, 0), it means that the GPU has a compute capability of version 8.0. The major score is 8, and the minor score is 0. If the GPU is bfloat16 compatible then we set compute_dtype to torch.bfloat16 instead because bfloat16 ensures better precision than float16(read this).\n",
            "Next, we load our base model using AutoModelForCausalLM.from_pretrained just like we discussed earlier. On line 31 we set model.config.use_cache to False , when the cache is enabled, the model’s forward pass behavior can be less variable, as it reuses cached results. Disabling caching introduces a degree of randomness in terms of the order in which computations are performed, which can be useful when fine-tuning see here and here. On line 32 we set model.config.pretraining_tp = 1 the tp there stands for tensor-parallelism and according to tips for Llama 2 here:\n",
            "Next, we load our Llama tokenizer, using the model_name . If you look at the files for NousResearch/Llama-2 here you’ll notice there is a tokenizer.model file. Using the model_name the AutoTokenizer is able to download that tokenizer.model file. On line 36 we call add_special_tokens({‘pad_token’: ‘[PAD]’}) this is another important tip since the text in our dataset can vary in length, sequences within a batch may have different lengths. To ensure that all sequences in a batch have the same length, padding tokens are added to the shorter sequences. These padding tokens are typically tokens that don’t carry any meaning, such as <pad> or <PAD>. On line 37 we set tokenizer.pad_token = tokenizer.eos_token , by doing this we align the pad token with the EOS token, and we make our tokenizer configuration more consistent. Both tokens (pad_token and eos_token) have a role in indicating the end of a sequence. When they are the same, it simplifies tokenization and padding logic. On line 38 we set the padding side, that is the side where the padding tokens are added to make the batch sequences the same length. This is counter to this, however, setting the padding side to right fixes an overflow issue discovered here. Finally, on line 41, we initialize our LoraConfig , refer back to our discussion on LoraConfig from earlier if you need a refresher.\n",
            "On line 2 we initialize our TrainingArguments using the parameters we discussed in detail earlier. Then we pass TrainingArguments into our SFTTrainer on line 30 along with the other relevant parameters we discussed. One parameter we didn’t discuss though is dataset_text_field=”text” on line 27. In summary, the dataset_text_field parameter is used to indicate which field in the dataset contains the text data that serves as input to to model. It enables the datasets library to automate the creation of a ConstantLengthDataset based on the text data in that field, simplifying the data preparation process and ensuring efficient training. Mind you, this is all possible because we are using a HuggingFace formatted dataset. If you haven’t noticed by now, the HuggingFace ecosystem is a tight-knit ecosystem of libraries that automate a lot of work behind the scenes for you.\n",
            "On line 6 we see the pipeline initialization we discussed earlier. Then we use the pipeline on line 7 by passing our input text constructed using our prompt from line 5. We use <s> to indicate the start of the sequence while [INST] and [/INST] are added as control tokens to indicate the start and end of user messages. Read this chat templating guide to learn more about constructing chat inputs using control tokens.\n",
            "On line 2, we use the AutoModelForCausalLM.from_pretrained to (re)load the base model, of course, we’ll do this without any quantization configurations because we are not fine-tuning it, we just want to merge it with the adapters. Earlier when we discussed PeftModel I talked about why we use it on line 10 tomerge_and_unload the base_model with the new_model (the fine-tuned adapter weights). We also reload the tokenizer on line 13 and make the same modifications we made earlier from lines 13–14.\n",
            "Finally, we can then upload both our newly fine-tuned model and its tokenizer to HuggingFace.\n",
            "This was a long one but I’m sure if you made it all the way here then it was worth it. Even though it may seem like we covered a lot, we actually just scratched the surface of so many topics. But it is a good start because we can take most of the things we learned here and apply them to the task of fine-tuning any LLM. With respect to fine-tuning Llama 2, you can find more info on some interesting next steps here. For me, some of the things on my mind now are, how can we properly evaluate our fine-tuning performance? Can we fine-tune a larger model (maybe 70B) without spending too much? Working with larger datasets? What about model deployment? I’ll cover some of these next time.\n",
            "Thanks for reading!\n"
        ]
    },
    {
        "link": "https://medium.com/@indirakrigan/a-purposeful-rendezvous-with-milvus-the-vector-database-2acee4da25e2?source=list-499d6e31470a--------7-------aa9880ad746d---------------------",
        "title": "A purposeful rendezvous with Milvus — the vector database.",
        "subtitle": "false",
        "autorName": "Indira KriGan",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*jEYX7yQie2UyPipxU62ZAA.jpeg",
        "clap": "2",
        "response": "1",
        "timeForRead": "4 min read",
        "dateCreate": "Jul 23",
        "text": [
            "One of the important components in using the RAG — retriever augmented generator- method for implementing a QnA use case using large language models, is the vector database, assuming we don’t use a managed service like AWS Kendra or say Azure Semantic Search as the retriever.\n",
            "The most (most) crucial component in the above method is the embedding algorithm used to generate the embeddings but today I am just jotting down how I have been using Milvus — right from installation to using it with and without the support from Langchain. Again, there are multiple options available like Pinecone , Chroma , Weaviate etc along with libraries like Annoy or FAISS ; but this post is all about Milvus.\n",
            "Installation\n",
            "Very straightforward — download the yml file\n",
            "and run docker-compose.\n",
            "[ the only issue I faced on my MacOS was that docker-compose was not found :-| , updated docker and it started working! ]\n",
            "Time for some theory\n",
            "The main purpose of a vector database is to be able to store and index vectors in such a way that when a query is asked, it is able to quickly retrieve those ‘documents’ that are most similar to the query being asked. So, if we want to think of the design considerations, we should typically be looking at —\n",
            "2. what kind of algorithm is used while constructing the index like HNSW, FLAT etc.\n",
            "3. what metric do you use to find the similarity between the query and the base documents , typically L2 or inner product\n",
            "We have some fantastic documentation available in the Milvus website on how to go about using it. I am presenting two ways of using Milvus — one directly and the other one using Langchain ( which is a lot cooler :) )\n",
            "Using Milvus directly.\n",
            "Step 1,2,3: You need to create what is called a “Collection “ — for which you a to define a schema ( field schema ) like so:\n",
            "Now that we’ve created a Collection, let’s look at what goes into it.\n",
            "Step 4 : Using SentenceTransformers to generate the embeddings :\n",
            "Step 5 : Insert into the db and create the index by providing the parameters.\n",
            "Step 6 : Do a search on the encoded query:\n",
            "Step 7 :Use it like so:\n",
            "Using Langchain:\n",
            "The same thing using langchain, is amazingly simpler —\n",
            "The output looks like this :\n",
            "Though this kind of solves the problem of retrieval, one of the main requirements that I hear about is on access — authentication and authorisation (RBAC). This post will get way too long with that part, I should hopefully be able to cover that soon.\n",
            "Please find the code that I had used in the link below-\n",
            "Git Link : https://github.com/indi4u/LLM/blob/main/using-Milvus-vector-db.ipynb\n",
            "I had used the file from this link for the working.\n",
            "You can find a lot more information on Milvus params in their website :\n",
            "— https://milvus.io/docs/index.md\n",
            "— https://milvus.io/docs/create_collection.md\n",
            "— https://milvus.io/docs/metric.md\n",
            "Langchain link : https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/milvus#:~:text=Milvus%20is%20a%20database%20that,Milvus%20instance%20up%20and%20running.\n",
            "Thanks for reading, see you soon with another post.\n"
        ]
    },
    {
        "link": "https://medium.com/@fareedkhandev/prompt-engineering-complete-guide-2968776f0431?source=list-590702974d3e--------23-------4cad45bc2839---------------------",
        "title": "Prompt Engineering Complete Guide",
        "subtitle": "false",
        "autorName": "Fareed Khan",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ujdMB17AE56yPSA3zeZcNA.jpeg",
        "clap": "376",
        "response": "10",
        "timeForRead": "14 min read",
        "dateCreate": "May 24",
        "text": [
            "You may be wondering why prompt engineering is necessary when you already know how ChatGPT works and can communicate with it to get answers. However, consider this example:\n",
            "I asked GPT to sum up the odd numbers and tell me whether the result is even or not. Unfortunately, it didn’t give me the correct answer. The reason behind this failure lies in the prompt that I used. You might be thinking that I could easily pass a better prompt for this particular problem, but imagine a larger or more complex scenario where many people struggle to generate a solution from ChatGPT.\n",
            "In such cases, the prompt you provide truly matters. As you can see from my simple twist on the prompt, it failed to give me the right answer. So, does that mean we shouldn’t use GPT? No, we should definitely use it, but with proper prompting. This is where prompt engineering comes into play, allowing us to optimize the input and guide GPT in producing more accurate and desired outputs.\n",
            "The examples provided in this blog are sourced directly from their official documentation found at\n",
            "Official documentation link — https://www.promptingguide.ai/.\n",
            "Created by — dair-ai\n",
            "Furthermore, the examples showcased in the guide were tested using a specific model known as text-davinci-003 on the OpenAI’s playground platform. It is important to note that the guide assumes the default settings of the model, with a temperature value of 0.7 and a top-p value of 1.\n",
            "So, buckle up and get ready to unleash your inner prompt engineer. Let’s get started!\n",
            "Imagine you have a super-smart assistant, let’s call it AI Helper, that can answer any question you ask. For example, if you ask it, “What is the capital of France?” it will give you the correct answer, “Paris.”\n",
            "Now, let’s say you want to make AI Helper even more impressive. You want it to not only tell you the capital of a country but also provide a short history about it. In prompt engineering, you would fine-tune your instructions to achieve that. Which means that instead of just asking, “What is the capital of France?” you might rephrase it to say, “Tell me about the capital of France and its historical significance.” By tweaking the prompt, you’re guiding AI Helper to give you the desired result.\n",
            "In real life, prompt engineering is used in many applications. For instance, think of virtual assistants like Siri or Alexa. When you ask them a question, the way you phrase it influences the quality of their response. By understanding prompt engineering, developers can improve these systems to give us even more accurate and helpful answers.\n",
            "Among the various parameters that influence the output of LLM (large language models), two play a significant role: temperature and top_p value. Let’s define each of these parameters to understand their impact on the generated results.\n",
            "Temperature: Think of temperature like a spice level in cooking. A lower temperature value makes the language model play it safe and stick to the most likely predictions. It’s like adding less spice, resulting in more consistent and predictable outputs. On the other hand, a higher temperature value adds more randomness and creativity to the mix, just like adding more spice to a dish and getting unexpected flavor combinations.\n",
            "Top_p Value: Imagine you have a multiple-choice question with various possible answers. The top_p value is like setting a threshold for how many options to consider. A lower top_p value means only the most probable answers will be selected, keeping things focused and precise. It’s like only considering the top few choices. On the contrary, a higher top_p value expands the range of options, including more possibilities and diverse responses.\n",
            "In a nutshell, temperature affects the level of randomness in the language model’s output, while top_p value controls the range of choices considered.\n",
            "Let’s pass a very simple prompt:\n",
            "When you start the sentence with “The sky is” it gives you different options instead of one definite answer. But if you include more important details in your sentence, you increase the chances of getting a clear and accurate response. Let’s look at another example where adding crucial information to the prompt can make a difference.\n",
            "Is that clearer? You instructed the model to finish the sentence, resulting in a more accurate response that aligns with your prompt. This technique of constructing effective prompts to guide the model’s task is known as prompt engineering.\n",
            "In simple terms, the basic rule is that a question should be formatted as:\n",
            "while an instruction should be formatted as:\n",
            "When it comes to formatting question answering, it is common practice to utilize a question answering (QA) format, which is widely used in many QA datasets.\n",
            "The format mentioned above is commonly known as zero-shot prompting. It is referred to as such because it does not involve providing any specific examples or demonstrations of how the question and answer should be structured.\n",
            "In the format I mentioned earlier, there is another technique called few-shot prompting that is widely used and effective. In few-shot prompting, you include demonstrations to guide the model. Here’s how you can format few-shot prompts:\n",
            "The QA format version would look like this:\n",
            "To make it clearer how few shot prompts work, here is a small classification example:\n",
            "The provided format is quite clear. Each review is followed by two forward slashes (//) and then the sentiment value, which can be either positive or negative.\n",
            "Few-shot prompts allow language models to learn tasks by providing them with a few examples, which helps them understand the context and perform better.\n",
            "A prompt can include different elements:\n",
            "Not all four elements are necessary for a prompt, and the format depends on the specific task being performed.\n",
            "Start simple — When designing prompts, it’s important to start with simplicity and try again or iterate to achieve optimal results. As mentioned in the official guide that beginning with a basic playground, such as OpenAI or Cohere, is recommended. You can gradually enhance your prompts by adding more elements and context to improve outcomes. Throughout the process, iterating your prompt is crucial.\n",
            "To design effective prompts for simple tasks, use instructive commands like “Write,” “Classify,” “Summarize,” “Translate,” “Order,” etc. Experimentation is key to finding the best approach. Some recommendations include placing instructions at the beginning of the prompt and using a clear separator like “###” to distinguish instructions from context.\n",
            "For example:\n",
            "Be extremely specific and detailed in your instructions and desired task for the model. Focus on having a well-structured and descriptive prompt. Including examples within the prompt can be highly effective. Consider the length of the prompt, as there are limitations on its size. Strive for a balance between being specific and avoiding unnecessary details.\n",
            "Consider this example, where we want to extract information from piece of text:\n",
            "While it’s important to be detailed and improve the format of prompts, it’s crucial to avoid overcomplicating them and creating imprecise descriptions. Being specific and direct is often more effective, like effective communication.\n",
            "Here’s a quick example of what I am trying to say!. Let’s say you want to understand prompt engineering. Initially, you might ask ChatGPT for a brief explanation without being too detailed. You might try something like this:\n",
            "However, that prompt may not provide clear instructions on the number of sentences or the style. While you may still receive decent responses, a better approach would be (very specific, concise, and to the point):\n",
            "When designing prompts, instead of specifying what not to do, provide clear instructions on what the model should do.\n",
            "Let’s look at an example of a movie recommendation chatbot that fails to meet expectations because of the way the instruction was written. The user asked it to avoid doing something specific, which caused the chatbot to focus on the wrong things instead of what user actually wanted it to do.\n",
            "Now, instead of instructing the bot on what not to do, let’s provide clear instructions on what we want the bot to do.\n",
            "In this section, we will explore various use cases of prompt engineering across different domains, such as text summarization, question answering, and more.\n",
            "is a common task in natural language generation. We can try a simple summarization task using prompts. The below example summarize the antibiotics information into a single sentence.\n",
            "Now we will utilize a language model to perform information extraction. This involves extracting relevant information from a given paragraph.\n",
            "Where red arrow highlight the asked information from the paragraph.\n",
            "Here’s a guide on how to perform question answering using Language Models (LLM’s).\n",
            "To get the desired label format, such as “neutral” instead of “Neutral,” provide specific instructions within the prompt for better results.\n",
            "For example:\n",
            "When you provide a sample of how the model should return the sentiment value, it will return the value in the same format as the provided sample.\n",
            "Let’s try the above example but with a little change in it:\n",
            "The model returns “neutral” instead of “nutral” because there was no specific example provided in the prompt to guide the desired output format. Being specific and providing clear examples is essential in prompt engineering to ensure the model understands what is expected.\n",
            "Prompt engineering allows you to instruct the LLM system to act as a conversational system (such as chatbot etc.). This is where role prompting comes to play.\n",
            "To make the bot less technical and more easily understandable, provide additional information in the prompt, such as specifying that the response should be in a language understandable by a 7th-grade student. This will guide the bot to use simpler language and avoid excessive technical terms.\n",
            "is indeed an important use case of Language Models (LLMs). GitHub Copilot serves as an example of how LLMs can be utilized for generating code.\n",
            "didn’t specify the programming language for the answer. It highlights how even a small detail missing from the prompt can significantly impact the understanding and accuracy of the response.\n",
            "One of the most challenging tasks for Language Models (LLMs) is reasoning, which involves the ability to engage in logical thinking and draw conclusions based on given information.\n",
            "Here is a complex prompt that challenges our LLM’s understanding:\n",
            "The author explicitly stated that they made numerous attempts to achieve this. It emphasizes that reasoning is indeed one of the most challenging aspects to address when working with LLMs.\n",
            "Large LLMs like GPT-3 can perform tasks without explicit training, thanks to their ability to follow instructions and extensive training on massive datasets. This is known as “zero-shot” learning. Since they are already familiar with the words in the prompt, there is minimal additional learning involved.\n",
            "Let’s recall our sentiment prompt example:\n",
            "Despite not explicitly mentioning the word “sentiment,” the model’s zero-shot capabilities enable it to understand and generate responses related to sentiment due to its training on a large dataset. It can infer the concept based on its pre-existing knowledge and context.\n",
            "when zero shot didn’t works we use few shot prompting, where we gave example to our model. One example means one shot, giving two examples means two shot and so on.\n",
            "An example of one shot:\n",
            "First, we provided the definition of the word “whatpu” to our model. Then, we gave an example sentence that includes the word “whatpu” before asking the model to use it in a sentence.\n",
            "If we recall correctly, in our previous reasoning prompt, we asked the model to add odd numbers and determine if the result was even. Now, let’s attempt to solve the same problem using a few-shot approach.\n",
            "Unfortunately, the few-shot prompting approach did not yield reliable responses for this reasoning problem. It appears that additional techniques or approaches might be required to achieve more accurate and consistent results in such cases.\n",
            "Chain-of-thought (CoT) prompting, when used alongside few-shot prompting, enhances the model’s reasoning capabilities for complex tasks. It breaks down the problem into smaller steps, enabling the model to reason through intermediate stages before providing a response. This combination is effective for achieving better results on challenging tasks that require reasoning.\n",
            "Breaking down complex problems into subproblems significantly helps LLMs in providing accurate and proper responses to complex questions. It allows the model to reason through each subproblem individually, leading to a more comprehensive understanding of the overall question and generating more reliable answers.\n",
            "Let’s try to solve our adding odd numbers task using COT prompting:\n",
            "This time the answer is correct because of providing reasoning steps while solving the problem.\n",
            "By combining zero-shot prompting with Chain-of-Thought (CoT) prompting, we can tackle problems by encouraging the model to think step by step.\n",
            "Here is an example of it:\n",
            "The combination of zero-shot and few-shot with Chain-of-Thought (CoT) prompting has shown superior performance compared to other approaches when solving word problems. By incorporating both techniques, the model benefits from the ability to reason step by step and generate accurate responses, even in challenging problem-solving scenarios.\n",
            "Self-consistency is an advanced technique in prompt engineering that helps improve the performance of few-shot Chain-of-Thought (CoT) prompting. It involves generating multiple responses using CoT prompting and selecting the most consistent answer. This technique is especially useful for tasks involving arithmetic and commonsense reasoning, as it enhances the accuracy of the model’s responses.\n",
            "Here is an example of a very simple arithmetic task:\n",
            "The answer is wrong, Using self-consistency in prompt engineering, we can improve the performance of our model in tasks like answering specific questions.\n",
            "In the context of Chain-of-Thought (CoT) prompting, we ask multiple questions and provide answers for each question. The last question is the one for which we need an answer. By applying this approach, we can guide the model to generate more accurate and consistent responses.\n",
            "Here are the multiple outputs:\n",
            "we compute the final answer using the self-consistency technique, additional steps are involved. For more detailed information on the process, you can refer to the paper titled “Self-Consistency Training for Compositional Reasoning” available at the following link: https://arxiv.org/pdf/2203.11171.pdf. The paper provides in-depth insights and techniques for effectively applying self-consistency training in the context of compositional reasoning tasks.\n",
            "One popular technique in prompt engineering is to incorporate knowledge or information to enhance the model’s prediction accuracy. By providing relevant knowledge or information related to the task at hand, the model can leverage this additional context to make more accurate predictions. This technique enables the model to tap into external resources or pre-existing knowledge to improve its understanding and generate more informed responses.\n",
            "Here is an example of why knowledge is important:\n",
            "This mistake shows that LLMs have limitations when it comes to tasks that need a deeper understanding of the world.\n",
            "Let’s enhance this by not just answering the question, but also imparting some knowledge along the way.\n",
            "For example, active-prompting is a technique that dynamically guides language models toward desired outputs. Directional stimulus prompting explores ways to steer models in specific directions. ReAct focuses on improving model performance through active learning and iterative feedback. These are just a few examples among many other exciting areas to explore.\n",
            "Prompt engineering encompasses diverse applications, such as generating data, generating code, and graduate job classification case studies. You can also delve into different model architectures like Flan, ChatGPT, LLaMA, and even learn about the highly anticipated GPT-4. Additionally, it’s crucial to be aware of the risks and potential misuses associated with prompt engineering, such as adversarial prompting, factuality concerns, and biases in models.\n",
            "To expand your knowledge, you can explore papers, tools, notebooks, and datasets related to prompt engineering. These resources will provide valuable insights and additional readings to further enhance your understanding of this fascinating field. So, grab the official guide and embark on your own exploration of prompt engineering’s vast possibilities!\n"
        ]
    },
    {
        "link": "https://medium.com/@thomascherickal/how-to-create-your-own-llm-model-2598615a039a?source=list-46b5cb511765--------0-------ecb349332558---------------------",
        "title": "How to Create Your Local LLM Model (Revised and Updated) — Extra Curated LLMOps Resources",
        "subtitle": "false",
        "autorName": "Thomas Cherickal",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*1QhDkj4rWjoCfGD-XE87iA.jpeg",
        "clap": "90",
        "response": "7",
        "timeForRead": "8 min read",
        "dateCreate": "Apr 1",
        "text": [
            "Creating a local large language model (LLM) is a significant undertaking, typically requiring substantial computational resources and expertise in machine learning. It was not feasible to run local LLMs on your own local system because of the computational costs involved. However, with the advent of new software, GPT4All and LM-Studio can be used to create complete software packages that work locally. But let’s start with a HuggingFace Transformers source code example that shows you how to use the HuggingFace Libraries and PyTorch for LLMs (cloud-based, not local in this case):\n",
            "A complete program that uses the GPT-2 model, GPT-2 tokenizer, and is fine-tuned on the AG NEWS dataset (a small dataset used for utility purposes) is given below and explained in code snippets. We can leverage the power of pre-trained models and fine-tune them on specific tasks.\n",
            "This script is a good example of how to fine-tune a pre-trained language model on a specific task. However, it’s worth noting that fine-tuning a large model like GPT-2 can be computationally intensive and may require a powerful machine or cloud-based resources. Also, this script doesn’t include some important steps like splitting the data into training and validation sets, shuffling the data, and batching the data. These steps are crucial for training a robust model. For convenience, the entire program is given below (please do report errors or corrections in the comments below):\n",
            "There are two packaged solutions for Local LLMs (and many more popping up, everyday). Two of the best of them are given below. I especially have a preference for LM-Studio.\n",
            "You don’t need any of this code anymore because the GPT4All open-source application has been released that runs an LLM on your local computer without the Internet and without a GPU. I’m linking tothe site below:\n",
            "This is the best solution for those of you who want a completely open-source on-premises system. Have fun! But make sure you have at least 32 GB of local RAM, 16 GB GPU RAM, a 3+ Ghz multicore(the more, the better) processor, and a local SSD. LLMs are almost as computationally expensive as Bitcoin mining!\n",
            "The relevant GitHub repository is:\n",
            "Now, of course there’s a lot more to LLM models than just chat. But considering the expensive;y daunting computational requirements for fine-tuning musical and pictures and audio for LLMs, I am just going to mention some popular, already built and ready-to-go solutions as well as some interesting source material:\n",
            "https://medium.com/@abonia/best-llm-and-llmops-resources-for-2023-75e96ac37feb\n",
            "This article is an absolute gem. Please do visit, there are some incredible both standard and extraordinary resources here.\n",
            "Of course, we cannot leave out:\n",
            "https://learn.deeplearning.ai/ — The courses that are curated and run by Andrew Ng himself. Need we say more?\n",
            "LM-Studio is a powerful tool for training and deploying language models. It provides a user-friendly interface and a wide range of features to help you fine-tune your models, visualize their performance, and deploy them in production.\n",
            "LM-Studio supports various transformer-based models like GPT-2, GPT-3, BERT, Falcon, Llama2, Llama-Python, and many, many others. It also provides various options for data preprocessing, model training, and hyperparameter tuning. This makes it a versatile tool for both beginners and experienced machine learning practitioners.\n",
            "One of the key features of LM-Studio is its support for fine-tuning. Fine-tuning is a process where you take a pre-trained model and train it further on a specific task. This can significantly improve the model’s performance on that task. LM-Studio makes this process easy by providing a simple interface for loading pre-trained models and training them on your data.\n",
            "Another important feature of LM-Studio is its visualization tools. These tools allow you to monitor your model’s performance during training and evaluate its performance on a test set. This can help you identify issues early and make necessary adjustments to your training process.\n",
            "LM-Studio also provides a robust deployment pipeline. Once your model is trained and tested, you can easily deploy it to a production environment. This makes LM-Studio a great tool for end-to-end machine learning projects.\n",
            "Usability: LM-Studio is designed to be user-friendly. It provides a clean and intuitive interface that makes it easy to navigate through different features. It also provides detailed documentation and tutorials to help you get started.\n",
            "Scalability: LM-Studio can handle large datasets and complex models. It leverages the power of modern hardware and software to train models efficiently. This makes it a suitable tool for both small and large-scale projects.\n",
            "Community Support: LM-Studio has a vibrant community of users and contributors. They provide valuable feedback and contribute to the development of the tool. This ensures that LM-Studio is always up-to-date with the latest trends and technologies in the field of machine learning.\n",
            "LM-Studio is a comprehensive tool for training and deploying language models. It provides a wide range of features, a user-friendly interface, and robust performance.\n",
            "The field of LLMs and advanced AI is what I’ve decided will easily be the most versatile technology of the future. Junior Programmers, Artists, ML Engineers, Data Processing Analysts, Beginner Data Scientists, and practically every other digital job should be learning this technology since advanced versions in the future will have no errors and be production-ready — from a single line of text. Learn Generative Learning. It really is the future of the Digital World. And artists are suffering already! I can see a similar situation for even junior-level software engineers very soon. People, skill up! Get into Generative AI! 10x your productivity! The future belongs to those who will use these tools the best. Get Going! ASAP.\n"
        ]
    },
    {
        "link": "https://medium.com/@jimeng_57761/autotrial-a-leap-towards-automated-clinical-trial-design-unveiled-at-emnlp23-61cb27acd376?source=list-ce01ef2736c2--------3-------847f40a2cdd4---------------------",
        "title": "AutoTrial: A Leap Towards Automated Clinical Trial Design — Unveiled at EMNLP’23",
        "subtitle": "false",
        "autorName": "Jimeng",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*jXYrgHsXa8PJbY_rct13tQ.png",
        "clap": "2",
        "response": "4",
        "timeForRead": "8 min read",
        "dateCreate": "Oct 8",
        "text": [
            "Clinical trials are the cornerstone of drug development, ensuring the safety and efficacy of new medical interventions. However, the process of designing robust clinical trials is complex and demands precise eligibility criteria for participant recruitment. A significant challenge arises in devising these criteria, with nearly 57% of trial protocols undergoing at least one substantial amendment, leading to substantial financial losses and time delays.\n",
            "In our new paper accepted at EMNLP’23, we introduce “AutoTrial: Prompting Language Models for Clinical Trial Design,” a new method aiming to automate the design of clinical eligibility criteria leveraging Large Language Models (LLMs). This method stems from the ability of LLMs to generate coherent and human-like text, an attribute we harness for automating the critical planning stage of clinical trials. The paper focuses on generating eligibility criteria for clinical trial protocols, but the methods used can be extended to cover other sections of the trial protocols.\n",
            "Key Features of AutoTrial:\n",
            "1. Comprehending Instructions: — AutoTrial is designed to comprehend key trial information and additional instructions to generate precise eligibility criteria tailored to the specified objectives of a trial.\n",
            "2. Referring to Prior Studies: — Mirroring the human experts’ practice of referencing prior successful trials, AutoTrial leverages context information to generate enhanced trial designs.\n",
            "3. Rationalizing the Generation: — Offering a rationale behind the generated criteria is a fundamental feature of AutoTrial, enabling clinical experts to understand and adopt the generation results in practice.\n",
            "4. Technical Enrichments: — Instruction prompting for granular control, scalable knowledge expansion via a hybrid of external and internal memory, and explicit supervision for generating grounding rationales are among the technical features that power AutoTrial.\n",
            "AutoTrial Method\n",
            "AutoTrial uses a decoder-based architecture to generate a target criterion based on input trial synopsis and manual instructions. The training process has two stages: pretraining and finetuning.\n",
            "In the pretraining stage, the model is trained on a large corpus of trial documents. This helps the model to learn to reason through multiple steps and mimic the retrieved input criteria exemplars.\n",
            "In the finetuning stage, the model is trained to generate the target criterion according to the input instructions. For instance, if there is an instruction <age> that urges the model to populate the criterion describing the participant’s age requirement, it will do so accordingly.\n",
            "It is worth noting that the model can be extended to new instructions and trial exemplars without retraining. You can see the flowchart of the process in Figure. 1.\n",
            "Next, we will discuss the details of the training and inference procedures of AutoTrial.\n",
            "Problem SetupThe generation model, represented by function f, creates a target criterion yc based on input x = {xs, xe, xr}. Here, xs signifies trial setups, a blend of the trial title, condition, and treatment as depicted in Figure 1. xr represents the discrete prompt outlining the objective criterion, for instance, “bmi” prompts the model to formulate the criterion for participants’ body mass index. xe refers to exemplars fetched from pertinent trials, aiding the in-context learning of LLMs, and is defined as xe = {xe^t, xe^r, xe^c}, encompassing the reasoning steps xe^t, the targeting instruction xe^r, and the target criterion xe^c which delineates the requirement per the instruction.\n",
            "Additionally, a continuous prompt hp steers the model, tailored to each instruction type, like the targeting entity the criterion must encompass. The model is trained to sequentially generate criteria y through multi-step reasoning, culminating in the target criterion, expressed as y = f(xs, xe, xr, hp) in Eq. (1). With reference to exemplar xe, the model outputs y = yt ⊕ yc, where yt entails the reasoning steps and yc represents the target criterion.\n",
            "Hybrid Prompting Strategy\n",
            "AutoTrial introduces two different prompting strategies namely discrete and neural prompting to facilitate LLM to generate criteria based on specific instructions.\n",
            "The discrete prompt is motivated by the prospect of in-context learning, as the reasoning ability of LLMs can be enhanced via the input-output exemplars, e.g., the concatenation of a series of criteria xe^t, the target instruction xe^r, and the target criteria xe^c. We formulate the discrete prompts with specialized tokens:Got it! Let’s dive straight in without the concert vibes:\n",
            "Trial Setup: This is all about laying down the foundation. It introduces the main components of the study, which can be thought of as the building blocks. The tags like <title>, <disease>, and <treatment> give an overview of what the trial is all about.\n",
            "For example:\n",
            "<title> The Ultimate Drug Trial<disease> Diabetes<treatment> MagicPill 2.0\n",
            "In-context Exemplar: This part is about setting specific conditions for participants. It determines who can join the trial and who cannot. Using tags like <inc> and <exc>, the criteria for inclusion and exclusion are clearly defined.\n",
            "For instance:\n",
            "<inc> Must have had Diabetes for over 5 years<exc> No history of heart disease\n",
            "To help guide the model in generating the criteria, there’s an instruction wrapped with the <statement> tag. This tag essentially points the model in the right direction.\n",
            "For example:\n",
            "<statement> age Leading to:<target> age is above 18 yrs old\n",
            "Textual Instruction: This is a direct command that emphasizes what the model should focus on next. It’s like providing a spotlight on a specific topic. The <statement> tag is used to specify this topic or area of interest.\n",
            "For instance:\n",
            "<statement> gender\n",
            "By providing these structured prompts and guidelines, the model can generate answers that align with what’s needed. It’s a way of streamlining and enhancing the communication between the user and the model.\n",
            "Sure, let’s tighten that up:\n",
            "2. Neural Prompt\n",
            "In addition to discrete prompting, there is another strategy called neural prompting. This method occurs at the embedding level and is described in section 3.2.2 of the paper. While the mathematical notation may be complex, the high-level concept is simple. The approach involves embedding a text input x<l into matrix H<l and supplementing it with another embedding hp=MLP(Er[i,:]) that corresponds to the i-th instruction from instruction set I. Er is a trainable embedding matrix. The final augmented embedding is denoted as \\tilde(H<l) = hp⊕H<l.Neural prompting is modular, making it easy to incorporate additional instructions I′ by simply expanding the index set I ={I, I′} and the embedding matrix Er ={Er, E′r} for those instructions. When fine-tuning the model on new data, we can update only the instruction embedding E′r while keeping the rest of the model frozen. This allows the model to learn how to generate based on a wider range of instructions while minimizing the risk of catastrophic forgetting, which is the performance degradation on previous data.\n",
            "Multi-stage Training\n",
            "AutoTrial creates a dataset that includes pairs of input instructions (represented as xr) and their respective criteria (represented as yc). We derive clinical relationships from the raw criteria to formulate the training and testing data. For example, we extract the relation \"NYHA ∈ {III, IV}\" from the criteria \"NYHA class is above II.\" However, the parser may not always be able to extract all relevant instructions from all available trial documents. Therefore, we suggest training our method in two stages: first, pre-training on a large set of unlabeled trial documents, and then fine-tuning on the processed dataset of instruction-criteria pairs. This approach enables us to make the most of the available data and improve the model's performance.\n",
            "Pretraining. We create a pretraining datasetCpre = {(xs, xe, yt, yc)i} , where the model f tries to generate y = yt ⊕yc in Eq. (1). The inputs comprise the trial setup xs and the exemplar xe which is also composed of multiple criteria. We decide to include prompts and special tokens in the pretraining stage. Specifically, we explicitly emphasize the step-by-step reasoning task by inserting the separate tokens <inc> and <exc> into xe and yt, and the model is supervised to generate the intermediate rationales and yield the target criterion.Our method is built based on decoder-based CLM (e.g., GPT2 (Radford et al., 2019)) where the decoder predicts y autoregressively. Denotethe learned decoding distribution as pθ(·), the objective is the maximum log-likelihood estimation given by\n",
            "Finetuning. After pretraining, the model is finetuned on the dataset C, and taught to follow the instruction when generating criteria. The inputs and outputs are described in Eq. (1). In addition to the MLE loss above, we apply a contrastive loss L_CL to enhance the model representation learning, as in\n",
            "The finetuning loss combines the objectives into L_FT = L_MLE + L_CL.\n",
            "GenerationDenote the vocabulary by V, we conduct top-k sampling repeatedly to acquire diverse candidates\n",
            "We further adopt clustering and ranking to select samples from the generated candidates. We first encode ˆy by Trial2Vec to dense embeddings hˆy and apply k-means clustering with kq clusters. We then compute the perplexity ( ppl ) of each output ˆyq, and pick the sample with the minimum ppl in each cluster to form the final candidate set with kq samples.\n",
            "Experiment Performance:\n",
            "AutoTrial has demonstrated a high level of efficacy, achieving a precision score of 0.91, a recall score of 0.92, and an F1 score of 0.91 in clinical accuracy evaluations. This performance is significantly better than the baselines, which scored less than 0.5 in these metrics. Additionally, AutoTrial performed robustly with a winning rate of around 60% against GPT-3.5 in trial design tasks through human evaluation. For more detailed experimental results, please refer to the paper. Some experiment tables and figures are also available for in-depth study.\n",
            "Implications and Future Directions:\n",
            "AutoTrial marks a significant stride towards utilizing AI to facilitate clinical trial design, ensuring a more efficient, accurate, and streamlined process. The success of AutoTrial opens up exciting avenues for further research, potentially revolutionizing the landscape of clinical trial design and speeding up the drug development pipeline.\n",
            "The presentation at EMNLP’23 will delve into the proposed method in detail, showcasing the experiment results and discussing the future trajectory of this initiative. AutoTrial is not just a novel method; it’s a promising step towards harnessing the power of AI in addressing real-world challenges in healthcare, ensuring faster and more reliable clinical trial designs for a better tomorrow. We are actively deploying AI based trial design solution called TrialMind based on AutoTrial. If you are interested, please check out Keiji.ai\n"
        ]
    },
    {
        "link": "https://medium.com/@haifengl/a-tutorial-to-llm-f78dd4e82efc?source=list-c2ea05c4399f--------5-------c97b7008c754---------------------",
        "title": "A Tutorial on LLM",
        "subtitle": "false",
        "autorName": "Haifeng Li",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*ArVflTnV5rnvPe0BR54gew.jpeg",
        "clap": "919",
        "response": "10",
        "timeForRead": "15 min read",
        "dateCreate": "Sep 14",
        "text": [
            "Generative artificial intelligence (GenAI), especially ChatGPT, captures everyone’s attention. The transformer based large language models (LLMs), trained on a vast quantity of unlabeled data at scale, demonstrate the ability to generalize to many different tasks. To understand why LLMs are so powerful, we will deep dive into how they work in this post.\n",
            "Formally, a decoder only language model is simply a conditional distribution p(xi|x1···xi−1) over next tokens xi given contexts x1 · · · xi−1. Such a formulation is an example of Markov process, which has been studied in many use cases. This simple setup also allows us to generate token by token in an autoregressive way.\n",
            "Before our deep dive, I have to call out the limitation of this formulation to reach artificial general intelligence (AGI). Thinking is a non-linear process but our communication device, mouth, can speak only linearly. Therefore, language appears a linear sequence of words. It is a reasonable start to model language with a Markov process. But I suspect that this formulation can capture the thinking process (or AGI) completely. On the other hand, thinking and language are interrelated. A strong enough language model may still demonstrates some sort of thinking capability as GPT4 shows. In what follows, let’s check out the scientific innovations that makes LLMs to appear intelligently.\n",
            "There are many ways to model/represent the conditional distribution p(xi|x1···xi−1). In LLMs, we attempt to estimate this conditional distribution with a neural network architecture called Transformer. In fact, neural networks, especially a variety of recurrent neural networks (RNNs), have been employed in language modeling for long time before Transformer. RNNs process tokens sequentially, maintaining a state vector that contains a representation of the data seen prior to the current token. To process the n-th token, the model combines the state representing the sentence up to token n-1with the information of the new token to create a new state, representing the sentence up to token n. Theoretically, the information from one token can propagate arbitrarily far down the sequence, if at every point the state continues to encode contextual information about the token. Unfortunately, the vanishing gradient problem leaves the model’s state at the end of a long sentence without precise, extractable information about preceding tokens. The dependency of token computations on the results of previous token computations also makes it hard to parallelize computation on modern GPU hardware.\n",
            "These problems were addressed by self-attention mechanisms in Transformer. Transformer is a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The attention layer can access all previous states and weigh them according to a learned measure of relevance, providing relevant information about far-away tokens. Importantly, Transformers use an attention mechanism without an RNN, processing all tokens simultaneously and calculating attention weights between them in successive layers. Since the attention mechanism only uses information about other tokens from lower layers, it can be computed for all tokens in parallel, which leads to improved training speed.\n",
            "The input text is parsed into tokens by a byte pair tokenizer, and each token is converted into an embedding vector. Then, positional information of the token is added to the embedding. The transformer building blocks are scaled dot-product attention units. When a sentence is passed into a transformer model, attention weights are calculated between every token simultaneously. The attention unit produces embeddings for every token in context that contain information about the token itself along with a weighted combination of other relevant tokens each weighted by its attention weight.\n",
            "For each attention unit, the transformer model learns three weight matrices; the query weights WQ, the key weights WK, and the value weights WV. For each token i, the input word embedding is multiplied with each of the three weight matrices to produce a query vector qi, a key vector ki, and a value vector vi. Attention weights are dot product between qi and kj, scaled by the square root of the dimension of the key vectors, and normalized through softmax. The output of the attention unit for token i is the weighted sum of the value vectors of all tokens, weighted by the attention from token i to each token j. The attention calculation for all tokens can be expressed as one large matrix calculation:\n",
            "One set of (WQ, WK, WV) matrices is called an attention head, and each layer of transformer has multiple attention heads. With multiple attention heads the model can calculate different relevance between tokens. The computations for each attention head can be performed in parallel and the outputs are concatenated and projected back to same input dimension by a matrix WO.\n",
            "In an encoder, there is a fully-connected multilayer perceptron (MLP) after the self-attention mechanism. The MLP block further processes each output encoding individually. In the encoder-decoder setting (e.g. for translation), an additional attention mechanism is inserted between self-attention and MLP into the decoder to draw relevant information from the encodings generated by the encoders. In a decoder only architecture, this is not necessary. No matter encoder-decoder or decoder only architecture, decoder must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow, which allows for autoregressive text generation. To generate token by token, the last decoder is followed by a softmax layer to produce the output probabilities over the vocabulary.\n",
            "Decoder-only GPT is essentially a unsupervised (or self-supervised) pre-training algorithm that maximizes the following likelihood:\n",
            "where k is the size of context window. While the architecture is task-agnostic, GPT demonstrates that large gains on natural language inference, question answering, semantic similarity, and text classification can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task.\n",
            "After pre-training the model with the above objective, we can adapt the parameters to the supervised target task. Given a labeled dataset C, where each instance consists of a sequence of input tokens, x1, . . . , xm, along with a label y. The inputs are passed through the pre-trained model to obtain the final transformer block’s activation hlm, which is then fed into an added linear output layer with parameters Wy to predict y:\n",
            "Correspondingly, we have the following objective function:\n",
            "In addition, it is helpful including language modeling as an auxiliary objective as it improves generalization of the supervised model and accelerates convergence. That is, we optimize the following objective:\n",
            "Text classification can be directly fine-tuned as described above. Other tasks, like question answering or textual entailment, have structured inputs such as ordered sentence pairs, or triplets of document, question, and answers. Since the pre-trained model was trained on contiguous sequences of text, it needs some modifications to apply to these tasks.\n",
            "Textual entailment: concatenate the premise p and hypothesis h token sequences, with a delimiter token ($) in between.Similarity: there is no inherent ordering of the two sentences being compared. Therefore, the input sequence contain both possible sentence orderings (with a delimiter in between) and process each independently to produce two sequence representations, which are added element-wise before being fed into the linear output layer.Question Answering and Commonsense Reasoning: each sample has a context document z, a question q, and a set of possible answers {ak}. GPT concatenates the document context and question with each possible answer, adding a delimiter token in between to get [z;q;$;ak]. Each of these sequences are processed independently and then normalized via a softmax layer to produce an output distribution over possible answers.\n",
            "While GPT shows that supervised fine-tuning works well on task specific datasets, to achieve strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands of examples specific to that task. Interestingly, GPT2 demonstrates that language models begin to learn multiple tasks without any explicit supervision, conditioned on a document plus questions (aka prompts).\n",
            "Learning to perform a single task can be expressed in a probabilistic framework as estimating a conditional distribution p(output|input). Since a general system should be able to perform many different tasks, even for the same input, it should condition not only on the input but also on the task to be performed. That is, it should model p(output|input, task). Previously, task conditioning is often implemented at an architectural level or at an algorithmic level. But language provides a flexible way to specify tasks, inputs, and outputs all as a sequence of symbols. For example, a translation training example can be written as the sequence (translate to french, english text, french text). In particular, GPT2 is conditioned on a context of example pairs of the format english sentence = French sentence and then after a final prompt of english sentence = we sample from the model with greedy decoding and use the first generated sentence as the translation.\n",
            "Similarly, to induce summarization behavior, GPT2 adds the text TL;DR: after the article and generate 100 tokens with Top-k random sampling with k = 2 which reduces repetition and encourages more abstractive summaries than greedy decoding. Likewise, a reading comprehension training example can be written as (answer the question, document, question, answer).\n",
            "Note that zero-shot transfer is different from zero-shot learning in next section. In zero-shot transfer, “zero-shot” is in the sense that no gradient updates are performed, but it often involves providing inference-time demonstrations to the model (e.g. the above translation example), so is not truly learning from zero examples.\n",
            "I find an interesting connection between this meta learning approach with Montague semantics, which is a theory of natural language semantics and of its relationship with syntax. In 1970, Montague formulated his views:\n",
            "Philosophically, both zero-shot transfer and Montague semantics treat natural language same as programming language. LLMs capture the task through the embedding vectors in a black box approach. It is not clear to us how it really works though. In contrast, the most important features of Montague semantics are its adherence to the principle of compositionality — that is, the meaning of the whole is a function of the meanings of its parts and their mode of syntactic combination. This may be an approach to improve LLMs.\n",
            "GPT3 shows that scaling up language models greatly improves task-agnostic, few-shot performance. GPT3 further specialize the description to “zero-shot”, “one-shot”, or “few-shot” depending on how many demonstrations are provided at inference time: (a) “few-shot learning”, or in-context learning where we allow as many demonstrations as will fit into the model’s context window (typically 10 to 100), (b) “one-shot learning”, where we allow only one demonstration, and © “zero-shot” learning, where no demonstrations are allowed and only an instruction in natural language is given to the model.\n",
            "For few-shot learning, GPT3 evaluates each example in the evaluation set by randomly drawing K examples from that task’s training set as conditioning, delimited by 1 or 2 newlines depending on the task. K can be any value from 0 to the maximum amount allowed by the model’s context window, which is nctx = 2048 for all models and typically fits 10 to 100 examples. Larger values of K are usually but not always better.\n",
            "For some tasks GPT3 also uses a natural language prompt in addition to (or for K = 0, instead of) demonstrations. On tasks that involve choosing one correct completion from several options (multiple choice), the prompt includes K examples of context plus correct completion, followed by one example of context only, and the evaluation process compares the model likelihood of each completion.\n",
            "On tasks that involve binary classification, GPT3 gives the options more semantically meaningful names (e.g. “True” or “False” rather than 0 or 1) and then treat the task like multiple choice.\n",
            "On tasks with free-form completion, GPT3 uses beam search. The evaluation process scores the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand.\n",
            "The capacity of the language model is essential to the success of task-agnostic learning and increasing it improves performance in a log-linear fashion across tasks. GPT-2 was created as a direct scale-up of GPT-1, with both its parameter count and dataset size increased by a factor of 10. But it can perform downstream tasks in a zero-shot transfer setting — without any parameter or architecture modification.\n",
            "GPT3 uses the same model and architecture as GPT2 with the exception using alternating dense and locally banded sparse attention patterns in the layers of the transformer.\n",
            "On TriviaQA, GPT3’s performance grows smoothly with model size, suggesting that language models continue to absorb knowledge as their capacity increases. One-shot and few-shot performance make significant gains over zero-shot behavior.\n",
            "While less discussed, data quality matters too. Datasets for language models have rapidly expanded. For example, the CommonCrawl dataset constitutes nearly a trillion words, which is sufficient to train largest models without ever updating on the same sequence twice. However, it was found that unfiltered or lightly filtered versions of CommonCrawl tend to have lower quality than more curated datasets.\n",
            "Therefore, GPT2 created a new web scrape which emphasizes document quality by scraping all outbound links from Reddit which received at least 3 karma, which acts as a heuristic indicator for whether other users found the link interesting, educational, or just funny. The final dataset contains slightly over 8 million documents for a total of 40 GB of text after de-duplication and some heuristic based cleaning.\n",
            "Further, GPT3 took 3 steps to improve the average quality of datasets: (1) filtered CommonCrawl based on similarity to a range of high-quality reference corpora, (2) fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of held-out validation set as an accurate measure of overfitting, and (3) added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity.\n",
            "Similarly, GLaM develops a text quality classifier to produce a high-quality web corpus out of an original larger raw corpus. This classifier is trained to classify between a collection of curated text (Wikipedia, books and a few selected web-sites) and other webpages. GLaM uses this classifier to estimate the content quality of a webpage and then uses a Pareto distribution to sample webpages according to their score. This allows some lower-quality webpages to be included to prevent systematic biases in the classifier.\n",
            "GLaM also sets the mixture weights based on the performance of each data component in a smaller model and to prevent small sources such as Wikipedia from being over-sampled.\n",
            "As pointing out earlier, the prediction of next token is not same as the thinking process. Interestingly, some reasoning and arithmetic ability of LLMs can be unlocked by Chain-of-thought prompting. A chain of thought is a series of intermediate natural language reasoning steps that lead to the final output. Sufficiently large language models can generate chains of thought if demonstrations of chain-of-thought reasoning are provided in the exemplars for few-shot prompting: ⟨input, chain of thought, output⟩. Why and how it works is not clear to us though.\n",
            "The language modeling objective used for LLMs — predicting the next token — is different from the objective “follow the user’s instructions helpfully and safely”. Thus, we say that the language modeling objective is misaligned.\n",
            "InstructGPT aligns language models with user intent on a wide range of tasks by using reinforcement learning from human feedback (RLHF). This technique uses human preferences as a reward signal to fine-tune models.\n",
            "Step 1: Collect demonstration data, and train a supervised policy. Labelers provide demonstrations of the desired behavior on the input prompt distribution. Then fine-tune a pre-trained GPT3 model on this data using supervised learning.\n",
            "Step 2: Collect comparison data, and train a reward model. Collect a dataset of comparisons between model outputs, where labelers indicate which output they prefer for a given input. Then train a reward model to predict the human-preferred output.\n",
            "Step 3: Optimize a policy against the reward model using PPO. Use the output of the RM as a scalar reward. Fine-tune the supervised policy to optimize this reward using the PPO algorithm.\n",
            "Steps 2 and 3 can be iterated continuously; more comparison data is collected on the current best policy, which is used to train a new RM and then a new policy.\n",
            "While supervised fine-tuning introduced in GPT-1 focuses on task specific tuning, T5 is trained with a maximum likelihood objective (using “teacher forcing”) regardless of the task. Essentially, T5 leverages the same intuition as zero-shot transfer that NLP tasks can be described via natural language instructions, such as “Is the sentiment of this movie review positive or negative?” or “Translate ‘how are you’ into Chinese.” To specify which task the model should perform, T5 adds a task-specific (text) prefix to the original input sequence before feeding it to the model. Further, FLAN explores instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data.\n",
            "For each dataset, FLAN manually composes ten unique templates that use natural language instructions to describe the task for that dataset. While most of the ten templates describe the original task, to increase diversity, for each dataset FLAN also includes up to three templates that “turned the task around,” (e.g., for sentiment classification we include templates asking to generate a movie review). We then instruction tune a pretrained language model on the mixture of all datasets, with examples in each dataset formatted via a randomly selected instruction template for that dataset.\n",
            "The so-called prompt engineering is essentially a reverse engineering how the training data are prepared for instruction fine-tuning and in context learning.\n",
            "Due to the cost and time, LLMs in production usages are often lagged in term of training data freshness. To address this issue, we may use LLMs in the way of Retrieval Augmented Generation (RAG). In this use case, we do not want the LLM to generate text based solely on the data it was trained over, but rather want it to incorporate other external data in some way. With RAG, LLMs can also answer (private) domain specific questions. Therefore, RAG is also referred as “open-book” question answering. LLM + RAG could be an alternative to classic search engine. In other word, it acts as information retrieval with hallucination.\n",
            "Currently, the retrieval part of RAG is often implemented as k-nearest neighbor (similarity) search on a vector database that contains the vector embedding of external text data. For example, DPR formulates encoder training as a metric learning problem.However, we should notice the information retrieval is generally based on relevance, which is different from similarity. I expect that there will be many more improvements in this area in the future.\n",
            "LLM is an exciting area and will experience rapid innovations. I hope that this post helps you a little bit understand how it works. Besides excitement, we should also notice that LLMs learn language in a very different way from humans — they lack access to the social and perceptual context that human language learners use to infer the relationship between utterances and speakers’ mental states. They are also trained in a different way from human’s thinking process. These could be the areas to improve LLMs or to invent new paradigms of learning algorithms.\n"
        ]
    },
    {
        "link": "https://medium.com/@gathnex/fine-tuning-llama-2-llm-on-google-colab-a-step-by-step-guide-dd79a788ac16?source=list-ce01ef2736c2--------6-------847f40a2cdd4---------------------",
        "title": "Fine-Tuning Llama-2 LLM on Google Colab: A Step-by-Step Guide.",
        "subtitle": "false",
        "autorName": "Gathnex",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/0*oHgD_0zXXL0OKuRY",
        "clap": "186",
        "response": "2",
        "timeForRead": "12 min read",
        "dateCreate": "Sep 18",
        "text": [
            "Llama, Llama, Llama: 🦙 A Highly Speakable Model in Recent Times. 🗣️ Llama 2: 🌟 It’s like the rockstar of language models, developed by the brilliant minds over at Meta. But what makes it so special? Well, it’s not just one model; it’s a family of models, ranging from 7 billion to 70 billion parameters. Think of it as the Swiss Army knife of AI language models. While it’s built on the trusted Google transformer architecture, Meta added their own secret sauce to make it even more powerful. Whether you’re looking to chat with an AI assistant or need a language model for a specific task, Llama 2 has you covered.\n",
            "Now, let’s talk about Llama 2’s journey to greatness. It started with some serious training — we’re talking a massive dataset of text and code, including books, articles, code repositories, and more. But what truly sets it apart is the fine-tuning process, where it learned from over 1 million human annotations. This is where it honed its skills, becoming incredibly accurate and fluent. And guess what? Llama 2 doesn’t just shine in the lab; it outperforms other open-source language models in various real-world tests. The best part is that you can use it for research and commercial applications, making it a versatile tool with boundless potential. So, buckle up, because Llama 2 is on a mission to redefine the AI landscape.\n",
            "Let’s understand the LLM’s Training process.\n",
            "There is mainly 2 steps:-\n",
            "Pre-training: It’s like teaching a language model the ABCs of language by exposing it to a massive amount of text from the 🌐 internet. Think of it as giving the model a broad understanding of grammar 📝, vocabulary, and common patterns in language . During this phase, the model learns to predict what comes next in a sentence 🤖, helping it grasp the structure of language 🧠. It’s like teaching a student the ABCs before moving on to reading books 📖.\n",
            "Fine-tuning ✨: Fine-tuning on the other hand, is where the magic happens. After the model has a general understanding of language from pre-training, fine-tuning narrows its focus. It’s like taking that well-rounded student and giving them specific lessons for a particular task. For example, you might fine-tune the model to be an expert in answering questions or generating code. It’s like guiding that student to excel in a specific subject in school. Fine-tuning adapts the general language knowledge gained during pre-training to perform specific tasks accurately and effectively.\n",
            "After Fine-tuning, the model still we had a problem. These include occasional generation of incorrect or nonsensical information, sensitivity to input phrasing, susceptibility to bias present in the fine-tuning data, and difficulty handling nuanced context in complex conversations. Additionally, models can struggle with generating coherent long-form content, which can affect their suitability for certain applications like content generation and chatbots. These limitations highlight the need for ongoing research and development efforts to refine fine-tuned models and address these issues for more reliable and ethical AI applications.\n",
            "Responsible AI is our goal🎯 not only a Fine-tuned model.\n",
            "Reinforcement Learning from Human Feedback : RLHF is like giving your language model a tutor🎓. After pre-training and fine-tuning, RLHF steps in to provide additional training. It’s a bit like having a teacher review and grade the model’s responses to improve them further. Human feedback, in the form of evaluations and corrections✅, helps the model learn from its mistakes and refine its language skills. Just as students benefit from feedback to excel in their studies, RLHF helps language models become even better at specific tasks by learning from human guidance.\n",
            "so look like lot of hard work needed for RLHF. so new buddy entering the game🎮.\n",
            "DPO Direct Preference Optimization is a new technique🤝 designed to address the limitations of Reinforcement Learning from Human Feedback (RLHF) in fine-tuning large language models (LLMs). Unlike RLHF, which relies on complex reward function learning, DPO simplifies the process by treating it as a classification problem based on human preference data.\n",
            "The Biggest Misconception: Fine-Tuning LLM on Google Colab 🧐.\n",
            "Let’s debunk a common myth 🌟: Yes, you can fine-tune a Language Model (LLM) on the free version of Google Colab, but with a catch! 🙅‍♂️\n",
            "Here’s the scoop: Google Colab offers a free environment, but there are time limits ⏳. You get a generous 12-hour window to run your code continuously after that it automatically disconnected , but here’s the twist — if there’s no activity, it disconnects after just 15–30 minutes of inactivity ⏱️. Colab also has a GPU limitation; you can only use GPUs for around 12 hours/day.\n",
            "Fine-tuning a large LLM on Google Colab’s free version? Not the easiest feat! 🤯 Due to these constraints, you might find yourself limited to fine-tuning smaller LLMs with smaller datasets, often maxing out at around 2 epochs ⚙️ with 10k samples will be difficult. So, while it’s possible, it can be quite challenging to fine-tune a substantial LLM using Google Colab’s free tier. 🚀\n",
            "We are going to use 🦙Llama-2–7B-HF, a pre-trained small model in the Llama-2 family, for fine-tuning with Qlora technique.\n",
            "QLoRA (Quantized Low-Rank Adaptation) is an extension of LoRA (Low-Rank Adapters) that uses quantization to improve parameter efficiency during fine-tuning. QLoRA is more memory efficient than LoRA because it loads the pretrained model to GPU memory as 4-bit weights, compared to 8-bits in LoRA. This reduces memory demands and speeds up calculations.\n",
            "In simple terms, we’re not going to train the entire model. 🚂 Instead, we’ll add an adapter in between the model and train only that adapter. 🧩 This way, we can fine-tune the LLM on the consumer GPU, 🎮 and it’s also a faster training process. ⏩\n",
            "The system setup we used to fine-tune a model included a Tesla V100 32GB GPU, and it ran on an Ubuntu VM. If you want to set up a similar VM for training LLMs, feel free to reach out to us Email : gathnexorg@gmail.com📧.\n",
            "Install required packages\n",
            "Import libraries\n",
            "Check system spec\n",
            "output:-\n",
            "Setting the model variable\n",
            "Log into hugging face hub\n",
            "Note : You need to enter the access token, before that you need to apply for access the llama-2 model in Meta website.\n",
            "Monitoring\n",
            "Apart from training, monitoring is a crucial part we need to consider in LLM training🚧.\n",
            "To get started, create a WandB account. Click here to log in🔗. After creating your account, enter the authorization token🔑 here.\n",
            "Load dataset\n",
            "We are utilizing the pre-processed dataset vicgalle/alpaca-gpt4 from Hugging Face.\n",
            "Loading the model and tokenizer\n",
            "We are going to load a Llama-2–7B-HF pre-trained model with 4-bit quantization, and the computed data type will be BFloat16.\n",
            "Lora config\n",
            "Training arguments\n",
            "SFTT Trainer arguments\n",
            "We’re all set to begin the training process.\n",
            "You can now monitor various training metrics, including loss, GPU usage, RAM usage, and more, directly on the WandB website. The link will be provided when you initiate above code.\n",
            "The UI look like,\n",
            "Now, at this crucial phase, it’s imperative to closely monitor the training loss. If the loss starts to exhibit unusual behaviour or anomalies🚨, it’s a signal to consider stopping the training. Overfitting is a common concern in such cases, and it may be necessary to fine-tune hyperparameters and retry to achieve the best results📉.\n",
            "Good training loss\n",
            "This is an image depicting our training loss, showcasing favourable trends. 📈While there may be occasional spikes, we have effectively applied exponential moving average to mitigate them. 🧮The total number of epochs used for training was 5. So the primary goal is being the gradual reduction of the loss curve over time🎯.\n",
            "Bad Training loss\n",
            "This training graph indicates overfitting as the training loss oscillates between 2 and 3. 🔴This could be due to issues such as inadequately pre-processed data or suboptimal hyperparameter settings🛠️ etc.\n",
            "In many articles📝, we often come across training loss curves that appear unusual and resemble overfitting, yet they are included in the blog posts🌐. This highlights the importance of exercising caution when utilizing code from the internet.\n",
            "What after training ?\n",
            "So, after training is completed, we need to save the model for testing. It only saves the trained adapter.\n",
            "Let’s test the model\n",
            "Output:-\n",
            "🤖 The results we’ve obtained 📊 reflect the model’s performance during testing. However, one of the primary challenges we’re facing with this model pertains to the stopping criteria used during its training 🛑. We’re committed to addressing this issue through ongoing research and development efforts 🔍, with the aim of identifying and implementing more optimal parameters ⚙️. While the model has demonstrated promise 🤝, it’s important to acknowledge that there is room for improvement in its performance 📈.\n",
            "Upload a model to hugging face repository\n",
            "Step 1 : Once you are finished training your model, you can use the code you provided to free this memory. This is important because it can help to prevent your computer from running out of memory, and it can also improve the performance of other programs that you are running.\n",
            "Step 2: Merging the adapter with model.\n",
            "Step 2 : Pushing the merged model to hugging face hub\n",
            "Conclusion\n",
            "In conclusion, our assessment indicates that the model’s performance is promising but falls short of being outstanding. Recognizing this, our team remains dedicated to continuous Research and Development (R&D) efforts to craft a superior model. 🌟 We are committed to providing more effective solutions for Language Models (LLMs) that cater to the needs of AI enthusiasts and practitioners. 🧠\n",
            "It’s essential to highlight that fine-tuning a model on platforms like Google Colab comes with its set of challenges. 🤯 The time limitations and resource constraints can make this task a formidable one. However, our team is actively exploring ways to navigate these difficulties, aiming to make fine-tuning on such platforms more accessible and efficient for everyone. 🕒📈\n",
            "In essence, our journey in the world of LLMs continues, fueled by the desire to deliver superior models and streamline the fine-tuning process. 💡 Stay tuned for more exciting updates! 📢\n",
            "Thanks & Regards to Gathnex team🎉.\n",
            "Additionally, we’d like to clarify that we’ve utilized certain images from the internet to enhance our explanations for the audience’s better understanding. We want to extend our credits and appreciation🙏 to the original owners of these images🖼️.\n",
            "Google colab\n",
            "Reference : Huggingface\n"
        ]
    },
    {
        "link": "https://medium.com/@iryna230520/dynamic-few-shot-prompting-overcoming-context-limit-for-chatgpt-text-classification-2f70c3bd86f9?source=list-590702974d3e--------33-------4cad45bc2839---------------------",
        "title": "Dynamic Few-Shot Prompting: Overcoming Context Limit for ChatGPT Text Classification",
        "subtitle": "false",
        "autorName": "Iryna Kondrashchenko",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*dXzJ63aPs4BwAfahw6WYMg.jpeg",
        "clap": "70",
        "response": "1",
        "timeForRead": "6 min read",
        "dateCreate": "Jun 12",
        "text": [
            "Recent explosion in the popularity of large language models like ChatGPT has led to their increased usage in classical NLP tasks like language classification. This involves providing a context (sample) and candidate labels to the model to reason about. More precisely, this approach is called zero-shot classification, which implies that the model does not have to be retrained to generalize to new/unseen categories. Naturally, not having to fine-tune a model for a specific task can be highly beneficial from a business perspective, as it significantly reduces development time as well as the additional burden of maintaining custom models.\n",
            "However, one downside of the zero-shot approach is its limited ability to leverage existing labeled samples (training data). In this article, we will see how to solve this problem using dynamic few-shot prompting, which includes only a relevant subset of the training data in the prompt.\n",
            "All code examples in this article use the Scikit-LLM library. Please check my previous post for more information.\n",
            "Before diving into dynamic few-shot prompting, let’s briefly recap the concepts of zero-shot and few-shot prompting.\n",
            "Zero-shot prompting is the method where ChatGPT, or any other language model, is used to classify text without any additional task-specific training. It involves framing a question or task for the model and providing it with options to choose from. Essentially, the model uses the knowledge it was trained with to complete the task.\n",
            "A very simple zero-shot prompt can look like this:\n",
            "Under the hood, a `ZeroShotGPTClassifier` from Scikit-LLM also uses zero-shot prompting and allows allows building an estimator in just three lines of code:\n",
            "On the other hand, few-shot prompting gives ChatGPT several examples alongside the input. This serves to contextualize the task and offer direction for the model’s responses. Seeing these examples, the model understands the expected output, which improves its performance across a range of tasks.\n",
            "Example prompt:\n",
            "Example code:\n",
            "While few-shot prompting looks great on paper, as it allows the use of information from the training dataset to make predictions, it has significant scalability issues.\n",
            "To understand the problem, let’s take a sample from the previous few-shot prompt.\n",
            "If we pass this through the web interface of the OpenAI tokenizer, we can see that the text corresponds to 19 tokens.\n",
            "Why is this important?\n",
            "Firstly, modern LLMs have limited context length. For example, gpt-3.5-turbo, the most popular OpenAI model, has a context limit of 4096 tokens, while most of the current generation open-source models are limited to 2048 tokens. Given that our sample was 19 tokens long, we could provide at most 215 samples to gpt-3.5-turbo or 107 samples to an open-source LLM like LLaMA. In real scenarios, this number will be even lower since we do not account for additional tokens consumed by the prompt itself. Additionally, the text samples that need to be classified are often much longer.\n",
            "Even if the context size was unlimited, processing longer prompts would require more computational resources, which is usually associated with higher financial costs.\n",
            "A very natural solution to the problem is to use only a subset of the training data for the prompt itself.\n",
            "This is exactly what `DynamicFewShotGPTClassifier` does. During inference, for each unlabeled datapoint, it dynamically selects N training examples from each class to be used in the prompt.\n",
            "To better understand how it works let’s consider a toy example, where the goal is to figure out whether the person is talking about the books or movies.\n",
            "These are the examples that were automatically picked by the classifier to be included in the prompt:\n",
            "Notice how both of the examples are clearly similar to the query as the person is talking about the science-fiction genre in all of the cases.\n",
            "But how exactly does it select examples dynamically based on the new input?\n",
            "This is achieved by adding a classical KNN-like algorithm as an additional preprocessor. If we assume that the most relevant examples are the most similar ones, then the problem reduces to a nearest neighbors search and can be tackled in three steps:\n",
            "(1) Vectorization\n",
            "Before doing the nearest neighbors search, the training set must be embedded into fixed-dimensional vectors. This can easily be achieved using the OpenAI embedding API (or any other alternative).\n",
            "(2) Index construction\n",
            "While any nearest neighbor algorithm can be used for this task, even the brute force option from scikit-learn, it is important to keep in mind the scalability aspects as we are dealing with very high-dimensional data and potentially a high number of samples as well. Luckily, there are plenty of tools that handle these scenarios extremely well. For example, Annoy, a library for fast approximate nearest neighbors search developed by Spotify. By constructing the index once during training, it is possible to perform fast neighbors searches during inference.\n",
            "(3) Balanced sampling\n",
            "The last thing to be accounted for is class balancing. If only N nearest neighbors are selected for a few-shot prompting, there is a very high risk that some of the classes will be underrepresented or missing completely. To mitigate this issue, instead of creating a single index, the training data is partitioned by class. In this way, we are able to sample N examples from each class, ensuring the equal representation of each class.\n",
            "In this article, we explored the concept of dynamic few-shot prompting, an enhancement of the zero-shot and few-shot prompting approaches for text classification with large language models like ChatGPT. We learned how it utilizes the existing labeled data to improve classification accuracy by dynamically selecting relevant examples to include in the prompt. By leveraging a KNN-like algorithm, the dynamic few-shot prompting process creates a balance between using the inherent capacity of language models to generalize from limited examples, and the need to process large-scale data for practical applications.\n",
            "Furthermore, it is an efficient method that adapts to the constraints of context length in current language models and optimizes the usage of computational resources.\n"
        ]
    },
    {
        "link": "https://medium.com/@scholarly360/mistral-7b-complete-guide-on-colab-129fa5e9a04d?source=list-ce01ef2736c2--------4-------847f40a2cdd4---------------------",
        "title": "Mistral-7b Complete Guide on Colab",
        "subtitle": "false",
        "autorName": "Yogendra Sisodia",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*70Yqvxho4IuRlEDbp6m41w.png",
        "clap": "66",
        "response": "1",
        "timeForRead": "4 min read",
        "dateCreate": "Oct 1",
        "text": [
            "Introduction\n",
            "The performance of Mistral 7B surpasses that of Llama 2 13B across all criteria and is comparable to Llama 34B. Furthermore, it exhibits significant superiority in terms of code quality and logical analysis benchmarks.\n",
            "An LLM that is open source provides a level of transparency with respect to its operational mechanisms, architectural design, training data, and methodology, as well as its utilization. Moreover, the efficient optimization of an open-source LLM like Mistral-7B has the potential to decrease latency and enhance performance.\n",
            "I am excited to explore Mistral-7B instruct, and I am covering the following technologies in this article, along with a walkthrough video:\n",
            "Colab Snippets\n",
            "Lang-Chain Integration and BitsAndBytes\n",
            "Retrieval Augmented Generation (RAG) with Chroma-DB\n",
            "GGUF Format\n",
            "Conclusion\n",
            "Organizations have the ability to incorporate additional functionalities into the open-source LLM that might cater to their particular requirements. Furthermore, the LLMs can be trained using specific datasets. Implementing modifications or requirements on a proprietary LLM system necessitates collaboration with a vendor, resulting in the expenditure of both time and financial resources. Mistral-7B is definitely a strong candidate for the open-source community and I am excited.\n",
            "Resources (Colab and Walkthrough Video)\n"
        ]
    },
    {
        "link": "https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476?source=list-ad8a629da509--------3-------ef5881be2446---------------------",
        "title": "Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data",
        "subtitle": "false",
        "autorName": "Murtuza Kazmi",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*gv6a2JEm9xAALB50XSExJA.jpeg",
        "clap": "1K",
        "response": "11",
        "timeForRead": "9 min read",
        "dateCreate": "Jul 24",
        "text": [
            "Over the past few weeks, I have been playing around with several large language models (LLMs) and exploring their potential with all sorts of methods available on the internet, but now it’s time for me to share what I have learned so far!\n",
            "I was super excited to know that Meta released the next generation of its open-source large language model, LLaMA 2 (on 18th July 2023) and the most interesting part of the release was, they made it available free of charge for commercial use to the public. Therefore, I decided to try it out and see how its performs.\n",
            "In this article, I’m going share on how I performed Question-Answering (QA) like a chatbot using Llama-2–7b-chat model with LangChain framework and FAISS library over the documents which I fetched online from Databricks documentation website.\n",
            "LLaMA 2 model is pretrained and fine-tuned with 2 Trillion 🚀 tokens and 7 to 70 Billion parameters which makes it one of the powerful open source models. It comes in three different model sizes (i.e. 7B, 13B and 70B) with significant improvements over the Llama 1 models, including being trained on 40% more tokens, having a much longer context length (4k tokens 🤯), and using grouped-query attention for fast inference of the 70B model 🔥. It outperforms other open source LLMs on many external benchmarks, including reasoning, coding, proficiency, and knowledge tests.\n",
            "LangChain is a powerful, open-source framework designed to help you develop applications powered by a language model, particularly a large language model (LLM). The core idea of the library is that we can “chain” together different components to create more advanced use cases around LLMs. LangChain consists of multiple components from several modules.\n",
            "Modules:\n",
            "FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors. It can search multimedia documents (e.g. images) in ways that are inefficient or impossible with standard database engines (SQL). It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning.\n",
            "In this section, I will briefly describe each part of the process flow.\n",
            "In this section, I will go through the code to explain you each step in detail.\n",
            "You can use the open source Llama-2-7b-chat model in both Hugging Face transformers and LangChain. However, you have to first request access to Llama 2 models via Meta website and also accept to share your account details with Meta on Hugging Face website. It typically takes a few minutes or hours to get the access.\n",
            "🚨 Note that your Hugging Face account email MUST match the email you provided on the Meta website, or your request will not be approved.\n",
            "If you’re using Google Colab to run the code. In your notebook, go to Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4. You will need ~8GB of GPU RAM for inference and running on CPU is practically impossible.\n",
            "First of all, let’s start by installing all required libraries using pip install.\n",
            "You have to initialize a text-generation pipeline with Hugging Face transformers. The pipeline requires the following three things that you must initialize:\n",
            "You have to initialize the model and move it to CUDA-enabled GPU. Using Colab, this can take 5–10 minutes to download and initialize the model.\n",
            "Also, you need to generate an access token to allow downloading the model from Hugging Face in your code. For that, go to your Hugging Face Profile > Settings > Access Token > New Token > Generate a Token. Just copy the token and add it in the below code.\n",
            "The pipeline requires a tokenizer which handles the translation of human readable plaintext to LLM readable token IDs. The Llama 2 7B models were trained using the Llama 2 7B tokenizer, which can be initialized with this code:\n",
            "Now, we need to define the stopping criteria of the model. The stopping criteria allows us to specify when the model should stop generating text. If we don’t provide a stopping criteria the model just goes on a bit tangent after answering the initial question.\n",
            "You have to convert these stop token ids into LongTensor objects.\n",
            "You can do a quick spot check that no <unk> token IDs (0) appear in the stop_token_ids — there are none so we can move on to building the stopping criteria object that will check whether the stopping criteria has been satisfied — meaning whether any of these token ID combinations have been generated.\n",
            "You are ready to initialize the Hugging Face pipeline. There are a few additional parameters that we must define here. Comments are included in the code for further explanation.\n",
            "Run this code to confirm that everything is working fine.\n",
            "Now, you have to implement the Hugging Face pipeline in LangChain. You will still get the same output as nothing different is being done here. However, this code will allow you to use LangChain’s advanced agent tooling, chains, etc, with Llama 2.\n",
            "You have to ingest data using WebBaseLoader document loader which collects data by scraping webpages. In this case, you will be collecting data from Databricks documentation website.\n",
            "You have to make sure to split the text into small pieces. You will need to initialize RecursiveCharacterTextSplitter and call it by passing the documents.\n",
            "You have to create embeddings for each small chunk of text and store them in the vector store (i.e. FAISS). You will be using all-mpnet-base-v2 Sentence Transformer to convert all pieces of text in vectors while storing them in the vector store.\n",
            "You have to initialize ConversationalRetrievalChain. This chain allows you to have a chatbot with memory while relying on a vector store to find relevant information from your document.\n",
            "Additionally, you can return the source documents used to answer the question by specifying an optional parameter i.e. return_source_documents=True when constructing the chain.\n",
            "Now, it’s time to do some Question-Answering on your own data!\n",
            "Output:\n",
            "This time your previous question and answer will be included as a chat history which will enable the ability to ask follow up questions.\n",
            "Output:\n",
            "You can also see the source of the information used to generate the answer.\n",
            "Output:\n",
            "Et voilà! You have now the capability to do question-answering on your on data using a powerful language model. Additionally, you can further develop it into a chatbot application using Streamlit.\n",
            "[1] https://huggingface.co/blog/llama2\n",
            "[2] https://venturebeat.com/ai/llama-2-how-to-access-and-use-metas-versatile-open-source-chatbot-right-now/\n",
            "[3] https://www.pinecone.io/learn/series/langchain/langchain-intro/\n",
            "[4] https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/\n",
            "[5] https://ai.meta.com/tools/faiss/\n",
            "[6] https://blog.bytebytego.com/p/how-to-build-a-smart-chatbot-in-10\n",
            "[7] https://newsletter.theaiedge.io/p/deep-dive-building-a-smart-chatbot\n",
            "[8] https://www.youtube.com/watch?v=6iHVJyX2e50\n",
            "[9] https://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/llama-2/llama-2-70b-chat-agent.ipynb\n"
        ]
    },
    {
        "link": "https://medium.com/@kelvin.lu.au/disadvantages-of-rag-5024692f2c53?source=list-412323769000--------0-------07f9e4e23822---------------------",
        "title": "Disadvantages of RAG",
        "subtitle": "false",
        "autorName": "Kelvin Lu",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*FrL61XBGRKjEzvM7O1Lxtg.jpeg",
        "clap": "287",
        "response": "7",
        "timeForRead": "9 min read",
        "dateCreate": "Aug 25",
        "text": [
            "This is the first part of the RAG analysis:\n",
            "Recently, the rise of large language models (LLMs) has sparked a lot of interest in RAG systems. Many practitioners are eager to learn how RAG can benefit their own organisations, and some businesses have already released RAG-based services. In my previous posts, I addressed my research on how to host and fine-tune a project-specific embedding model[1, 4] and some of the considerations for developing a vector database, which is the cornerstone of the RAG system[1]. In this article, I will explore some of the limitations of RAG systems.\n",
            "If you are unfamiliar with RAG and would like to quickly get an idea of how it works in a case study, please check out[2].\n",
            "Table of Contents\n",
            "· It Starts With Semantic Search· The Chunk Size and Top-k· World Knowledge· Multi-hop Q&A· Information Loss· Conclusion· References\n",
            "Before we go any further, let's do an experiment. The following code piece compares the cosine similarity score of a query against a series of statements. It uses GCP VertexAI’s textembedding-gecko001 model to produce 768-dimensional embedding vectors.\n",
            "And if we use the above code pieces to try the following data:\n",
            "The output is the following:\n",
            "Surprise, surprise! When we ask when not to use SVM, the semantic search returns the advantages of SVM. And let’s have another example:\n",
            "The algorithm not only disregarded the sentimental difference; it was also very sensitive to language nuances like plural vs. singular. And these experiments reveal the limitation of the RAG: semantic similarity search is not magic, as with many other machine learning technologies.\n",
            "The embedding vector we got from the embedding model is the top layer weights of the LLM. One thing we need to notice is that the embedding LLM and the generative LLM are different. The embedding models were designed to predict masked segments in the input text. Therefore, they can learn the intention of the input text. And these types of LLM are called autoencoders. While the generative LLM was designed to predict the next token based on the prior input string. And these types of LLM are called autoregressors. ChatGPT, Google Palm, and Llama are all autoregressors.\n",
            "The embedding models, or autoencoders, learn input data features into the weights, which we call embedding vectors. We found that the embedding vectors attract important information from the input text, and the vector similarity can be used to compare the closeness of the texts. Nevertheless, we don’t know what information has been extracted or how the information was organised in the vector, let alone how to make it more efficient or develop a more accurate similarity function.\n",
            "As a consequence, please be prepared that semantic similarity searches may miss the goal from time to time. Assuming semantic search will always retrieve reasonable results is unrealistic.\n",
            "A sophisticated RAG should support flexible chunking and may add a little bit of overlap to prevent information loss. Generally speaking, the chunking process disregards the content of the text, and that causes a problem. The ideal content of the chunk should be consistent around a single topic for the embedding models to work better. They should not jump from one topic to another; they should not change the scenes. As depicted in the SVM test case, the model prefers short and polarised input.\n",
            "Then how about we choose all small chunks? In this case, we need to consider the impact of the parameter top_k. RAG systems use top_k to choose how many top-scored chunks to feed into the generative LLM. In most designs, top_k is a fixed number. Therefore, if the chunk size is too small or the information in the chunks is not dense enough, we may not be able to extract all the necessary information from the vector database.\n",
            "To people who are familiar with machine learning model tuning, does the pair of chunk size and top_k ring a bell? They look like the machine learning model's superparameters, don’t they? To make sure the RAG systems perform at their best, the chunk-size and top_k do need to be tuned to make sure they are the best fit. The old wisdom of superparameter tuning still apply, the only difference is that they are way more expensive to tune.\n",
            "Consider the scenario that we are building a Harry Potter Q&A system. We have imported all Harry Potter stories into a vector database. Now, a question arises: how many heads does a dog have?\n",
            "Most likely, the system will answer three because there are mentions of a huge dog that has three heads, and the system has no idea how many heads a normal dog may have.\n",
            "Therefore, don't let the idea that the LLMs already know the solution fool you when we develop RAG systems. They don’t.\n",
            "Let’s consider another scenario: we built a RAG system based on social media. Then we request: Who knows Elon Musk? Then the system will iterate through the vector database to extract a list of contacts for Elon Musk. Because of the limits of the chunk size and top_k, we can expect the list to be incomplete; nevertheless, functionally, it works.\n",
            "Now, if we reframe our question and ask: Who can introduce Johnny Depp to Elon Musk, except Amber Heard? A single round of information retrieval cannot answer that kind of question. This type of question is called multi-hop Q&A. One way to solve it is:\n",
            "There are several architectures to accommodate this complicated algorithm; one of them uses sophisticated prompt engineering like ReACT, and another uses an external graph database to assist the reasoning. We just need to know that this is one of the limits of RAG systems.\n",
            "If we look at the chain of processes in the RAG system:\n",
            "1. Chunking the text and generating embedding for the chunks\n",
            "2. Retrieving the chunks by semantic similarity search\n",
            "3. Generate response based on the text of the top_k chunks\n",
            "We will see that all the processes are lossy, which means there’s no guarantee that all information will be preserved in the result. As discussed above, chunking and embedding were lossy because of the selection of the chunk size and the power of embedding models; the retrieving process couldn’t be perfect because of the top_k limit and the similarity function we used; and the response generation process was imperfect because of the content length limit and the power of the generative LLMs.\n",
            "If we put all the limits together and rethink the RAG-based enterprise search some companies are going to roll out, I’m really curious how much they could be better than the traditional full-text search engine. Bear in mind that the traditional search engine is very tough to beat. Microsoft E5 was the first LLM to surpass BM25, the popular search algorithm, not long ago.\n",
            "What I mean is that the marriage of search engines and LLM is doable; however, it’s too difficult for simple RAG to perform better than search engines.\n",
            "RAG, as a simple and powerful LLM application design pattern, has its pros and cons. We do need to know the technology inside out to be confident in our design. My personal take is that despite all the hype about LLM and the amazing breakthroughs, LLMs should be placed as important components of the enterprise AI architecture. They shouldn’t be the main framework itself.\n",
            "The limited power of the LLMs is one of my concerns, and explainability is another. All LLMs work like black boxes. People have no visibility into how they store their knowledge or how they reason. This is not a major issue for no-obligation applications, but it’s critical in enterprise settings. We can see that more and more regulatory rules were released to make sure the AI was doing no harm. We just need to do our due diligence in our project work.\n",
            "In future research, I’m going to explore how to hybrid LLM with other external knowledge bases like graph databases to achieve harder-to-reach goals.\n"
        ]
    },
    {
        "link": "https://medium.com/@3minutesnapshot/top-5-vector-databases-and-when-to-use-them-6c321e8ccc33?source=list-499d6e31470a--------1-------aa9880ad746d---------------------",
        "title": "Top 5 vector databases and when to use them",
        "subtitle": "Vector databases are the future for semantic search, similarity search, clustering, and recommendations for both text and images. Here’s how and when to use them.",
        "autorName": "3 Minute Snapshot",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*tQ1vUPBqLOVkmxnOqHGUTQ.png",
        "clap": "101",
        "response": "2",
        "timeForRead": "10 min read",
        "dateCreate": "Jul 14",
        "text": [
            "Vector databases have been the hot new thing in the database space for a while now. Even PostgreSQL has added an extension, pgvector, with support for vector fields and cosine similarity search. Vector databases power a laundry list of use cases for your business:\n",
            "This is accomplished through the magic of embeddings, which convert a text or an image into a high-dimensional vector. In this article, we will be working with text — if you would like me to do a writeup of how image embeddings work, then leave a comment.\n",
            "This gives us the opportunity to work with text and images using the vast array of numerical tools that computers excel at. Most importantly, we can use what is known as cosine distance to find which embeddings are similar to each other.\n",
            "A pair of embeddings which have low cosine distance between them are more semantically similar than a pair of embeddings which have a high distance between them.\n",
            "Embeddings are often have between 384 and 1536 dimensions, and they may or may not be normalized depending on the actual model used for embedding. They are generated using an embeddings model, a model that is trained to transform text into this kind of high-dimensional vector while preserving the meaning of the text in relation to other texts.\n",
            "Some embeddings models are simple by-products of training large language models, while other embeddings models are fine-tuned specifically for the task of enabling efficient and accurate semantic search by generating high-quality embeddings.\n",
            "The make-or-break of your semantic search, clustering, or recommendations feature is often…\n"
        ]
    },
    {
        "link": "https://medium.com/@colabdoge/what-is-rag-retrieval-augmented-generation-b0afc5dd5e79?source=list-499d6e31470a--------6-------aa9880ad746d---------------------",
        "title": "What is RAG (Retrieval-Augmented Generation)?",
        "subtitle": "false",
        "autorName": "Jacky",
        "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*cW1ivsEi8JwN1ccnQm3cFw.jpeg",
        "clap": "196",
        "response": "1",
        "timeForRead": "3 min read",
        "dateCreate": "Jun 24",
        "text": [
            "Retrieval-augmented generation is a technique used in natural language processing that combines the power of both retrieval-based models and generative models to enhance the quality and relevance of generated text.\n",
            "To understand retrieval-augmented generation, let’s break it down into its two main components: retrieval models and generative models.\n",
            "Now, retrieval-augmented generation combines these two approaches to overcome their individual limitations. In this framework, a retrieval-based model is used to retrieve relevant information from a knowledge base or a set of documents based on a given query or context. The retrieved information is then used as input or additional context for the generative model.\n",
            "By incorporating the retrieved information, the generative model can leverage the accuracy and specificity of the retrieval-based model to produce more relevant and accurate text. It helps the generative model to stay grounded in the available knowledge and generate text that aligns with the retrieved information.\n",
            "About retrieval models\n",
            "Retrieve models are a type of language model that focus on finding relevant information from a dataset, in response to a given query. These models can benefit from vast stores of knowledge and are usually trained to produce meaningful and context-specific results. The most common examples of retrieval models:\n",
            "Retrieval models are generally designed to find and rank relevant pieces of information from a dataset in response to a query. Here are some examples of popular retrieval models and algorithms:\n",
            "Applications\n",
            "Retrieval-augmented generation has several applications. For example, in question-answering systems, the retrieval-based model can find relevant passages or documents containing the answer, and the generative model can then generate a concise and coherent response based on that information. In content generation tasks, such as summarization or story writing, the retrieval-based model can provide relevant facts or context, which the generative model can use to create more informative and engaging content.\n",
            "In summary, retrieval-augmented generation combines the strengths of retrieval-based models and generative models to improve the quality and relevance of generated text. By leveraging the retrieval-based model’s ability to find accurate information and the generative model’s ability to produce creative text, this approach enables more robust and contextually grounded language generation systems.\n",
            "Building your own RAG engine\n",
            "There are a few solutions out there where you can test building your own RAG engine (I will be writing and sharing my experiences on these soon!).\n"
        ]
    }
]
