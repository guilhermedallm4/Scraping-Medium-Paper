[{"link": "https://medium.com/@indusnet/what-to-know-about-semantic-search-using-nlp-be387c688ec9?source=list-2c27d980d3f3--------24-------338c7da11cbf---------------------", "title": "What To Know About Semantic Search Using NLP", "subtitle": "false", "autorName": "Indus Net Technologies", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*xfG8KptyoIUcr_1DOl2BcQ.png", "clap": "6", "response": "1", "timeForRead": "4 min read", "dateCreate": "Jan 12, 2022", "text": ["Have you used your application or search engine to understand the underlying meaning behind your query? If yes, the solution to this requirement is through Semantic Search. A couple of years ago, a simple keyword search would have yielded search results matching just the keywords. We call it  lexical search . Today, we can have machines and applications understand the semantics behind a query through Natural Language Processing (NLP). The credit goes to the Artificial Intelligence revolution.\n", "Let s say you search the nursery rhyme,  Hey Diddle Diddle  on Google. And the search results will return both lexical and semantic instances of it. The former is an example of computational information retrieval below semantic search. So, we can say that  Semantic search describes a search engine s attempt to generate the most accurate Search Engine Results Page (SERP) results possible by understanding based on searcher intent, query context, and the relationship between words. \n", "Through the superset of machine learning, we have the following abilities today:\n", "Natural Language Processing (NLP): It is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language. It can be further divided into 3 fields:\n", "The above machines help to  comprehend  both intent and context of human communication. Imagine the positive impact that this emerging technological paradigm has had on global trade, academics, scientific research, and culture. After all, there are over 6,500 human natural languages all over the world!\n", "The best part of this technology is that both speech and text can use it. However, we would stick to the dynamics of semantic search alone. It involves a pre-processing data stage called text processing. This allows the understanding and processing of large amounts of text data. It is the process of analyzing textual data into a computer-readable format for machine learning algorithms.\n", "A language model is a tool to incorporate concise and abundant information reusable in an out-of-sample context by calculating a probability distribution over words or sequences of words.\n", "The problem of NLP cannot be explained without citing BERT (Bidirectional Encoder Representations from Transformers) as an example of a state-of-the-art pre-trained language model. The bidirectional encoder representations from transformers can answer more accurate and relevant results for semantic search using NLP. Jacob Devlin created a well-known state-of-the-art language model in 2018. And Google leveraged in 2019 to understand user searches.\n", "There are many open-source frameworks for solving NLP problems such as NLTK, GPT3, and spaCey. We at INT. use those frameworks for engineering NLP-driven software.\n", "GPT3 (Generative Pre-trained Transformer- think GAN of NLP) was a wonder framework released in 2020 by OpenAI. It has the power to thrill and scare people due to its accuracy in mimicking human natural language. It used a transformer, which is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. NLP and computer vision (CV) primarily use the GPT3 framework. Its ability to differentially weight features works out terrifically for us as the model can discern different words in a sample. Also, it can assign probabilities of them occurring in the past, present, and future.\n", "Language models such as BERT need a truly humongous amount of data in the targeted language to fine-tune its general understanding of the language. Data engineering is an absolute need for the accuracy of a language model. Crowdsourcing is one such strategy to get abundant data.\n", "The other way is to have an application/algorithm crawl through targetted or available resources on the internet.\n", "Lastly, companies specializing in the required data for NLP can provide data for purchasing.\n", "Source: https://www.indusnet.co.in/what-to-know-about-semantic-search-using-nlp/\n"]}, {"link": "https://medium.com/@cees-roele/detecting-persuasion-with-spacy-6b6beba51076?source=list-a13ace4f182c--------37-------f7e9b3597071---------------------", "title": "Detecting Persuasion with spaCy", "subtitle": "false", "autorName": "Cees Roele", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*TEwhJUNB4hkwxEv8Wx4EVg.jpeg", "clap": "124", "response": "1", "timeForRead": "10 min read", "dateCreate": "Jun 7, 2022", "text": ["Persuasion techniques express shortcuts in the argumentation process, e.g., by leveraging on the emotions of the audience or by using logical fallacies to influence it. In this article we will create a spaCy pipeline with a SpanCategorizer to detect and classify spans in which persuasion techniques are used in a text.\n", "Our training data identifies 20 categories. Spans may overlap, that is, a word can be part of different spans.\n", "Here is a partial example:\n", "We will train different models with the dataset, use different spaCy configurations, and compare the results.\n", "Table of Contents:\n", "Unwarranted reasoning goes by different names. Philosophers talk of fallacies, psychologists focus on manipulation, political scientists speak of propaganda, and linguists interested in the venerable tradition of rhetorics address persuasion. Each domain has its own focus on what the relevant impact of unwarranted reasoning is.\n", "Detecting and explaining unwarranted reasoning might require epistemology, logic, estimation of intention, psychological biases, knowledge of pre-existing narrative, and even physical context. As all this doesn t fit a feasible machine learning problem description, we convert unwarranted reasoning into a problem of classification: given a set of categories and a dataset of texts with marked spans belonging to categories, we train a model to detect such spans and classify them. We call these categories persuasion techniques.\n", "Different studies have come up with different sets of persuasion techniques, e.g. ranging from a single classification of a whole text as  propaganda  to distinguishing 69 different techniques.[4] Presently, we will not discuss whether some categorisation is  better  than another or whether any categorisation is fit for purpose. Here we adopt one set of twenty techniques, with the understanding that a different set would be possible.\n", "Let s look at the description of some of the techniques described in [1]:\n", "These techniques are described in the context of subtask 2 of SemEval-2021 Task 6:  Detection of Persuasion Techniques in Texts and Images  [1].\n", "The present article focuses on the implementation of a system detecting and classifying spans using the spaCy SpanCategorizer and will pay only cursory attention to the meaning of techniques and to the used dataset. For detailed information on the dataset and on the meaning of the different techniques into which we classify spans, please check out the mentioned article.\n", "The dataset we use was created for SemEval-2021 task 6  Detection of Persuasion Techniques in Texts and Images . It can be found on github [3].\n", "The dataset consists of a total of 951  memes , short texts taken from social media posts, in which 2083 spans with persuasion techniques are identified by a team of annotators. The texts are overlaid on an image   hence they contain numerous line breaks  and many are written in uppercase. We ignore the images.\n", "Here is an example:\n", "Persuasion spans in this text:\n", "Below the overview of persuasion techniques distinguished in the dataset with for each the number of occurrences in the dataset and the average number of tokens in a span. To determine the number of words in a span the standard spaCy tokeniser was used. Note that interpunction and whitespace tokens are included in the counting.\n", "In total there are 2083 spans.\n", "Looking at the table you see:\n", "Generally:\n", "Defining spans is like taking a coloured marker and highlighting a fragment of the original text. As we need exact fragments, we will not modify the original text by pre-processing it in any way.\n", "Our dataset is already divided into train, dev, and test parts. We convert each of these files separately into a binary .spacy file which is used as input for training.\n", "As it is an open question what base model would best fit our requirements we will try small, large, and transformer models and compare the results.\n", "Once our corpus is defined we can start a training using the spacy train command. For readability and repeatability we define this in a spaCy project. You can find an example of a regular pipeline consisting of corpus, train, and evaluate steps in the project.yml of my article  Detecting Toxic Spans with Spacy .\n", "Such a pipeline makes fine-tuning training of one model easy. Here we will not tune, but instead use three basis models and see how they perform. To define their configuration we use the  Quickstart  dialog in the spaCy documentation.\n", "We select the spancat component and generate three configurations:\n", "This will result in small, large, and transformer models. The default spaCy transformer model is RoBERTa-base.\n", "Using default configuration values, we will train each of these models and compare the results.\n", "To detect spans, spaCy first generates a set of possible spans for a document. This is done by a component named Suggester.\n", "SpaCy 3.3 comes with two implementations of Suggesters, both based on generating n-grams, that is, spans of n tokens. The ngram_suggester is configured with a list of lengths of n-grams, e.g. [1, 2, 3, 4]. The ngram_range_suggester is configured with a minimum and maximum of a range of lengths, e.g. min_size=1, max_size=4.\n", "Named entities typically consist of only a few tokens. With a token-length of 5 the named entity  Berlin Brandenburg Airport Willy Brandt  is relatively long. In our current dataset, however, we deal with spans that might even range across sentences. Here is an example fragment from our dataset of the category Causal Simplification:  Childish Trump Won t Meet With Pelosi On Coronavirus\\nBecause He Doesn t Like Her . SpaCy tokenises this fragment into 15 tokens, including one for the newline, where  won t  is broken up into [ wo ,  n t ].\n", "As training, evaluation, and prediction of any span can only succeed if it doesn t contain more tokens than generated by the Suggester, we must look into our dataset and see how many samples are cut off.\n", "In the table below, we take 8-grams, 16-grams, and 32-grams as maximums for the ngram-range-suggester.\n", "We apply some markup based on arbitrary limits:\n", "What we see:\n", "We will train with suggesters with maximums of 16-grams and 32-grams. We will ignore a suggester with a maximum of 8-grams as this looks unpromising given that so many techniques are not covered satisfactorily by that suggester.\n", "Looking at F1 scores for the n-gram suggester set to maximums of 16 and of 32 tokens we find:\n", "As expected, the large (lg) model does better than the small (sm) model and the transformer (trf) model does better than the large model. Surprisingly, however, the small and large models do worse with 32-grams than with 16-grams. Also we find that the transformer model is doing significantly better with 32-grams than with 16-grams.\n", "In the table below the data for the F1 score used in the diagram above plus precision and recall measure. Values that have decreased for the 32-grams suggester are marked in red.\n", "What we see is that for the small and large models the recall decreases for the 32-gram suggester, but the precision increases.\n", "This means that the small and large models label too many tokens that shouldn t be labelled for the 32-gram suggester, although they do cover more of those that should be labelled. Let s call is over-optimistic labelling. The transformer models don t suffer from that defect.\n", "It would be interesting to determine why this is happening, but that would require further research.\n", "Let s look at the F1 scores for each individual persuasion technique:\n", "We see:\n", "Created models should be able to predict spans for different classes that overlap each other. However, what happens   and what should not happen   is that there are predictions of spans with the same label that overlap each other. The following image illustrates the prediction of a 32-grams transformer-based model.\n", "I consider this an error of the SpanCategorizer.\n", "Metrics are merely numbers unless serving to compare different systems producing them. As we effectively implemented subtask 2 of Task 6 of SemEval 2021, we can compare our outcome with other systems on the leaderboard for that task. SpaCy uses token-based metrics, but the mentioned contest uses character-based metrics.\n", "Using the best model to produce a prediction for the test dataset and having the result evaluated by the scorer method provided for the contest leads to a character-based F1 of 0.449. That would place it second in the Task 6 ranking published in [1]!\n", "This encourages future research comparing the architecture of the spaCy model-with-suggester with the models participating in Task 6.\n", "We have seen that spaCy s SpanCategorizer can be used to detect spans and classify them. As the dataset used for the present article contains spans of widely varying length, we needed to take the functionality and configuration of the spaCy Suggester into account, which is the function generating spans. For this dataset transformer models proved significantly more accurate than spaCy s small and large models. The resulting model ranked well within alternative systems for detecting and classifying spans.\n", "[1]  SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and Images  (2021) D. Dimitrov et al\n", "[2]  WVOQ at SemEval-2021 Task 6: BART for Span Detection andClassification  (2021) Cees Roele\n", "[3]  Data for SemEval-2021 Task 6: Detection of Persuasive Techniques in Texts and Images , github\n", "[4]  Fine-Grained Analysis of Propaganda in News Articles  (2019) G. Da San Martino et al\n"]}, {"link": "https://medium.com/@cobusgreyling/llms-contextual-demonstration-af99de936cf0?source=list-e28f6edecf84--------13-------7b153c9756d3---------------------", "title": "LLMs & Contextual Demonstration", "subtitle": "Large Language Models are able to learn in-context via a number of examples which acts as demonstrations, also referred to as few-shot learning. In the recent past, there has been little understanding on how models learn from few-shot, in-context demonstrations & what part of the demonstration is the most important to performance.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "65", "response": "9", "timeForRead": "4 min read", "dateCreate": "false", "text": ["With human language in general and LLMs in specific, context is of utmost importance. When a few-shot learning approach is followed via Prompt Engineering, a contextual reference is established for the LLM to serve as an input specific contextual reference.\n", "A recent study explored how models learn and which aspects contribute most to the tasks and performance. The study found that the key drivers of few-shot training are:\n", "The analysis creates a new understanding of how in-context learning works and challenges notions of what can be achieved at inference alone.\n", "Considering the image below, the performance of two use-cases are shown across three models. The model performance where no demonstration is given, varies quite a bit.\n", "Considering the no demonstration performance of GPT-J for a moment the GPT-J model can be access via a few playgrounds; generally casual users are disappointed with the model s performance.\n", "However, consider the boost in performance with gold and random labels are used at inference. This goes to show that apart from fine-tuning, implementing an accurate and succinct prompt engineering pipeline can boost the performance of LLMs.\n", "This finding has implications for local installations of smaller models which are open-sourced; models which are often deemed not good enough when being casually inspected.\n", "However, when the principles detailed here are followed and as seen in the graph below, exceptional performance can be extracted from models.\n", "This approach can solve for cost, data privacy, corporate governance requirements and more; considering smaller models can be made use of.\n", "Below the impact of the distribution of the inputs are shown, notice the disparity between Direct & Channel. The direct model exploits the label space better than the input distribution, and the channel model exploits the input distribution better than the label space.\n", "Below is a good breakdown of practical examples where format, input distribution, label space and input-label mapping are experimented with.\n", "It needs to be noted these experiments were limited to classification and multi-choice tasks.\n", "Any study on how to optimise inference data for other tasks like completion, editing and chat will immensely useful.\n", "For few-shot text classification. Instead of predicting the label from input; like intent detection works; channel models compute the conditional probability of the input given the label.\n", "Direct prompting is shown very clearly in the image below. And I do get the sense that direct inference is more widely used as opposed to channel; while there are studies showing that channel outperforms direct.\n", "  Follow me on LinkedIn for updates on Large Language Models  \n", "I m currently the Chief Evangelist @ Kore AI. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}, {"link": "https://medium.com/@cobusgreyling/react-synergy-between-reasoning-acting-in-llms-36fc050ae8c7?source=list-e28f6edecf84--------3-------7b153c9756d3---------------------", "title": "ReAct: Synergy Between Reasoning & Acting In LLMs", "subtitle": "An element of human intelligence is the ability to seamlessly combine task-oriented actions with verbal or inner speech. This inner speech plays an important role in human cognition and enables self-regulation and strategising.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "12", "response": "1", "timeForRead": "5 min read", "dateCreate": "Jun 28", "text": ["I m currently the Chief Evangelist @ HumanFirst. I explore and write about all things at the intersection of AI and language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces and more.\n", "With humans the tight synergy between reasoning & acting allows for humans to learn new tasks quickly and perform robust reasoning and decision making. We can perform this even when unforeseen circumstances, information or uncertainties are faced.\n", "LLMs have demonstrated impressive results in chain-of-thought reasoning (CoT) and prompting, and acting (generation of action plans).\n", "The idea of ReAct is to combine reasoning and taking action.\n", "Reasoning enables the model to induce, track and update action plans, while actions allow for gathering additional information from external sources.\n", "Combining these to ideas are named ReAct, and it was applied to a diverse set of language and decision making tasks to demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness.\n", "  Please follow me on LinkedIn for updates on Conversational AI  \n", "According to the study, ReAct overcomes issues of hallucination and error cascading of CoT reasoning by interacting with a knowledge source like Wikipedia. Human-like task-solving trajectories are generated.\n", "As seen below in the sequence of events of a ReAct based Agent, reasoning traces makes the final result of the LLM more interpretable with various references along the thought process.\n", "Below is an example of a ReAct agent implemented via LangChain. Consider the following complex question:\n", "Author David Chanoff has collaborated with a U.S. Navy admiral who served as the ambassador to the United Kingdom under which President?\n", "The first thought of the LLM Agent is to search David Chanoff and determine the U.S. Navy admiral he collaborated with. Followed by determining the U.S. President:\n", "Here is the complete code to run the ReAct agent based on OpenAI, Wikipedia and LangChain:\n", "And the output from the agent:\n", "A lot has been said about chain of thought reasoning and promting and reasoning.\n", "The fact that many LLMs have a set time stamp and time cutoff in terms of general knowledge is also impacts LLMs negatively.\n", "Having an external data source like Wikipedia plays a big role in the LLM agent being able to take action.\n", "  Please follow me on LinkedIn for updates on Conversational AI  \n", "I m currently the Chief Evangelist @ HumanFirst. I explore and write about all things at the intersection of AI and language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces and more.\n"]}, {"link": "https://medium.com/@albertoromgar/openai-could-lose-its-ai-lead-to-google-for-the-first-time-in-4-years-c611fe5d85d4?source=list-2eb23a991a63--------5-------0a856388a93a---------------------", "title": "OpenAI Could Lose Its AI Lead to Google For the First Time in 4 Years", "subtitle": "The AI race is now at a tipping point", "autorName": "Alberto Romero", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/2*oMdIZBsnK8EFhQLUaAB5ZA.jpeg", "clap": "82", "response": "3", "timeForRead": "2 min read", "dateCreate": "false", "text": ["It was in 2019 that OpenAI released GPT-2, surpassing Google in the race to create better generative AI models.\n", "Will Google retake the AI throne with Gemini before 2023 ends?\n", "The Information scooped yesterday that in mid-2023 OpenAI had to stop working on a new model, codenamed Arrakis, that would presumably make ChatGPT run more efficiently.\n", "This was the company s main ongoing development after finishing GPT-4 in the summer of 2022.\n", "Why did they stop? Because the model didn t work as expected. In a space where a gap of a few months is the difference between being the leader or not, this was an important setback for OpenAI.\n", "The young startup is more than fine   this isn t a life-or-death situation. It s making $1.3 billion in ARR, has been releasing new juicy models like GPT-4 vision, DALL-E 3, etc., and has more aces up the sleeve for the DevDay conference on November 6th.\n", "But this  failure  (I guess we can say that if we compare it with the flawless 4-year run it s had so far!) may allow Google to surpass OpenAI with Gemini, which is posited to beat GPT-4.\n", "If that happens, we can take away two insights from all this:\n", "No one knows which steps are safer or more promising on our way toward AGI. OpenAI s  aura of invincibility  as The Information puts it, was an outlier feature that lasted much more than anyone expected.\n", "AGI is somewhere ahead of us, awaiting patiently, but the path there is full of obstacles we must overcome. OpenAI couldn t get its next release on time and that s both illuminating and humbling.\n", "OpenAI is seen by everyone as the favorite candidate. If it doesn t manage to overcome this hurdle and maintain its leadership or release soon something much better than GPT-4, people will cease to praise it so fervently.\n", "They claimed Google dead after ChatGPT was announced   who knows if now they will predict OpenAI s death. OpenAI will have to solve its technical problems with the model, its business pressures with Microsoft, and its social complaints with the general public.\n", "What do you think, will OpenAI keep or lose its leadership before 2023 ends?\n", "This article is a selection from The Algorithmic Bridge, an educational newsletter to bridge the gap between AI, algorithms, and people. It will help you understand the impact AI has in your life and develop the tools to better navigate the future.\n", "You can also become a Medium member and support my work here.\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-8-5435d573d660?source=list-660438a01f7f--------8-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing (Part 8)", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "1", "response": "2", "timeForRead": "3 min read", "dateCreate": "Aug 27", "text": ["You will now use everything that you learned to create a matrix that corresponds to all the features of your training example. Specifically, I will walk you through an algorithm that allows you to generate this x matrix.Let s take a look at how you can build it.\n", "Previously, you saw how to preprocess a tweet like this one to get a list of words that contain all the relevant information for the sentiment analysis tasks in NLP. With that list of words, you would be able to get a nice representation using a frequency dictionary mapping. And finally, get a vector with a bias unit and two additional features that store the sum of the number of times that every word on your process tweets appears in positive tweets and the sum of the number of times they appear in negative ones.\n", "In practice, you would have to perform this process on a set of m tweets. So given a set of multiple raw tweets, you would have to preprocess them one by one to get these sets of lists of words one for each of your tweets. And finally, you d be able to extract features using a frequencies dictionary mapping. At the end, you would have a matrix, X with m rows and three columns where every row would containthe features for each one of your tweets.\n", "The general implementation of this process is rather easy. First, you build the frequencies dictionary, then initialize the matrix X to match your number of tweets. After that, you ll want to go over through your sets of tweets carefully deleting stop words, stemming, deleting URLs, and handles and lower casing. And finally, extract the features by summing up the positive and negative frequencies of the tweets. For this week s assignment, you ve been provided some helper functions, build_freqs and process_tweet. However, you ll have to implement the function to extract the features of a single tweet. That was a lot of code, but at least now you have your X matrix. And in the next video, we will show youhow you can feed in that X matrix into your logistic regression classifier. Let s take a look at how you can do that.\n", "Please Follow coursesteach to see latest updates on this story\n", "If you want to learn more about these topics: Python, Machine Learning Data Science, Statistic For Machine learning, Linear Algebra for Machine learning Computer Vision and Research\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "Note:if you are a NLP export and have some good suggestions to improve this blog to share, you write comments and contribute.\n", "if you need more update about NLP and want to contribute then following and enroll in following\n", " Course: Natural Language Processing (NLP)\n", " GitHub Repository\n", "   Notebook\n", "1- Natural Language Processing with Classification and Vector Spaces\n", "2-Putting it All Together\n"]}, {"link": "https://medium.com/@cobusgreyling/a-new-prompt-engineering-technique-has-been-introduced-called-step-back-prompting-b00e8954cacb?source=list-e28f6edecf84--------30-------7b153c9756d3---------------------", "title": "A New Prompt Engineering Technique Has Been Introduced Called Step-Back Prompting", "subtitle": "Step-Back Prompting is a prompting technique enabling LLMs to perform abstractions, derive high-level concepts & first principles from which accurate answers can be derived.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "823", "response": "9", "timeForRead": "5 min read", "dateCreate": "Oct 12", "text": ["As we have seen with most prompting techniques published, Large Language Models (LLMs) need guidance when intricate, multi-step reasoning is demanded from a query, and decomposition is a key component when solving complex request.\n", "A process of supervision with step-by-step verification is a promising remedy to improve the correctness of intermediate reasoning step\n", "The most well known prompting technique when it comes to decomposition is chain-of-thought reasoning. In this study Step-Back Prompting is compared to COT prompting.\n", "The text below shows a complete example of STP with the original question, the stepback question, principles, and the prompt for the final answer to be generated by the LLM.\n", "This chart shows the strong performance of Step-Back Prompting which follows an abstraction and reasoning scheme. Evidently this approach leads to significant improvements in a wide range of more complex tasks.\n", "The chart below shows the Step-Back Prompting approach on the TimeQA dataset. Step-Back combined with RAG compared to baseline predictions.\n", "On the left is Step-Back & RAG vs baseline predictions.\n", "On the right, Step-Back RAG vs RAG predictions.\n", "Step-Back Prompting fixed 39.9% of the predictions where the baseline prediction is wrong, while causing 5.6% errors.\n", "Step-Back Prompting + RAG fixes 21.6% errors coming from RAG. While introducing 6.3% errors.\n", "This study again illustrates the versatility of Large Language Models and how new ways of interacting with LLMs can be invented to leverage LLMs even further.\n", "This technique also shows the ambit of static prompting and clearly shows that as complexity grows, more augmented tools like prompt-chaining and autonomous agents need to be employed.\n", "  Follow me on LinkedIn for updates on Large Language Models  \n", "I m currently the Chief Evangelist @ Kore AI. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}, {"link": "https://medium.com/@cobusgreyling/langsmith-hub-by-the-numbers-55e962ba5cf5?source=list-2eb23a991a63--------23-------0a856388a93a---------------------", "title": "LangSmith Hub By The Numbers", "subtitle": "LangSmith can be divided into four sub-products named Projects, Data, Testing & Hub. The first three of these sub-products are focussed on improving production implementations while Hub focusses more on pre-launch testing and refinement.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "104", "response": "1", "timeForRead": "3 min read", "dateCreate": "Oct 13", "text": ["Below is a matrix of all the models available in the LangSmith Hub access to models is a significant drawcard and allowing users to experiment with different models while tweaking prompts and comparing model output.\n", "Considering the table below, the 14 use cases are listed according to the number of prompts in the LangSmith Hub. The biggest use case is chatbots, followed by summarisation and QnA over documents. The top rated use cases include extraction and agents.\n", "This is a good indication of how Large Language Models are being used in implementations.\n", "Considering the table below, chat based prompts are almost on par with string (completion) based templates. This almost 50/50 split is interesting considering the push from OpenAI to deprecate complete and edit modes and favour the chat mode.\n", "Something I found curious is how high the Chinese language is ranked in the number of prompts.\n", "Lastly, prompt count according to models is dominated by OpenAI, followed by Anthropic and Google.\n", "In closing, there is a need for a LLM focused workspace where experimentation is possible referencing different LLMs. There are a few prompt hubs, most notably that of Haystack.\n", "In upcoming articles I will be focussing on data, and the four fundamental pillars of data in terms of discovery, design, development and delivery.\n", "  Follow me on LinkedIn for updates on Large Language Models  \n", "I m currently the Chief Evangelist @ Kore AI. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}, {"link": "https://medium.com/@rajistics/explaining-predictions-from-transformer-models-55ab9c6cab24?source=list-a13ace4f182c--------34-------f7e9b3597071---------------------", "title": "Explaining predictions from   transformer models", "subtitle": "false", "autorName": "Rajiv Shah", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*_FB_9GE68trweRsuptPUFQ.png", "clap": "51", "response": "1", "timeForRead": "3 min read", "dateCreate": "Aug 15, 2022", "text": ["This post covers 3 easy-to-use   packages to get started. You can also check out the Colab   companion notebook at https://bit.ly/raj_explain and the Youtube   video for a deeper treatment.\n", "Explanations are useful for explaining predictions. In the case of text, they highlight how the text influenced the prediction. They are helpful for   diagnosing model issues,   showing stakeholders understand how a model is working, and   meeting regulatory requirements.\n", "Here is an explanation   using shap. For more on explanations, check out the explanations in machine learning video.\n", "Let s review 3 packages you can use to get explanations. All of these work with transformers, provide visualizations, and only require a few lines of code.\n", "2. Transformers Interpret uses Integrated Gradients from Captum to calculate the explanations. This approach is   quicker than SHAP! Check out this space to see a demo.\n", "3. Ferret is built for benchmarking interpretability techniques and includes multiple explanation methodologies (including Partition Shap and Integrated Gradients). A spaces demo for ferret is here along with a paper that explains the various metrics incorporated in ferret.\n", "You can see below how explanations can differ when using different explanation methods. A great reminder that explanations for text are complicated and need to be appropriately caveated.\n", "Ready to dive in?  \n", "For a longer walkthrough of all the   packages with code snippets, web-based demos, and links to documentation/papers, check out:\n", "  Colab notebook: https://bit.ly/raj_explain\n", "  https://youtu.be/j6WbCS0GLuY\n", "Originally published at http://projects.rajivshah.com on Aug. 14, 2022.\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-7-6e73b81ecc7c?source=list-660438a01f7f--------9-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing (Part 7)", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "1", "response": "2", "timeForRead": "14 min read", "dateCreate": "Aug 20", "text": ["What is text-processing\n", "Text pre-processing is the process of transforming unstructured text to structured text to prepare it for analysis. When you pre-process text before feeding it to algorithms, you increase the accuracy and efficiency of said algorithms by removing noise and other inconsistencies in the text that can make it hard for the computer to understand. Making the text easier to understand also helps to reduce the time and resources required for the computer to pre-process data.These words need to then be encoded as integers, or floating-point values, for use as inputs in machine learning algorithms. This process is called feature extraction (or vectorizations).\n", "Scikit-learn s CountVectorizer is used to convert a collection of text documents to a vector of term/token counts. It also enables the  pre-processing of text data prior to generating the vector representation. This functionality makes it a highly flexible feature representation module for text.\n", "In this section, we will discuss the steps involved in preparing the data for sentiment analysis using logistic regression.\n", "1- Data Collection:\n", "The first step in any sentiment analysis project is to collect a suitable dataset. This can be done by scraping data from social media platforms, online reviews, or any other relevant sources.\n", "2- Data Cleaning\n", "Once the data is collected, it needs to be cleaned by removing unnecessary characters, punctuation marks, and stopwords. Stop-words are words that do not carry much meaning and can be safely ignored. When it comes to low-level text processing problems  it is advisable to remove all punctuations and special characters ( including emojis ) for several significant reasons, which are Dimensionality issues, Computational Efficiency, Noise Reduction, and Generalization you can state other issues as well \n", "Dimensionality Reduction: Keeping every punctuation mark and special character as a separate feature can significantly increase the dimensionality of the data, making it computationally expensive and potentially leading to overfitting. By removing them, you reduce the dimensionality of the feature space.\n", "Computational Efficiency: Some NLP algorithms and models, especially those based on neural networks, are computationally more efficient when trained on preprocessed text. Removing punctuations and special characters can help speed up the training and inference processes.\n", "Noise Reduction: Punctuation and special characters often don t carry significant semantic meaning on their own. Removing them can help reduce the noise in the text and make it easier for NLP models to focus on the meaningful words and phrases.\n", "Generalization: Ignoring punctuations and special characters helps NLP models generalize better. For instance, if you remove the period from the end of a sentence, the model can better learn the relationship between words without being overly influenced by sentence boundaries.\n", "However, it s important to note that there are cases where punctuations and special characters might convey valuable information, such as in sentiment analysis (e.g.,  I love it  vs.  I love it! , in the second case the speaker seems to be more excited). In such cases, you may choose to retain certain punctuation marks or handle them differently in your preprocessing pipeline. The choice of whether to remove or retain punctuations depends on the specific NLP task and the goals of your analysis.\n", "3- Tokenization:\n", "Tokenization is the process of splitting text into individual words or tokens. This step helps in creating a structured format for further analysis.\n", "4- Feature Extraction: After tokenization, relevant features need to be extracted from the text. This can be done using techniques such as bag-of-words, TF-IDF (Term Frequency-Inverse Document Frequency), or word embeddings.\n", "Labeling: Each data point in the dataset needs to be labeled with the corresponding sentiment category (positive or negative). This can be done manually or by using pre-labeled datasets for training purposes.\n", "During human conversations, punctuation marks like  , ! , [, }, *, #, /, ?, and   are incredibly relevant and necessary to have a proper conversation. Thelp to fully convey the message of the writer. Since we have a Twitter dataset, we'd like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks. We'll use the re library to perform regular expression operations on our tweet. We'll define our search pattern and use the sub() method to remove matches by substituting with an empty character (i.e. '\n", "By removing punctuation marks from our text we allow the model to focus on the text alone rather than distracting it with symbols. This makes it easier for the text to be analysed.\n", "Tokenization is the process of transforming a string or document into smaller chunks, which we call tokens. When a sentence breakup into small individual words or Phrases, these pieces of words are known as tokens, and the process is known as tokenization.This is usually one step in the process of preparing a text for natural language processing. This allows the computer to work on your text token by token rather than working on the entire text in the following stage. There are many theories and rules regarding tokenization, and you can create your own tokenization rules using regular expressions, but normally tokenization will do things like break out words or sentences, often separate punctuation or you can even just tokenize parts of a string like separating all hashtags in a Tweet [1].The NLTK library includes a range of tokenizers for different languages and use cases.\n", "Why bother with tokenization? Because it can help us with some simple text processing tasks like mapping part of speech, matching common words and perhaps removing unwanted tokens like common words or repeated words. Here, we have a good example. The sentence is: I don t like Sam s shoes. When we tokenize it we can clearly see the negation in the not and we can see possession with the  s. These indicators can help us determine meaning from simple text [1].\n", "The two main types of tokenization are word and sentence tokenization.\n", "1- Word tokenization is the most common kind of tokenization. Here, each token is a word, meaning the algorithm breaks down the entire text into individual words\n", "2- sentence tokenization: On the other hand, sentence tokenization breaks down text into sentences instead of words. It is a less common type of tokenisation only used in few Natural Language Processing (NLP) tasks.\n", "In normalization, your text is converted to standard form. An example of this is converting all text to lowercase, removing numbers, or removing punctuations. Normalization helps to make the text more consistent. There are a couple of different normalization techniques, but I ll give you an explanation of some of the most commonly employed normalisation techniques below.\n", "This technique converts all the letters in your text to a single case, either uppercase or lowercase. Case normalisation ensures that your data is stored in a consistent format and makes it easier to work with the data. An example would be looking for all the instances of a word and searching for it in your text. Without case normalisation, the result of searching for the word  Boy  would be different from the result of searching for  boy .\n", "Stop word is used to filter some words which are repeat often and not giving information about the text. In Spacy, there is a built-in list of some stop words.One of the important preprocessing steps in NLP is to remove stop words from text. Stop words are basically connector words such as  to ,  with ,  is , etc. which provide minimal context. spaCy allows easy identification of stop words with an attribute of the  doc  object called  is_stop . We iterate over all the tokens and apply the  is_stop  method [2].Stop words, such as  the,   and,   is,  and  an,  are common words that appear frequently in a language. These terms are frequently irrelevant to the analysis and can be removed to reduce the noise in the data. The NLTK library includes a list of English stop words for this purpose.\n", "These are the most common words which do not add much value to the meaning of the document.[1]\n", "Let s take a look at how you can do this. Let s process this tweet. First, I remove all the words that don t add significant meaning to the tweets, aka stop words and punctuation marks. In practice, you would have to compare your tweet against two lists. One with stop words in English and another with punctuation. These lists are usually much larger, but for the purpose of this example, they will do just fine. Every word from the tweet that also appears on the list of stop words should be eliminated. So you d have to eliminate the word and, the word are, the word a, and the word at. The tweet without stop words looks like this.\n", "Note that the overall meaning of the sentence could be inferred without any effort. Now, let s eliminate every punctuation mark. In this example, there are only exclamation points. The tweet without stop words and punctuation looks like this.\n", "However, note that in some contexts you won t have to eliminate punctuation. So you should think carefully about whether punctuation adds important information to your specific NLP task or not. Tweets and other types of texts often have handles and URLs, but these don t add any value for the task of sentiment analysis. Let s eliminate these two handles and this URL. At the end of this process, the resulting tweets contains all the important information related to its sentiment.\n", "Tuning the GREAT AI model is clearly a positive tweet and a sufficiently good model should be able to classify it.\n", "Lemmatization is better than stemming and informative to find beyond the word to its stem also determine part of speech around a word. That s why spacy has lemmatization, not stemming. So we will do lemmatization with spacy.Lemmatization is another important preprocessing step for NLP pipelines. It helps to remove different versions of a single word to reduce redundancy of same-meaning words as it converts the words to their root lemmas. For example, it will convert  is  ->  be ,  eating  ->  eat , and  N.Y.  ->  n.y. . With spaCy, the words can be easily converted to their lemmas using a  .lemma_  attribute of the  doc  object.[2]. We iterate over all the tokens and apply the  .lemma_  method.[2].Lemmatization is the process of reducing a word to its base or root form, called a lemma. Stemming is a similar process, but it often results in words that are not actual words.\n", "For example, the words  walked ,  walking , and  walk  would all be lemmatized to the word  walk . This is because they all have the same lemma, which is the dictionary form of the word.\n", "Lemmatization can be done using a variety of tools and techniques. Some popular lemmatizers include the Porter stemmer, the Snowball stemmer, and the WordNet lemmatizer.\n", "Lemmatization is a similar process to stemming, but it reduces words to their base form by using a dictionary or knowledge of the language. This can result in more accurate base forms than stemming [6].\n", "Another text preprocessing technique using which we reduce the words down to their root forms.[1]\n", "A basic example demonstrating how a lemmatizer works\n", "In the following example, we are taking the PoS tag as  verb,  and when we apply the lemmatization rules, it gives us dictionary words instead of truncating the original word:[4]\n", "What is stemming: Stemming is a process in which words are reduced to their root meaning.It s a technique to get to the root form of a word by removing the prefix and suffix of a word.[1].We use Stemming to normalize words. In English and many other languages, a single word can take multiple forms depending upon the context used. For instance, the verb  study  can take many forms like  studies,   studying,   studied,  and others, depending on its context. When we tokenize words, an interpreter considers these input words as different words even though their underlying meaning is the same. Moreover, as we know that NLP is about analyzing the meaning of content, to resolve this problem, we use stemming [4].\n", "Stemming normalizes the word by truncating the word to its stem word. For example, the words  studies,   studied,   studying  will be reduced to  studi,  making all these word forms to refer to only one token. Notice that stemming may not give us a dictionary, grammatical word for a particular set of words [4].In Natural Language Processing (NLP),  steaming  refers to the process of reducing a word to its base or root form. This is often done to group together different forms of a word so they can be analyzed together as a single item [6].\n", "Stemming is the process of reducing words to their base or stem form, by removing any prefixes or suffixes. This is a common technique for reducing the dimensionality of the data, as it groups similar words together.\n", "Now that the tweet from the example has only the necessary information, I will perform stemming for every word.\n", "Stemming in NLP is simply transforming any word to its base stem, which you could define as the set of characters that are used to construct the word and its derivatives. Let s take the firstword from the example. Its stem is tun, because adding the letter e, it forms the word tune. Adding the suffix ed, forms the word tuned, and adding the suffix ing, it forms the word tuning. After you performstemming on your corpus, the word tune, tuned, and tuning will be reduced to the stem tun. So your vocabulary would be significantly reduced when you perform this process for every word in the corpus.\n", "To reduce your vocabulary even further without losing valuable information, you d have to lowercase every one of your words. So the word GREAT, Great and great would be treated as the same exact word. This is the final preprocess tweet as a list of words. Now that you re familiar with stemming and stop words, you know the basics of texts processing.\n", "Types of stemmer\n", "Porter stemmer was developed in 1980. It is used for the reduction of a word to its stem or root word.one thing is noticed that the porter stemmer is not giving many good results. So, that s why the Snowball stemmer is used for a more improved method.\n", "Is a commonly used model that allows you to count all words in a piece of text. Basically, it creates an occurrence matrix for the sentence or document, disregarding grammar and word order. These word frequencies or occurrences are then used as features for training a classifier.\n", "Bag of Words is a text-processing methodology that extracts features from textual data. It uses a pre-defined dictionary of words to measure the presence of known words in your data and doesn t consider the order of word appearance.\n", "The algorithm uses this dictionary to loop through all the documents in the data and can use a simple scoring method to create the vectors. For example, it can mark the presence of a word in a vocabulary as 1 or 0 if absent. Additional scoring methods include looking at the frequency of each word appearing in the document.\n", "Here is an example of a bag-of-words representation of the sentence  John likes to watch movies. Mary likes movies too :\n", "This representation tells us that the words  john ,  likes ,  movies , and  mary  appear in the sentence, and that the word  likes  appears twice. It does not tell us anything about the order of the words in the sentence, or about the grammatical relationships between the words.\n", "The bag-of-words model is a simple and efficient way to represent text for use in machine learning algorithms. It is often used in tasks such as document classification, sentiment analysis, and topic modeling.\n", "Please Follow coursesteach to see latest updates on this story\n", "If you want to learn more about these topics: Python, Machine Learning Data Science, Statistic For Machine learning, Linear Algebra for Machine learning Computer Vision and Research\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "Note:if you are a NLP export and have some good suggestions to improve this blog to share, you write comments and contribute.\n", "if you need more update about NLP and want to contribute then following and enroll in following\n", " Course: Natural Language Processing (NLP)\n", " GitHub Repository\n", "   Notebook\n", "1-Day 2: 30 Days of Natural Language Processing Series with Projects\n", "3-Fully Explained Regular Expression with Python (Unread)\n", "4-Natural Language Processing (NLP) with Python   Tutorial( Unread)\n", "5-Python for Natural Language Processing: A Beginner s Guide\n", "6-Every Beginner NLP Engineer must know these Techniques (Unread)\n", "7-A Guide to Text Preprocessing Techniques in NLP\n", "8- Natural Language Processing with Classification and Vector Spaces\n", "9 6-How to Convert Text Into Vectors\n", "10-Text Preprocessing For NLP Part   1\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-10-80b392750ef4?source=list-660438a01f7f--------6-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing(Part 10)", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "1", "response": "2", "timeForRead": "3 min read", "dateCreate": "Sep 10", "text": ["In the previous tutorial, you learned how to classify whether a tweet has a positive sentiment or negative sentiment, using a theta that I have given you. In this tutorial, you will learn your own theta from scratch, and specifically, I ll walk you through an algorithm that allows you to get your theta variable.\n", "Let s see how you can do this. To train your logistic regression classifier, iterate until you find the set of parameters theta, that minimizes your cost function. Let us suppose that your loss only depends on the parameters theta1 and theta2, you would have a cost function that looks like this contour plots on the left. On the right, you can see the evolution of the cost function as you iterate. First, you would have to initialize your parameters theta. Then you will update your theta in the direction of the gradient of your cost function. After a 100 iterations, you would be at this point, after 200 here, and so on. After many iterations, you derive to a point near your optimum costs and you d end your training here.\n", "Let s look at this process in more detail. First, you d have to initialize your parameters vector theta. Then you d use the logistic function to get values for each of your observations. After that, you d be able to calculate the gradients of your cost function and update your parameters. Finally, you d be able to compute your cost J and determine if more iterations are needed according to a stop-parameter or maximum number of iterations. As you might have seen in the other courses, this algorithm is known as gradient descent. Now, that you have your theta variable, you want to evaluate your theta, meaning you want to evaluate your classifier. Once you put in your theta into your sigmoid function, do get a good classifier or do you get a bad classifier? In the next tutorial, we will show you how you can do this.\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "Note:if you are a NLP export and have some good suggestions to improve this blog to share, you write comments and contribute.\n", "if you need more update about NLP and want to contribute then following and enroll in following\n", " Course: Natural Language Processing (NLP)\n", " GitHub Repository\n", "   Notebook\n", "1- Natural Language Processing with Classification and Vector Spaces\n", "2-Logistic Regression: Training\n"]}, {"link": "https://medium.com/@avra42/summarizing-scientific-articles-with-openai-and-streamlit-fdee12aa1a2b?source=list-dee72bb8661c--------17-------c25b06fd87f2---------------------", "title": "Summarizing Scientific Articles with OpenAI   and Streamlit  ", "subtitle": "false", "autorName": "Avra", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*NzYk5R6Pl1fYyHJPkIl2rA.png", "clap": "97", "response": "1", "timeForRead": "6 min read", "dateCreate": "Dec 19, 2022", "text": ["  GitHub  |   Twitter |   YouTube |   BuyMeaCoffee | Ko-fi \n", "TL;DR: This blog post uses the OpenAI API and the Streamlit library to create a simple web application that allows a user to input a scientific article and generate a summary of the article. The user can choose the size of the output summary and save the generated summary to their device.\n", "To use the OpenAI API, you will need to obtain an API key by signing up for a free account on the OpenAI website and creating a new API key. You can then use the API key in your code by setting it as the value of the openai.api_key variable (refer to the code block). You can then use the OpenAI API by making requests to the API's endpoints using the openai library. This will allow you to use the AI-powered text generation capabilities of the OpenAI API in your projects. (Note: I've made a demo tutorial video that will guide you through this process)\n", "The st.secrets object is a special Streamlit object that allows you to store sensitive information in a secure way, as the secrets are not visible in the source code of your app while testing locally. This is useful for protecting API keys and other sensitive information from being exposed publicly. The st.secrets object is accessed like a dictionary, with the keys being the names of the secrets and the values being the secret values themselves.\n", "First, let s start by importing the required libraries and setting up our OpenAI API key:\n"]}, {"link": "https://medium.com/@siddiquimubasheer/text-summarization-using-bert-and-t5-e05dbbc757c6?source=list-2c27d980d3f3--------16-------338c7da11cbf---------------------", "title": "Text Summarization using BERT and T5", "subtitle": "false", "autorName": "Mubasheer Siddiqui", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*AiEM6FHS0zuDxpsL0feKUw.jpeg", "clap": "418", "response": "4", "timeForRead": "7 min read", "dateCreate": "Jan 7, 2022", "text": ["Many times we find ourselves in a situation where we need the summary of the details and not a full report of the same, then often we go through the whole text and markup important direct or indirect details and then rewrite. This is definitively a time consuming approach and when the no of documents increases, we realize the importance of automatic text summarization.\n", "The difficulty of producing a concise, accurate, and fluent summary of a lengthy text document is known as text summarization.\n", "Automatic text summarization methods are desperately needed to deal with the ever-increasing amount of text data available online, in order to improve both the discovery and consumption of relevant information.\n", "Automatic Text Summarization can be used to summarize research papers, long reports, full books, online pages, and news, among other things. We have seen new highs in this discipline as a result of recent breakthroughs in Machine Learning, particularly Deep Learning.\n", "Deep learning technologies have proven to be particularly promising for this endeavor since they attempt to replicate the way the human brain functions by managing multiple levels of abstraction and nonlinearly translating a given input into a given output (in this process the output of one layer becomes the input of the other layer and so on). Obviously, the deeper the layers, the deeper the depth. Deep neural networks are commonly utilized in NLP difficulties because their architecture fits well with the language s complicated structure; for example, each layer can handle a particular task before passing the output to the next.\n", "In Natural Language Processing (NLP), there are two main ways to summarize text :1. Extractive Summarization2. Abstractive Summarization\n", "Extracting essential words from a source document and combining them to make a summary is what extractive text summarization is all about. The extraction is done according to the predefined measure without making any changes to the texts.\n", "Methods for extractive summarization are :\n", "Parts of the source document are interpreted and trimmed as part of the abstraction approach. When deep learning is applied for text summarization, abstraction can overcome the grammar mistakes of the extractive method.\n", "The abstractive text summarization algorithms, like humans, produce new phrases and sentences that convey the most relevant information from the original text.\n", "Due to these reasons, abstraction outperforms extraction. The text summarization algorithms required for abstraction, on the other side, are more challenging to build, which is why extraction is still widely used.\n", "Methods for abstractive summarization are :\n", "Text : Joseph and Mary rode on a donkey to attend the annual event in Jerusalem. In the city, Mary gave birth to a child named Jesus.\n", "Extractive Summary : Joseph and Mary attend event Jerusalem. Mary birth Jesus.\n", "Abstractive Summary : Joseph and Mary came to Jerusalem where Jesus was born.\n", "It s not an exaggeration to mention that BERT has considerably altered the Natural Language Processing scene. Consider using a single model trained on a huge unlabeled dataset to obtain best-in-class results on eleven different NLP tasks. and every one of this with very little fine-tuning. That s BERT! It s a tectonic shift in how design we models.\n", "a lot of latest NLP architectures, training approaches, and language models, such as OpenAI s GPT-2, Google s TransformerXL, RoBERTa, ERNIE2.0, XLNet, etc. have been inspired byBERT.\n", "You ve probably heard about BERT and read about how amazing it is and how it may change the NLP landscape. But, first and foremost, what is BERT?\n", "The NLP framework is described as follows by the BERT research team:\n", " BERT stands for Bidirectional Encoder Representations from Transformers. It is intended to condition both left and right context to pre-train deep bidirectional representations from unlabeled text. As a result, with just one additional output layer, the pre-trained BERT model may be fine-tuned to generate state-of-the-art models for a wide range of NLP tasks. \n", "To begin, BERT stands for Bidirectional Encoder Representations from Transformers, which is simple to grasp. Each word has a significance, which we will discover one by one throughout this article. For now, the most important takeaway from this section is that BERT is built on the Transformer architecture.\n", "Second, BERT is pre-trained on a vast corpus of unlabeled text, which comprises the whole Wikipedia, which has 2,500 million words, and the Book Corpus, which contains around 800 million words.This pre-training step is responsible for half of BERT s success. This is due to the fact that when a model is trained on a large text corpus, it learns to pick up on deeper and more intimate understandings of how language works. This data may be used as a swiss army knife in almost any NLP project.\n", "Third, BERT is a  deeply bidirectional  model. During the training phase, BERT learns information from both the left and right sides of the context of a token.\n", "In this section we will be looking at Extractive Text Summarization using BERT. As we know, in Extractive Summarization we select sentences from the text as summary. Hence it can be considered as a classification problem where we classify if a sentence is part of a summary or not.\n", "The challenge here is that the model will have to interpret the entire text, choose the correct keywords and ensure that there is no loss. Hence to ensure that there is no compromise of speed and accuracy, we use BERTSUM which is able enough to parse meaning from language and do other preprocessing steps like stop word removal, lemmatization, etc. on its own.\n", "The BERTSUM model consists of 2 parts:\n", "1. BERT encoder.2. Summarization Classifier.\n", "The Encoder provides us with a vector representation of each sentence which is then used by the Summarization Classifier to assign a label to each sentence indicating whether or not it will be incorporated into the final report.\n", "The input of BERTSUM is a little bit different as compared to the BERT model. Here we add the [CLS] token before each sentence in order to separate each sentence and collect the features of its preceding sentence. Each sentence is also assigned and embedding i.e. it is given Ea if the sentence is of even and Eb if it is of odd length. It also gives a score to each sentence, depending on how important it is, and based on these scores the sentences are decided whether or not to be included in the summary.\n", "T5, built by one of the tech giants Google, is one of the most powerful tools for text summarizing. T5, or text to text transfer transformer, a transformer model allows fine tuning for any simple text to task.\n", "To the current study, T5 adds the following:1. It builds Colossal Cleaned Common Crawl (C4), a clean version of the enormous common crawl data collection . This data set dwarfs Wikipedia by two orders of magnitude.2. It proposes that all NLP jobs be reframed as an input text to output text formulation.3. It exhibits that fine tuning on various tasks   summarization, QnA, reading comprehension with the pretrained T5, and text-text formulation   produces state-of-the-art outcomes.4. The T5 team also conducted a thorough investigation into the best procedures for pre-training and fine-tuning\n", "T5 is one of the most qualified for Abstractive summarization for the reasons listed above. Abstractive summarization is a Natural Language Processing (NLP) job that seeks to produce a short summary of a source text. As aforementioned, abstractive summarization, unlike extractive summarization, does not merely reproduce essential phrases from the original text but also has the capacity to generate new relevant phrases, which is referred to as paraphrase.\n", "For abstractive summarization, T5 can be used so easily as follows :\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-15-bayes-rule-b87f9dff4a90?source=list-660438a01f7f--------0-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing(Part 15)-Bayes  Rule", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "137", "response": "2", "timeForRead": "5 min read", "dateCreate": "false", "text": ["We will be looking at conditional probabilities to help us understand Bayes rule. In other words, if I tell you, you can guess what the weather is like given that we are in California and it is winter, then you ll have a much better guess than if I just asked you to guess what the weather is like. In order to derive Bayes rule, let s first take a look at the conditional probabilities.\n", "Now think about what happens if, instead of the entire corpus, you only consider tweets that contain the word happy. This is the same as saying, given that a tweet contains the word happy with that, you would be considering only the tweets inside the blue circle, where many of the positive tweets are now excluded. In this case, the probability that a tweet is positive, given that it contains the word happy, simply becomes the number of tweets that are positive and also contain the word happy. We divide that by the number that contains the word happy. As you can see by this calculation, your tweet has a 75 percent likelihood of being positive if it contains the word happy.\n", "You could make the same case for positive tweets. The purple area denotes the probability that a positive tweet contains the word happy. In this case, the probability is 3 over 13, which is 0.231.\n", "With all of this discussion of the probability of missing certain conditions, we are talking about conditional probabilities. Conditional probabilities could be interpreted as the probability of an outcome B knowing that event A already happened, or given that I m looking at an element from set A, the probability that it s also belongs to set B.\n", "Here s another way of looking at this with a Venn diagram you saw before. Using the previous example, the probability of a tweet being positive, given that it has the word happy, is equal to the probability of the intersection between the tweets that are positive and the tweets that have the word happy divided by the probability of a tweet given from the corpus having the word happy.\n", "Let s take a closer look at the equation from the previous slide. You could write a similar equation by simply swapping the position of the two conditions. Now, you have the conditional probability of a tweet containing the word happy, given that it is a positive tweet. Armed with both of these equations, you re now ready to derive Bayes rule.\n", "To combine these equations, note that the intersection represents the same quantity, no matter which way it s written. Knowing that, you can remove it from the equation, with a little algebraic manipulation, you are able to arriveat this equation.\n", "This is now an expression of Bayes rule in the context of the previous sentiment analysis problem. More generally, Bayes rule states that the probability of x given y is equal to the probability of y given x times the ratio of the probability of x over the probability of y. That s it. You just arrived at the basic formulation ofBayes rule, nicely done.\n", "To wrap up, you just derive Bayes rule from expressions of conditional probability. Throughout the rest of this course, you ll be using Bayes rule for various applications in NLP. The main takeaway for now is that, Bayes rule is based on the mathematical formulation of conditional probabilities. That s with Bayes rule, you can calculate the probability of x given y if you already know the probability of y given x and the ratio of the probabilities of x and y. That s great work. I ll see you later. Congratulations. You now have a good understanding of Bayes rule. In the next video, you ll see how you can start applying Bayes rule to a model known as Naive Bayes. This will allow you to start building your sentiment analysis classifier using just probabilities.\n", "Please Follow coursesteach to see latest updates on this story\n", "If you want to learn more about these topics: Python, Machine Learning Data Science, Statistic For Machine learning, Linear Algebra for Machine learning Computer Vision and Research\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "Note:if you are a NLP export and have some good suggestions to improve this blog to share, you write comments and contribute.\n", "if you need more update about NLP and want to contribute then following and enroll in following\n", " Course: Natural Language Processing (NLP)\n", " GitHub Repository\n", "   Notebook\n", "Do you want to get into data science and AI and need help figuring out how? I can offer you research supervision and long-term career mentoring.Skype: themushtaq48, email:mushtaqmsit@gmail.com\n", "Contribution: We would love your help in making coursesteach community even better! If you want to contribute in some courses , or if you have any suggestions for improvement in any coursesteach content, feel free to contact and follow.\n", "Together, let s make this the best AI learning Community!  \n", " WhatsApp\n", "  Facebook\n", " Github\n", " LinkedIn\n", " Youtube\n", " Twitter\n", "1- Natural Language Processing with Classification and Vector Spaces\n"]}, {"link": "https://medium.com/@kedion/getting-started-with-hugging-face-5efae4984dee?source=list-a13ace4f182c--------12-------f7e9b3597071---------------------", "title": "Fine-Tuning NLP Models With Hugging Face", "subtitle": "Part 1: Getting Started", "autorName": "Kedion", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*NO-81irJRQMGyKcT4yvluw.png", "clap": "18", "response": "1", "timeForRead": "14 min read", "dateCreate": "Sep 21, 2021", "text": ["Written by Tigran Avetisyan\n", "This is Part 1 of our 3 PART SERIES on Hugging Face.\n", "See Part 2 here.\n", "Natural language processing, or NLP, is an exciting field that has seen dramatic development in recent years. Today, language models like GPT-3 (Generative Pre-trained Transformer 3) can produce text that is indistinguishable from that written by a human, which is sensational from a technological standpoint but controversial when we are talking about ethics.\n", "You can easily build and train quality NLP models right on your computer as well (though you probably won t be reaching GPT-3 levels of convincingness). However, starting with NLP can be tricky because of data requirements and preprocessing rules for text sequences.\n", "If you want to make use of natural language processing right now, you could leverage an NLP framework like Hugging Face, which is an API that facilitates the use of language models for large-scale inference.\n", "In this guide, we are going to introduce you to the features and capabilities of Hugging Face, and we will also showcase the basics of inference with pretrained Hugging Face models.\n", "Let s get started!\n", "Hugging Face is a community and NLP platform that provides users with access to a wealth of tooling to help them accelerate language-related workflows. The framework contains thousands of models and datasets to enable data scientists and machine learning engineers alike to tackle tasks such as text classification, text translation, text summarization, question answering, or automatic speech recognition.\n", "In a nutshell, the framework contains the following important components (there are more):\n", "The Inference API is the key component of Hugging Face and is the one that will interest most potential users of the framework. Hugging Face has built the API for those who are not willing or are not technically adept enough to delve into code and those who want to get started with their projects in a short timeframe. Additionally, since the Inference API is hosted in the cloud, you don t have to deploy any models in your local environment.\n", "To start using the Inference API, you need to sign up with Hugging Face. The platform offers a number of subscription plans   a free plan with limited features and several paid plans with increased API request limits and access to accelerated inference.\n", "The free plan is perfectly sufficient for testing out the Inference API. If you end up liking the framework, you may upgrade to a paid plan in accordance with your project s needs.\n", "Performing Inference with the Inference API\n", "    Building a query function    \n", "To leverage the Inference API, you simply need to craft an HTTP request as follows:\n", "Above, we defined a function to perform a query to the Inference API. The Inference API requires that you pass the following arguments:\n", "The function returns a response in JSON format (though you may freely manipulate the results in whichever way you see fit for your needs).\n", "The model_id argument determines which model will be used to carry out your requests. To locate model_id, you should choose a model from the Hugging Face model directory and copy the endpoint at the very top of the webpage. As an example, if we take the RoBERTa base SQuAD model, here s what the endpoint looks like:\n", "RoBERTa base SQuAD is a model built to answer questions based on an input question and provided context for the answer. The model choice in this example is arbitrary   for real-world applications, you would select a model based on your goals.\n", "Next, we need to define our payload. The payload needs to conform to the input format of the selected model, which you can find under Usage down the webpage of your respective model.\n", "In the case of RoBERTa base SQuAD (and other question answering models), input data is a dictionary with two keys and associated sequences:\n", "Lastly, we have the API key. Assuming you ve set up an account on Hugging Face, you will find your API key at https://huggingface.co/settings/token.\n", "Single-question requests with the Inference API\n", "Here s how we put the code together and make a request to the Inference API:\n", "Our response is a dictionary with the following contents:\n", "Here:\n", "You don t have to feed input sequences one-by-one   you can perform batch prediction by just stuffing your input dictionaries in a Python list (or any other container of your choice):\n", "For this query, the response would be as follows:\n", "From this example, we not only got to see the Inference API in action, but we also saw that the RoBERTa base SQuAD model can accurately answer our questions based on context!\n", "Performing Invalid Requests With The Inference API\n", "To conclude this section, let s demonstrate what happens if your query request fails (e.g. if your input data format is wrong). Note the key  text  instead of  context  in the input dictionaries.\n", "The response to this query would be as follows:\n", "The Inference API doesn t throw any exceptions   instead, whenever anything goes wrong, error messages will be delivered to you in the response.\n", "These have been the basics of using the Inference API. Play around with the code yourself to find out what it can do for you!\n", "Pipeline VS Direct Model Use In Inference\n", "The Inference API completely abstracts the  behind the scenes  aspects of inference, which is great if you want to start using NLP models quickly. But if you would like to get direct access to the models for fine-tuning or just for self-learning purposes, you should use the Transformers library.\n", "Transformers can be installed with pip:\n", "Or with Conda:\n", "There are more ways to install Transformers   check out the library s installation guide to learn more.\n", "Transformers allows you to run inference and training on your local machine. Once you get Transformers installed on your machine, you will get access to two primary ways to do inference and/or training:\n", "Let s have a look at each of these methods below!\n", "Similar to the Inference API, pipelines hide away the process of inference, allowing you to get predictions with just a few lines of code.\n", "Pipeline instantiation is done as follows:\n", "The pipeline method has only one mandatory parameter   task. The value passed to this parameter determines which pipeline will be returned. The following pipelines are available for selection:\n", "Optionally, you may also specify a pre-trained model and tokenizer to be used. If you don t provide these arguments, the pipeline will load the default model for the specified task and the default tokenizer for the specified model.\n", "In our case, the default model is DistilBert base cased distilled SQuAD.\n", "To keep things simple, we ve again selected  question-answering  for our pipeline.\n", "Note that if you request a model and tokenizer pair for the first time, Transformers will need to download the files onto your machine. Downloaded files are cached for reuse.\n", "You may also set up Transformers to work in a completely offline environment, but this is beyond the scope of this post. The installation guide of Transformers contains more information about offline use.\n", "Anyway, once we ve got our pipeline loaded, we may directly proceed to inference. To do this, we just pass a query to the pipeline, like so:\n", "As you can see, carrying out inference with pipelines is very similar to how you use the Inference API. However, pipelines are hosted on your local machine (rather than in the cloud), and their setup process is somewhat different.\n", "If you want to customize your code even further, you could access pre-trained Transformers models directly.\n", "Transformers offers models in TensorFlow and/or PyTorch. The model usage process is very similar in either of the libraries, but there are some subtle differences in the code. You should consult Transformers   Summary of the tasks  for more information about implementation details.\n", "We will be using TensorFlow to showcase direct model use. And once again, we will stick to the task of question answering to keep this section consistent with the previous ones.\n", "To get started with direct model use, we need to import two classes:\n", "You can find more information about available auto classes (for tokenizers and model loaders) in the Transformers documentation.\n", "And here s how we instantiate our tokenizer and model:\n", "To obtain a pretrained tokenizer and model, we use the from_pretrained method with both classes, supplying the ID of the associated model. Instead of the ID, you may also supply a path or URL to a saved vocabulary file or model weights.\n", "Here, note the use of the from_pt parameter. The RoBERTa base SQuAD model we have been using throughout this guide is built with PyTorch, and no  native  TensorFlow models are available (as of this post s writing). However, the from_pt parameter allows us to convert PyTorch models to TensorFlow. This works the other way around as well through the from_tf parameter in PyTorch auto models.\n", "With all that said, keep in mind that conversion may not always be smooth. In the case of RoBERTa base SQuAD, we ve got the following warning:\n", "So not all of the weights of the pre-trained PyTorch model were carried over to the TensorFlow model. This won t matter now since we are only going to show how to use Transformers models rather than how to achieve great results with them.\n", "Tokenizing Question-Context Pairs\n", "To perform inference with the loaded model, we need to tokenize our question and its corresponding context as follows:\n", "When calling the tokenizer, we do the following:\n", "Note that depending on your model, you may need to make use of the other parameters of the tokenizer (such as maximum input length). You can find out more about some tokenizer parameters here.\n", "Here s what the inputs variable contains, if you are curious:\n", "As we can see, the inputs variable contains inputs_ids, which are the tokens assigned to our input question and context. Additionally, inputs contains an attention mask that, in our case, assigns equal importance to our inputs.\n", "In the code block above, we ve extracted the input IDs from the tokenized object and assigned them to the input_ids variable. Let s see what it contains:\n", "If we converted the input_ids back to a string, we would get the following:\n", "This shows us that the tokenizer combined our question and context and indicated their beginning and end with a special token to let the model know which is which. Without these tokens, the model would not be able to accurately answer our questions.\n", "Now, let s feed our inputs into the model and examine the outputs:\n", "The model output contains two TF Tensors   start_logits and end_logits. Essentially, these Tensors show the positions in the input at which the model thinks the answer to the input question starts and ends.\n", "To be able to pinpoint the exact position of the answer, we need to locate the indices with the maximum scores, like so:\n", "Notice that with answer_end, we add 1 to the index with the maximum score. This is to ensure that we don t lose the last word in the answer after we slice the input sequence.\n", "Let s see the values for answer_start and answer_end:\n", "And to retrieve our answer, we need to convert the IDs between the start and end indices back to a string:\n", "Notice that the output contains a space before  1980 . We won t bother removing it in this guide, but you could easily do so if necessary.\n", "In this particular example, the model picked one word   1980   as the answer to our question. But it can output answers with several consecutive words as well, which we can see if we rephrase the question to  Where was I born? \n", "The examples above showed how to process a single-sample query. For batches of questions, one option would be to iterate over the questions & contexts and predict an answer for each of them separately:\n", "Alternatively (and more optimally), we can vectorize the inference of question answers. Let s see this in action using the questions and contexts from the previous example:\n", "Notice the new parameter padding. We need to use this parameter whenever our input sequences have unequal length. When padding is True, the tokenizer will pad all sequences to make them as long as the longest sequence in the input.\n", "Next, we pass our inputs to the model and extract the input IDs from our inputs Tensor:\n", "Let s inspect input_ids to figure out how they are different from the IDs with single-sample inference:\n", "inputs_ids contains two NumPy arrays   one for each input question-context pair. Let s convert the second array back to a string to see its contents:\n", "This is indeed our second question-context pair! We can also see how the tokenizer handled its padding.\n", "Next, let s have a look at the output of our model:\n", "The start_logits and end_logits Tensors are each composed of two arrays that correspond to our question-context pairs.\n", "Next, let s retrieve the indices with the highest scores:\n", "Here s what we get:\n", "Now, let s use the indices to retrieve our answers:\n", "And here s the result:\n", "And to put it all together and make sure that the answers correspond to our questions, we can do the following:\n", "The answers indeed match our questions, so we ve done everything correctly!\n", "And this concludes our guide for inference via direct model use with Hugging Face   for now!\n", "The usage of other Hugging Face models in other tasks will be similar   however, you will need to check:\n", "The basics of direct model use with Hugging Face are described in the documentation of Transformers, so make sure to check it out!\n", "Note that the documentation doesn t go into much detail about certain aspects of Transformers. Often, you will need to figure out model input formats on your own, and you ll also need to figure out how to interpret outputs. But this is part of the learning experience!\n", "That s it!\n", "We covered a lot in this article. Now check out Part 2, where we learn to fine-tune NLP models with Hugging Face.\n", "You can find all code for this article in the Jupyter notebook here.\n"]}, {"link": "https://medium.com/@skillcate/detecting-fake-news-with-a-bert-model-9c666e3cdd9b?source=list-a0aae78aa81b--------18-------5fb2bbebc495---------------------", "title": "Detecting Fake News   with a BERT Model", "subtitle": "false", "autorName": "Skillcate AI", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*elWG64moGeBZ-lir056cBA.png", "clap": "90", "response": "1", "timeForRead": "9 min read", "dateCreate": "Oct 2, 2022", "text": ["In the last couple of decades, the emergence of social media (Facebook, Instagram, Twitter, etc.) & messaging platforms (WhatsApp, Telegram, etc.) have brought us all closer than ever. Just imagine, how easy it is to voice an opinion today, on topics that matter to us.\n", "But on the contrary, this very ease of information dissemination has also made social media platforms tools for spreading falsehood, popularly called as fake news. It hardly takes a few hours for propaganda drivers to put something online and get it circulated, leading to conflicts and defamations. So, it s quite pertinent to build sophisticated fake news detection algorithms, that could flag online content spreading misinformation, with adequate reliability.\n", "Well, in this tutorial we shall build a powerful Fake News Detection Model, using the pre-trained BERT, with the help of Transfer Learning.\n", "Well, this article is actually the third & last instalment of my three part learning series, where we are:\n", "Now, let s continue further on this third part, where we build a sophisticated fake news detection model.\n", "If you are more of a video person, go ahead and watch it on YouTube, instead. Make sure to subscribe to my channel to get access to all of my latest content.\n", "This is the snapshot of the dataset we are using. Here s the source for this dataset.\n", "We have two separate .csv files, one having the real news, called true.csv and another one, having the fake news, called fake.csv. Both files have the exact same data form.\n", "We have the title of the news, the entire news article text, the subject, which is basically the category of news and the date on which it was published. For our use case, we shall merge these files into a single large dataset, and add a new column  Label , that will have  true  mentioned against all observations from the true.csv and  fake  mentioned against the observations from fake.csv\n", "Moving on, this is our step-by-step plan on building this project..\n", "BERT is a big neural network architecture, with a huge number of parameters, that can range from 100 million to over 300 million. And, training a BERT model from scratch on a small dataset would result in overfitting. So, it is better to use a pre-trained BERT model that was trained on a huge dataset, as a starting point. We can then further train the model on our relatively smaller dataset and this process is known as model fine-tuning. To do this, there are these three approaches:\n", "In this tutorial, we will use the third approach. We will freeze all the layers of BERT during fine-tuning and append a dense layer and a softmax layer to the architecture.\n", "Now, let s get started with our Fake New Detection Model building using Python. This is our project folder on Google Drive, having all the project related files in one place. I ll share a link to this in the description part below. Here, b2_FakeNewsDetection is our Jupyter notebook. Let s fire it up, to do a quick code walkthrough.\n", "By the way, to proceed with this tutorial, a Jupyter Notebook environment with a GPU is recommended. The same can be accessed through Google Colaboratory which provides a cloud-based Jupyter Notebook environment with a free GPU. For this tutorial, we shall be working on Colab. Once you are on Colab, activate the GPU runtime by clicking on Runtime -> Change runtime type -> Select GPU.\n", "Alright, now let s get coding. As first step, let s set up our working environment.\n", "Here, we install Huggingface s transformers library, which allows us to import a wide range of transformer-based pre-trained models. Additionally, we are installing pycaret. We also set up our working directory.\n", "Next up, let s load the dataset.\n", "Here, first up we load true and fake csv files as pandas dataframe. Then, we create a column  Target , where we put the labels as True / Fake. Finally, we merge the two dataframes into one data, by random mixing.\n", "Next up, the target column has string values, which a computer won t understand. So, we need to transform them into numeric form. To do this, we use Pandas get_dummies to create a new column called label, where we put all Fake labels as 1 and True as 0. Towards the end, to check if our data is balanced across the two labels, we may plot a pie chart. As you would see, our data is fairly well balanced.\n", "Next up, we split up our data into training validation and test set, in 70:15:15 ratio.\n", "Now we come to the BERT fine-tuning stage, where we shall perform transfer learning.\n", "This is what we are doing here:\n", "With this understanding, now let s go ahead to tokenize our sequences, that is titles in our training, test and validation sets.\n", "We also convert the integer sequences to tensors. And finally, we define data loaders for both train and validation set. These data loaders will pass batches of train data and validation data as input to the model during the training phase.\n", "Moving on, we freeze pre-trained model weights. If you can recall, earlier I mentioned in this tutorial, that we would freeze all the layers of the model before fine-tuning it. So, let s do it now. This will prevent updating of model weights during fine-tuning.\n", "If you wish to fine-tune even the pre-trained weights of the BERT model then you may not execute this code.\n", "Moving on, we define our model architecture.\n", "We are using PyTorch for defining, training, & evaluating our deep learning model. Post our BERT network, we are adding dense layers 1 & 2 followed by softmax activation. Then, we define our hyperparameters; we are using AdamW as our optimizer.\n", "Then we define our loss function. And lastly, we are keeping number of epochs to 2. With Colab s free GPU, one epoch might take upto 20 mins. So, I m taking this low numbers to not keep waiting on forever. Haha!\n", "So, just to summarise:\n", "Now, we need to define functions to train (or fine-tune) and evaluate our fake news detection model. Let s do it:\n", "And finally, now we can start fine-tuning our BERT Model to learn fake news detection:\n", "Now let s build a classification report on the test set using our fake news model:\n", "As you would see, we are getting a strong 88% accuracy.\n", "Both precision & recall for class 1 are quite high which means that the model predicts this class pretty well. If you look at the recall for class 1, it is 0.85 which means that the model was able to correctly classify 85% of the fake news as fake. Precision is 0.92, which means that 92% of the fake news classifications by the model, are actually fake news.\n", "Let s also run predictions on these sample news titles. First two are fake and the next two are real.\n", "Quite rightly, our model classifies all four of these reviews correctly.\n", "Guys, congratulations to you for making it to this point. Do give yourself a pat on the back for completing this Transfer Learning Fake News Detection Project all by yourself.  \n", "To summarize, in this tutorial we fine-tuned a pre-trained BERT model to perform text classification on a small dataset. I urge you to fine-tune BERT on a different dataset and see how it performs. For example, you may do a sentiment classification or a spam detection model. NLP use cases are endless, really.\n", "You can even perform multiclass or multi-label classification with the help of BERT. In addition to that, you can even train the entire BERT architecture as well if you have a bigger dataset.\n", "In case you have any doubts or got stuck somewhere, leave a comment below, and I ll help you out.\n", "Guys, if you need further guidance on building a career in data science or any help related to this vast domain, you may go to my website www.skillcate.com and set up a free 1:1 mentoring session with me, by filling out this small form.\n", "You may write to me over email and WhatsApp.\n", "Good luck to you, bye!!\n"]}, {"link": "https://medium.com/@pranik-chainani/transformers-in-nlp-de1db51ef08?source=list-2c27d980d3f3--------33-------338c7da11cbf---------------------", "title": "Transformers in NLP", "subtitle": "false", "autorName": "Pranik Chainani", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/0*ir3Rers95UsDkR8P.jpg", "clap": "69", "response": "7", "timeForRead": "7 min read", "dateCreate": "Dec 19, 2021", "text": ["First introduced in the renowned Attention is All You Need by Vaswani et al, Transformers have become the state-of-the art for many tasks in natural language processing and sequential models as a whole. In fact, there have also been recent experiments that have shown Transformers to generalize well to even computer vision tasks (consider An Image is Worth 16x16 words). As such, it is important to explore attention-based models as a robust framework in detail, given how well they can extend to numerous domains in Machine Learning.\n", "To start with, as starkly proposed in  Attention is All You Need,  Transformers are grounded in attention mechanisms. That is, attention, best put by Vaswani et al, is described as   an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences. \n", "In other words, attention mechanisms have the ability to generate concrete relationships between data points within sequences.\n", "In fact, Transformers use a specific type of attention mechanism, referred to as multi-head attention. However, to understand what this type of attention mechanism is, we must first introduce a simpler scaled dot-product attention scheme.\n", "Let us start with scaled dot-product attention. Simply, we can express this form of attention as:\n", "wherein which Q, K, and V are batches of matrices, each with shape (batch_size, length_of_sequence, num_of_features). We further observe that the inner product between the query Q and the key K results in a matrix of size (batch_size, length_of_sequence, length_of_sequence). We can, thus, interpret this new matrix as telling us roughly how important each element in the given sequence is. As such, we identify this multiplication as the core attention of the current layer, as it essentially determines which elements we  pay attention  to.\n", "This attention matrix is then normalized by the softmax nonlinearity, so that all the weights sum to one. Finally, we simply apply the value V to our attention matrix to observe our desired output.\n", "We can observe how simple it is to implement this form of attention below:\n", "Now that we have a decent idea of how scaled dot-product attention, we simply incorporate this dot-product attention scheme as shown in the diagram below to construct our multi-head attention layer.\n", "Namely, we observe that the multi-head attention is composed of several identical attention heads, where each so-called attention head contains 3 linear layers, followed by the scaled dot-product attention we know. We can simply implement this using a class structure as follows:\n", "Thus, to recap, we observe that each attention head in our multi-head attention scheme computes its own query, key, and value matrices, and then simply applies the scaled dot-product attention.\n", "We can interpret this intuitively as each head can attend to a different part of the given input sequence, independent of the others. Thus, if we increase the number of attention heads, we are able to  pay attention  to more parts of the given input sequence at once, which makes our model even more robust.\n", "Interestingly, it is important to note that our multi-head attention framework really has no trainable components that operates over the sequence in_dim. In fact, everything instead operates over the feature k-dim, and is thus independent of sequence length. As such, we must then provide positional information to our model, so as to ensure that our model knows about the relative position of our data points in the given input sequence.\n", "The way to go about this is as follows:\n", "We see that the usage of seemingly unusual sinusoidal encodings in turn allows for us to better extrapolate to longer sequence lengths. This is because the trigonometric position encodings are periodic, with a range of [0, 1], and thus behave nicely. We can observe this by supposing that, during model inference, we provide an input sequence longer than any used during training. By doing so, the positional encoding for the last elements in that given sequence might be different than anything the model as encountered during training. As such, the sinusoidal positional embeddings then allow for the learned model to extrapolate smoothly to sequence lengths longer than the ones seen before.\n", "Now, we can move on to construct our Transformer model. We start by observing a diagram of the full scheme:\n", "Upon first glance, we see that the transformer uses an encoder-decoder model architecture. The encoder (left) thus processes a given input sequence and returns a feature/latent vector. In turn, the decoder (right) then processes the target sequence, and incorporates information learned from the encoder memory. The output then from our decoder model is our model s prediction.\n", "We will first start by writing up our encoder layer before we move on to the decoder.\n", "Above, we implemented a simple feed forward network and a residual block that we will utilize in our Transformer model (consider reading more on residual blocks from ResNet for more background).\n", "Now to create our encoder, we simply incorporate these utility methods above as follows (following the diagram we introduce above):\n", "The decoder class follows in a similar manner. It is important to note, however, that the decoder accepts two arguments (target and memory/from encoder). Furthermore, the scheme introduce by Vaswani uses two multi-head attention modules per layer, instead of one.\n", "Observe our implementation below:\n", "Finally, we combine everything into a single Transformer class as follows:\n", "To conclude with, we have demonstrated a simply, intuitive explanation that sheds light on a powerful framework of neural networks known as Transformers. Particularly in NLP, transformers do not rely on past hidden states to capture dependencies with previous words, as they are able to process a sentence as a whole, with no risk of loosing (or  forgetting ) past information, as is the case with many RNN models. Moreover, since we incorporate multi-head attention scheme and positional embeddings, we are able to provide information about the intrinsic relationships between different words that aren t easily captured in standard recurrent or markov-based models.\n"]}, {"link": "https://medium.com/@antoine.louis/a-brief-history-of-natural-language-processing-part-2-f5e575e8e37?source=list-a0aae78aa81b--------34-------5fb2bbebc495---------------------", "title": "A Brief History of Natural Language Processing   Part 2", "subtitle": "false", "autorName": "Antoine Louis", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*4Z6gATiHqzYJEOAfIEPPiA@2x.jpeg", "clap": "98", "response": "1", "timeForRead": "6 min read", "dateCreate": "Jul 7, 2020", "text": ["Natural language processing (NLP) is a theoretically motivated range of computational techniques for analyzing and representing naturally occurring texts at one or more levels of linguistic analysis (Liddy, 2001). The purpose of these techniques is to achieve human-like language processing for a range of tasks or applications. Although it has gained enormous interest in recent years, research in NLP has been going on for several decades dating back to the late 1940s. This review divides its history into two main periods: NLP before (part 1) and during (part 2) the deep learning era.\n", "(If you missed part 1, check NLP before the Deep Learning Era.)\n", "Starting in the 2000s, neural networks begin to be used for language modeling, a task which aims at predicting the next word in a text given the previous words. In 2003, Bengio et al. proposed the first neural language model, that consists of a one-hidden layer feed-forward neural network. They were also one of the first to introduce what is now referred as word embedding, a real-valued word feature vector in R^d. More precisely, their model took as input vector representations of the n previous words, which were looked up in a table learned together with the model. The vectors were fed into a hidden layer, whose output was then provided to a softmax layer that predicted the next word of the sequence. Although classic feed-forward neural networks have been progressively replaced with recurrent neural networks (Mikolov et al., 2010) and long short-term memory networks (Graves, 2013) for language modeling, they remain in some settings competitive with recurrent architectures, the latter being impacted by  catas- trophic forgetting  (Daniluk et al., 2017). Furthermore, the general building blocks of Bengio et al. s network are still found in most neural language and word embedding models nowadays.\n", "In 2008, Collobert and Weston applied multi-task learning, a sub-field of machine learning in which multiple learning tasks are solved at the same time, to neural networks for NLP. They used a single convolutional neural network architecture (CNN; LeCun et al., 1999) that, given a sentence, was able to output many language processing predictions such as part-of-speech tags, named entity tags and semantic roles. The entire network was trained jointly on all the tasks using weight-sharing of the look-up tables, which enabled the different models to collaborate and share general low-level information in the word embedding matrix. As models are being increasingly evaluated on multiple tasks to gauge their generalization ability, multi-task learning has gained in importance and is now used across a wide range of NLP tasks. Also, their paper turned out to be a discovery that went beyond multi-task learning. It spearheaded ideas such as pre-training word embeddings and using CNNs for text, that have only been widely adopted in the last years.\n", "In 2013, Mikolov et al. introduced arguably the most popular word embedding model: Word2Vec. Although dense vector representations of words have been used as early as 2003 (Bengio et al.), the main innovation proposed in their paper was an efficient improvement of the training procedure, by removing the hidden layer and approximating the objective. Together with the efficient model implementation, these simple changes enabled large-scale training of word embeddings on huge corpora of unstructured text. Later that year, they improved the Word2Vec model by employing additional strategies to enhance training speed and accuracy. While these embeddings are not different conceptually than the ones learned with a feed-forward neural network, training on a very large corpus enables them to capture certain relationships between words such as gender, verb tense, and country-capital relations, which initiated a lot of interest in word embeddings as well as in the origin of these linear relationships (Mimno and Thompson, 2017; Arora et al., 2018; Antoniak and Mimno, 2018; Wendlandt et al., 2018). But what made word embeddings a mainstay in current NLP was the evidence that using pre-trained embeddings as initialization improved performance across a wide range of downstream tasks. Since then, a lot of work has gone into exploring different facets of word embeddings (as indicated by the staggering number of citations of the original paper, i.e. 19,071 citations at the time of writing). Despite many more recent developments, Word2Vec is still a popular choice and widely used today.\n", "The year 2013 also marked the adoption of neural network models in NLP, in particular three well-defined types of neural networks: recurrent neural networks (RNNs; Elman, 1990), convolutional neural networks (CNNs), and recursive neural networks (Socher et al., 2013). Because of their architecture, RNNs became popular for dealing with the dynamic input sequences ubiquitous in NLP. But Vanilla RNNs were quickly replaced with the classic long-short term memory networks (LSTMs; Hochreiter and Schmidhuber, 1997), as they proved to be more resilient to the vanishing and exploding gradient problem. At the same time, convolutional neural networks, that were then beginning to be widely adopted by the computer vision community, started to get applied to natural language (Kalchbrenner et al., 2014; Kim, 2014). The advantage of using CNNs for dealing with text sequences is that they are more parallelizable than RNNs, as the state at every time step only depends on the local context (via the convolution operation) rather than all past states as in the RNNs. Finally, recursive neural networks were inspired by the principle that human language is inherently hierarchical: words are composed into higher-order sentences, which can themselves be recursively combined according to a set of production rules. Based on this linguistic perspective, recursive neural networks treated sentences as trees rather than as a sequences. Some research (Tai et al., 2015) also extended RNNs and LSTMs to work with hierarchical structures.\n", "In 2014, Sutskever et al. proposed sequence-to-sequence learning, a general end-to-end approach for mapping one sequence to another using a neural network. In their method, an encoder neural network processes a sentence symbol by symbol, and compresses it into a vector representation. Then, a decoder neural network predicts the output sequence symbol by symbol based on the encoder state and the previously predicted symbols that are taken as input at every step. Encoders and decoders for sequences are typically based on RNNs, but other architectures have also emerged. Recent models include deep-LSTMs (Wu et al., 2016), convolutional encoders (Kalchbrenner et al., 2016; Gehring et al., 2017), the Transformer (Vaswani et al., 2017), and a combination of an LSTM and a Transformer (Chen et al., 2018). Machine translation turned out to be the perfect application for sequence-to-sequence learning. The progress was so significant that Google announced in 2016 that it was officially replacing its monolithic phrase-based machine translation models in Google Translate with a neural sequence-to-sequence model.\n", "In 2015, Bahdanau et al. introduced the principle of attention, which is one of the core innovations in neural machine translation (NMT) and the key idea that enabled NMT models to outperform classic sentence-based MT systems. It basically alleviates the main bottleneck of sequence-to-sequence learning, which is its requirement to compress the entire content of the source sequence into a fixed-size vector. Indeed, attention allows the decoder to look back at the source sequence hidden states, that are then combined through a weighted average and provided as additional input to the decoder. Attention is potentially useful for any task that requires making decisions based on certain parts of the input. For now, it has been applied to constituency parsing (Vinyals et al., 2015), reading comprehension (Hermann et al., 2015), and one-shot learning (Vinyals et al., 2016). More recently, a new form of attention has appeared, called self-attention, being at the core of the Transformer architecture. In short, it is used to look at the surrounding words in a sentence or paragraph to obtain more contextually sensitive word representations.\n", "The latest major innovation in the world of NLP is undoubtedly large pretrained language models. While first proposed in 2015 (Dai and Le), only recently were they shown to give a large improvement over the state-of-the-art methods across a diverse range of tasks. Pre-trained language model embeddings can be used as features in a target model (Peters et al., 2018), or a pre-trained language model can be fine-tuned on target task data (Devlin et al., 2018; Howard and Ruder, 2018; Radford et al., 2019; Yang et al., 2019), which have shown to enable efficient learning with significantly less data. The main advantage of these pre-trained language models comes from their ability to learn word representations from large unannotated text corpora, which is particularly beneficial for low-resource languages where labelled data is scarce.\n"]}, {"link": "https://medium.com/@fareedkhandev/exciting-news-claude-ai-is-now-available-in-95-countries-90047ebf1606?source=list-2eb23a991a63--------24-------0a856388a93a---------------------", "title": "Exciting News   Claude.ai is Now Available in 95 Countries!", "subtitle": "false", "autorName": "Fareed Khan", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ujdMB17AE56yPSA3zeZcNA.jpeg", "clap": "53", "response": "3", "timeForRead": "5 min read", "dateCreate": "Oct 17", "text": ["The world of artificial intelligence is evolving at a pace that can sometimes feel like science fiction come to life. From AI-powered chatbots to robotic helpers, technology is progressing faster than ever. In the midst of this AI revolution, there s one company that s taking a unique approach to ensure that the future remains safe and sound. That company is Anthropic, and they ve just made a groundbreaking move: Claude.ai, their AI chatbot, is now available in a whopping 95 countries.\n", "Check out the availability here: Claude.ai in 95 Countries\n", "Anthropic s CEO, Dario Amodei, has been vocal about the potential risks of AI becoming too autonomous, especially as it gains the ability to access the internet and control robots. And he s not alone in this concern. Many other AI leaders and scientists share his view. To address these concerns, Anthropic has taken an unusual approach by developing their own Large Language Model (LLM). Their latest iteration, Claude 2, is already being hailed as a potential  ChatGPT killer. \n", "But what makes Claude.ai and Claude 2 so special? Let s dive into it.\n", "Claude.ai is an AI chatbot powered by Anthropic s LLM, Claude 2. If you ve ever used ChatGPT or Google Bard, you ll feel right at home with Claude. It s a powerful and flexible chatbot that collaborates with you, writes for you, and answers your questions.\n", "Anthropic, the brains behind Claude, was founded in 2021 by a group of former OpenAI employees who were instrumental in developing GPT-2 and GPT-3. Their primary focus? AI research with an unwavering commitment to safety.\n", "After a successful closed alpha phase with select commercial partners in early 2023, Claude was integrated into products like Notion AI, Quora s Poe, and DuckDuckGo s DuckAssist. In March 2023, Claude opened up its API to a broader range of businesses and finally released its chatbot to the public in July 2023, alongside the launch of Claude 2.\n", "If you re curious about Claude and want to give it a spin, you re in luck. The Claude chatbot, powered by the latest Claude 2 model, is currently available through an open beta in the U.S. and U.K. Anthropic has plans to expand access globally in the future. To get started, simply sign up at Claude.ai. You can initiate a conversation or use one of Claude s default prompts to begin your journey.\n", "And if you re eager for more, Anthropic recently introduced Claude Pro, which offers high-traffic access and access to upcoming features.\n", "Now, you might wonder what sets Claude apart from other AI models. All AI models have their strengths and limitations, and Claude is no exception. One significant concern with AI is bias and inaccuracy, and hallucinations often occur when an AI doesn t know the answer. Claude s mission is to be  helpful, harmless, and honest. \n", "While most AI companies rely on human contractors to fine-tune their models, Anthropic took a different path. In addition to human fine-tuning, they developed a second AI model known as Constitutional AI. This model incorporates rules inspired by the United Nations  Declaration of Human Rights and Apple s terms of service. It ensures Claude s behavior aligns with values that prioritize safety and ethical conduct. These rules are easy to understand and adjust, allowing for transparency and adaptability.\n", "Anthropic takes red teaming to a whole new level. They intentionally provoke Claude to respond in ways that breach its benevolent guardrails. This process helps identify areas for safety improvements. Additionally, Anthropic collaborates with the Alignment Research Center (ARC) for third-party safety assessments, ensuring Claude s safety is rigorously evaluated.\n", "Unlike many AI companies, Anthropic operates as a public benefit corporation. This means that their decisions aren t solely driven by financial gains. While they do partner with big names like Google and Zoom and aim to secure investments, their unique structure enables them to prioritize safety over profits.\n", "One of the most compelling aspects of Claude 2 is its ability to handle up to 100K tokens per prompt. This is equivalent to about 75,000 words, which is twelve times more than GPT-4. Claude 2 performs admirably on standardized tests, though it excels in creative writing while lagging behind in coding and quantitative reasoning. It s also noteworthy that Claude 2 s knowledge extends up to early 2023, surpassing GPT-4 s September 2021 cutoff.\n", "To truly appreciate Claude s capabilities, I decided to put it to the test. I gave it various tasks and compared its performance with other chatbots.\n", "In a test to practice Spanish, Claude, ChatGPT, Llama 2, and Bard each had their moments, but ChatGPT emerged as the victor.\n", "For generating ideas for a dystopian young adult novel, Claude, ChatGPT, and Llama 2 performed similarly. Bard, however, missed the mark entirely.\n", "But where Claude truly shines is in its 100K context window. Although it declined my request to write a 30,000-word novel based on a plot outline, when I accessed the Claude 2 model through Poe, it effortlessly generated the first five chapters of a compelling young adult novel. This was a testament to its creative writing prowess.\n", "Anthropic s unique approach to AI safety doesn t end with Claude s development. They firmly believe that to advocate for AI safety, they need to compete commercially. This influences competitors to prioritize safety and accountability. While it s too early to assess the full impact of Claude s release on the AI industry, Anthropic s leaders were invited to brief the U.S. president and are actively cooperating with organizations like the U.K. s AI Safety Taskforce.\n", "In a surprising twist, a group of researchers who feared the existential threat of AI decided to take matters into their own hands and create a powerful AI model. Thus far, Anthropic s approach appears to be a promising step forward for AI safety.\n", "With Claude.ai now accessible in 95 countries, it s exciting to see how this unique endeavor will shape the future of AI and ensure its responsible and ethical use. Anthropic s dedication to safety is a breath of fresh air in the ever-evolving world of artificial intelligence.\n"]}, {"link": "https://medium.com/@davidsweenor/synthetic-data-and-surveys-64d95fcd429?source=list-e28f6edecf84--------32-------7b153c9756d3---------------------", "title": "Synthetic Data and Surveys", "subtitle": "Exploring ChatGPT s advanced data-analysis capabilities", "autorName": "David Sweenor", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*vw4eCCf1HQOPrYXqmaOFow.png", "clap": "53", "response": "6", "timeForRead": "10 min read", "dateCreate": "Oct 14", "text": ["Recently, a colleague of mine asked if I d be interested in analyzing survey data that was recently collected. I must say, there was a little trepidation since I haven t analyzed raw survey data in quite some time. For the past several years, survey tools like Qualtrics, SurveyMonkey, Pollfish, and Alchemer have embedded analytics and visualizations directly into their web apps, making it easy for anyone to analyze survey data at lightning speed. However, these options were not available for this request.\n", "The other wrinkle in this little story is that my Python skills are quite rusty, and I didn t have an analytics software platform available. So, what were my options? As I pondered this, I downloaded KNIME and thought it would be an excellent opportunity to explore ChatGPT s Advanced Data Analysis capability. I will share my KNIME experience in a separate post and focus this discussion on ChatGPT.\n", "Quite simply, it s artificially fabricated data usually with machine learning (ML) technologies. Synthetic data can be quite complex it can capture complex mathematical relationships between different variables. The synthetic data vault describes synthetic data:  Although the synthetic data is entirely machine generated, it maintains the original format and mathematical properties. This makes synthetic data versatile. It can completely replace the existing data in a workflow, or it can supplement the data to enhance its utility. [1]\n", "Synthetic data can undoubtedly be structured data (i.e. numbers) but can also be text, images, or other formats. In fact, images were one of the first types of synthetic data to take off. Many builders of computer vision applications utilize synthetic data to create different colors, shadows, angles, and other properties to train the algorithm to recognize objects under various conditions and perspectives. It s also relied on quite heavily to train autonomous vehicles.\n", "The need for synthetic data continues to grow with all of the data privacy regulations and restrictions. In fact, the analyst firm Gartner predicts that by \n"]}, {"link": "https://medium.com/@cobusgreyling/openai-has-expanded-their-fine-tuning-gui-0374796014df?source=list-e28f6edecf84--------36-------7b153c9756d3---------------------", "title": "OpenAI Has Expanded Their Fine-Tuning GUI", "subtitle": "OpenAI has simplified fine-tuning considerably by introducing a GUI for creating fine-tuned models.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "69", "response": "9", "timeForRead": "6 min read", "dateCreate": "Oct 8", "text": ["While fine-tuning changes the behaviour of the LLM and RAG provides a contextual reference for inference, fine-tuning has not received the attention it should have in the recent past. One can argue that this is due to a few reasons \n", "In the past OpenAI advised to have more than 1,000 fine-tuning records in the training data set. Hence preparing and formatting data was challenging and time-consuming.\n", "The new OpenAI fine-tuning process requires only 10 records for training a fine-tuned model. And the results of the fine-tuning can be demonstrated via a few simple prompts.\n", "The time the model took to create and train a fine-tuned model was long and following a process of rapid iterations was not feasible. Training time has shortened considerably and via the fine-tuning console and email notification the user is kept up to date.\n", "The fine-tuning UI was in the past command line or program based, the addition of a GUI to upload data, track the progress of fine-tuning jobs, etc. will democratise the process.\n", "The cost of fine-tuning has come down considerably making it accessible for organisation to create custom models.\n", "Data Privacy is still a consideration, with data being sent into the cloud for fine-tuning. Often enterprises demand all computing to take place via an on-premise data centre, or only in certain geographies.\n", "Fine-tuning for completion is important; but there is a significant use-case for classification which is not receiving the attention it should. Classification in the context of traditional chatbots is known as intent detection and NLU is still relevant for classification.\n", "Fine-tuned models are still hosted somewhere and compliance can be hard to reach in some instances. There is a significant opportunity to meet enterprise requirements for hosting and data privacy.\n", "With each expansion of base-LLM functionality, functionality included by default in the LLM offering, a number of products are wiped out. Or put differently, superseded.\n", "The real challenge for fine-tuning large language models in a scaleable and repeatable fashion lies with the data. And in particular data discovery, data design, data development and data delivery. More about the four D s in a follow-up post.\n", "The OpenAI fine-tuning UI which launched recently is very minimalistic, but effective. A list of fine-tuned models is visible on the left, with successful and failed attempts listed. On the top right new fine-tunings can be created, or users can navigate to the training files section.\n", "Below is a view of the files section, where a JSONL file can be uploaded. The minimum size of a training file is 10 records/lines.\n", "The training file text:\n", "The training file needs to be in the structure of the chat mode with roles of system and user.\n", "Once the file is uploaded, it is vetted by OpenAI and if no anomalies are found, a status of ready is assigned.\n", "Below the list of available models for fine-tuning is shown; babbage-002, davinci-002 and gpt-3.5-turbo-0613. There is the option to upload a new fie, or select an existing file. Something I find curious here is that the files are not listed, and users need to navigate to the files section, copy a file ID and navigate back to paste the ID.\n", "The file ID is shown, with the model to fine-tune.\n", "There is a big opportunity in terms of the data; as I have mentioned data discovery, data design and data development.\n", "The first step of data discovery, is to ensure that the training data is aligned with the conversation customers want to have. Considering the Venn diagram below, the bigger the commonality marked  a  is, the more successful the model will be.\n", "Increasing the size of commonality  a  involves the process of performing data discovery on existing customer conversations, and using that data as the bedrock of training data.\n", "  Follow me on LinkedIn for updates on Large Language Models  \n", "I m currently the Chief Evangelist @ Kore AI. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}, {"link": "https://medium.com/@venelinvalkov/autogen-build-powerful-ai-agents-with-chatgpt-gpt-4-426cc50ef720?source=list-e28f6edecf84--------22-------7b153c9756d3---------------------", "title": "AutoGen   Build Powerful AI Agents with ChatGPT/GPT-4", "subtitle": "Explore AutoGen, a Microsoft library that lets you create LLM applications with agents. These agents can communicate and help you solve complex tasks.", "autorName": "Venelin Valkov", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/2*OoQjeo1aWgiGKub_5QxwvA.jpeg", "clap": "231", "response": "1", "timeForRead": "5 min read", "dateCreate": "Oct 17", "text": ["We ll begin with an introduction to AutoGen and its benefits. Then, we ll kick off with a basic example of building a single agent for analyzing stock price trends. Afterward, we ll delve into a more advanced demonstration, using four agents to construct a cryptocurrency indicator, drawing insights from historical prices and news.\n", "AutoGen is like having a bunch of smart friends who work together to get things done, and it s made with help from top-notch researchers.\n", "You can install AutoGen with pip:\n", "Let s add the required libraries:\n", "Next, you need to enter your API key for OpenAI (get yours from https://platform.openai.com/account/api-keys(opens in a new tab)):\n"]}, {"link": "https://medium.com/@busra.oguzoglu/bert-fine-tuning-question-answering-and-named-entity-recognition-284f429f15ae?source=list-a13ace4f182c--------25-------f7e9b3597071---------------------", "title": "BERT Fine-Tuning   Question Answering and Named Entity Recognition", "subtitle": "false", "autorName": "Busra Oguzoglu", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ndWvFZeMt2eV3W7JShaOFA@2x.jpeg", "clap": "53", "response": "1", "timeForRead": "7 min read", "dateCreate": "May 29, 2022", "text": ["BERT (Bidirectional Encoder Representations from Transformers) is a very popular model for language representation and can be very helpful in many downstream tasks such as question answering, NER (named entity recognition), NLI (natural language inference), and so on. When supervised training is done for the problem, pre-trained BERT is used with extra layers. While the BERT model is fine-tuned, other parameters are also learned. In the previous article, fine-tuning process for NLI and sentiment analysis tasks were discussed. As a continuation, the process for QA and NER tasks will be explained in this article, in terms of how inputs and outputs are formed.\n", "Similar to the last article, there are some general rules that we conventionally follow when we use BERT for downstream tasks. They will be explained briefly again below:\n", "Some general notes to keep in mind when using BERT:\n", "1- Before using BERT with different tasks, it is better (and easier) to use BERT tokenizer to tokenize our inputs. The main reason for this is the fact that every tokenizer is implemented in its own way to handle unknown tokens, also, the vocabulary and index mapping that was used for training should match. If we use different tokenizers with BERT, we need to make sure that they are compatible so we will not have any issues down the road [1]. Simply put, make sure to use the same tokenization.\n", "2- After tokenization, we can use a specific format for our input including special tokens. This is different for different tasks so it will be explained in their respective section but as a general rule, we simply add special tokens at the beginning and end of each input sentence. These tokens are [CLS] and [SEP] tokens. [CLS] token stands for  classification , and in classification tasks like NLI it is added at the beginning of the given sentence or sequence of sentences. [SEP] token is the separator token and it is used to indicate the end of a sentence. Also, the [PAD] token (stands for padding) can be used to handle max input length.\n", "Question Answering:\n", "When using BERT in the QA task, we can plug the task-specific inputs and outputs, which are question and passage pairs, into a full sequence [3]. However, before we feed this input, we need to do some prepossessing as explained in the previous section. At first, special tokens [CLS] and [SEP] are added to the input as seen in Figures 1 and 2. [CLS] token is added to the beginning of the question, and [SEP] token is added at the end of the question. It makes sense since we are dealing with a classification task and our input is a sequence of a question, and an answer. In that sense, it can be treated similarly to the NLI task.\n", "We also have to make some structural operations to our input, again, similar to the NLI task which we have examined in the previous article. One of the operations that we need to be doing is adding padding to make the inputs the same size. After this operation our input sequences will look something like this according to their length:\n", "[CLS] context [SEP] question [SEP] [PAD] [PAD]\n", "Also, segment embeddings are added to the tokens as shown in figure 2. These embeddings are used to show which sentence every token belongs to, again, similar to the NLI task. As an example, all tokens in the question can be marked as  A  and all tokens in the answer can be marked as  B [4].\n", "In the QA task, the goal is to predict the start and end of the answer to the question inside a passage, when we have the question and the passage given. (As explained for SQuAD) [5]. Therefore, during the fine-tuning process, a start vector and end vector are used to represent the start and end of the text span [3]. During the training, for a token (word), the dot product of the embedding of the token and the start vector is taken and this gives the probability of the given token being the start word of the answer. Then, Softmax is applied over all words. A similar calculation is done for the end vector as well [4]. Then the score for a candidate span is calculated from position i to position j, and the supervised training objective is defined as the sum of the log-likelihoods of the correct start and end positions [3].\n", "The Stanford Question Answering Dataset (SQuAD) dataset is a popular dataset for this task, it is also used in the pre-trained model for the QA task of the HuggingFace library.\n", "An example entry from the SQuAD dataset is as follows:\n", "When we convert this into the format of BERT:\n", "[CLS] [what] [causes] [precipitation] [to] [fall] [SEP] [in] [meteorology] [precipitation]  [gravity]   [SEP] For this example, start and end vectors will both point to the word [gravity].\n", "Named Entity Recognition:\n", "We conventionally follow similar preprocessing steps for the NER task as well. At first, [CLS] token is added at the beginning of the sentence. In this task we do not have a sentence pair like in NLI or QA so we just add the[SEP] token at the end of the sentence like the sentiment analysis task. One important difference for NER comes with padding. For NER task, we need to do padding both for the sequence and for the labels. This makes sense since for this task the labels should match the input sequence.\n", "After adding extra tokens and padding, we use the BERTtokenizer to convert the tokens to id s. Similar to other tasks, we will also have an input mask to indicate which tokens in the sequence are real tokens and which are used for padding. Then again similarly there are segment ids or segment embeddings as explained before, but here we have a single sequence/segment therefore all can have 0 as their id s. We need to follow a similar procedure for the labels as well. We have label mask, indicating which elements are real labels and which are coming from the padding [7].\n", "As seen in Figure 5, we feed the preprocessed input sequence to the model in a similar way, but what we do with the output is different.\n", "Here, instead of ignoring the outputs for tokens and feeding the [CLS] token to the dense layer as in classification tasks or doing other operations as in QA, we feed all of the final representations of input tokens into the same fully connected network [6].\n", "CoNLL 2003 dataset is a commonly used dataset for the NER task, lets s see an example from this dataset to understand the input structure better.\n", "Sentence:[ORG FISP] official [PER Dermot Moran ] heads for [LOC Beijing ]\n", "There are 3 named entities in this sentence: FISP, Dermot Morgan, and Beijing. Their type is stated with the tags as ORG (Organization), PER (Person), and LOC(Location).\n", "Format of this data is defined in the following way, the last tag is the named entity tag [8]:\n", "official NN I-NP O\n", "Dermot Moran NNP I-NP I-PER\n", "heads VBZ I-VP O\n", "for INI- PP O\n", "Beijing NNP I-NP I-LOC\n", "For our task, we will have the following entry for this example:\n", "[CLS] [official] [Dermot] [Morgan] [heads] [for] [Beijing] [SEP]\n", "Thank you for reading this article, if you find it useful, please consider giving a clap :) Any comments and suggestions are highly appreciated. I suggest you visit https://d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html for more information about this topic, which was really helpful for me to write this brief explanation. If you want to learn more about the process for NLI and sentiment analysis tasks, consider reading my previous article.\n", "References\n", "[1]  15.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications,  15.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications   Dive into Deep Learning 0.17.1 documentation. [Online]. Available: https://d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html. [Accessed: 10-Jan-2022].\n", "[2] J. Alammar,  The illustrated bert, elmo, and co. (how nlp cracked transferlearning). \n", "[3] J. Devlin, M. Chang, K. Lee, and K. Toutanova,  BERT: pre-trainingof deep bidirectional transformers for language understanding,  CoRR,vol. abs/1810.04805, 2018\n", "[4] D. Bhageria,  Build a smart question answering system with fine-tuned bert, Jun 2020.\n", "[5] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang,  Squad: 100, 000+ questions for machine comprehension of text,  CoRR, vol. abs/1606.05250, 2016.\n", "[6]  15.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications,  15.6. Fine-Tuning BERT for Sequence-Level and Token-Level Applications   Dive into Deep Learning 0.17.1 documentation. [Online]. Available: https://d2l.ai/chapter_natural-language-processing-applications/finetuning-bert.html. [Accessed: 10-Jan-2022].\n", "[7] B. Kundumani,  Fine tuning bert for ner on conll 2003 dataset with tf 2.2.0,  Sep 2020.\n", "[8] A. Laplace,  Conll-2003 in the application of datasets of named entity recognition of 24th world congress of ,  Dec 2019.26\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-12-dac8146c288c?source=list-660438a01f7f--------4-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing(Part 12)", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "2", "response": "2", "timeForRead": "5 min read", "dateCreate": "Sep 24", "text": ["Specifically, you will understand why the cost function is designed that way. You will see what happens when you predict the true label and you ll see what happens when you predict the wrong label.\n", "Let s dive in and see how this logistic regression cost function is designed. Let s have a look now at the equation of the cost function, while this might look like a big complicated equation, it s actually rather straightforward, once you break it down into its components. First, have a look at the left hand side of the equation where you find a sum over the variable m, which is just the number of training examples in your training set. This indicates that you re going to sum over the cost of each training example.\n", "Out front, there is a -1/m, indicating that when combined with the sum, this will be some kind of average. The minus sign ensures that your overall costs will always be a positive number as you ll see clearly later in this tutorial.\n", "Inside the square brackets, the equation has two terms that are added together. To consider what each of these terms contributes to the cost function for each training example, let s have a look at each of them separately.\n", "The term on the left is the product of y superscript i, which is the label for each training example, most applied by the log of the prediction, which is the logistic regression function applied to each training example. Represented as h of superscript i, and a parameter theta.\n", "Now, consider the case when your label is 0. In this case, the function h can return any value, and the entire term will be 0 because 0 times anything is just 0.\n", "What about the case when your label is 1? If your prediction is close to 1, then the log of your prediction will be close to 0, because, as you may recall, the log of 1 is 0. And the product will also be near 0. If your label is 1, and your prediction is close to 0, then this term blows up and approaches negative infinity. Intuitively, now, you can see that this is the relevant term in your cost function when your label is 1.\n", "When your prediction is close to the label value, the loss is small, and when your label and prediction disagree, the overall cost goes up.\n", "Now consider the term on the right hand side of the cost function equation, in this case, if your label is 1, then the 1- y term goes to 0. And so any value returned by the logistic regression function will result in a 0 for the entire term, because again, 0 times anything is just 0. If your label is 0, and the logistic regression function returns a value close to 0, then the products in this term will again be close to 0. If on the other hand your label is 0 and your prediction is close to 1, then the log term will blow up and the overall term will approach to negative infinity.\n", "From this exercise you can see now that there is one term in the cost function that is relevant when your label is 0, and another that is relevant when the label is 1. In each of these terms, you re taking the log of a value between 0 and 1, which will always return a negative number, and so the minus sign out front ensures that the overall cost will always be a positive number.\n", "Now, let s have a look at what the cost function looks like for each of the labels. 0 and 1, overall possible prediction values. First, we re going to look at the loss when the label is 1. In this plot, you have your prediction value on the horizontal axis and the cost associated with a single training example on the vertical axis. In this case J, of theta simplifies to just negative log h(x(theta). When your prediction is close to 1, the loss is close to 0. Because your prediction agrees well with the label. And when the prediction is close to 0, the loss approaches infinity, because your prediction and the label disagree strongly. The opposite is true when the label is 0. In this case J(theta) reduces to just minus log(1- h(x, theta).\n", "Now when your prediction is close to 0, the loss is also close to 0. And when your prediction is close to 1, the loss approaches infinity. You now understand how the logistic regression cost function works. You saw what happened when you predicted a 1 and the true label was a 1. You also saw what happened when youpredicted a 0, and the true label was a 0. In the next week, you will learn about Naive Bayes, which is a different type of classification algorithm, which also allows you to predict whether a tweet is positive or negative.\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "Note:if you are a NLP export and have some good suggestions to improve this blog to share, you write comments and contribute.\n", "if you need more update about NLP and want to contribute then following and enroll in following\n", " Course: Natural Language Processing (NLP)\n", " GitHub Repository\n", "   Notebook\n", "1- Natural Language Processing with Classification and Vector Spaces\n", "2- Logistic Regression: Cost Function\n"]}, {"link": "https://medium.com/@anirudhlohia/automated-summarisation-of-pdfs-with-gpt-and-python-8cb398e5f029?source=list-dee72bb8661c--------10-------c25b06fd87f2---------------------", "title": "Automated Summarisation of PDFs with GPT API in Python", "subtitle": "false", "autorName": "Anirudh @ krysins.com", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*49jrAEyiE0yxp6zyB1QeNw.jpeg", "clap": "114", "response": "5", "timeForRead": "5 min read", "dateCreate": "Mar 2", "text": ["Of course, jump ahead if you have had too much on what GPT can do.\n", "The GPT (Generative Pre-trained Transformer) is a powerful language model developed by OpenAI, which has revolutionised the field of natural language processing (NLP). It is a neural network-based system that has been pre-trained on a massive corpus of text data and is capable of generating high-quality text content, including articles, summaries, and responses to queries.\n", "The GPT API was first released in 2018 and has since undergone several improvements and updates, including the latest version, GPT-3, which was released in June 2020. This version has 175 billion parameters, making it one of the largest and most powerful language models ever created. It has been trained on a diverse range of text sources, including books, articles, and web pages, and has been shown to outperform previous language models on a variety of NLP tasks.\n", "In this project, we will explore how to leverage the power of the GPT API in Python to automate the summarisation of PDF documents, providing a useful tool for researchers, analysts, and other professionals who work with large amounts of textual data.\n", "Three simple high level steps only:\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-11-79ba0c3b0623?source=list-660438a01f7f--------5-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing(Part 11)", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "2", "response": "2", "timeForRead": "4 min read", "dateCreate": "Sep 17", "text": ["Now that you have your Theta, you will use this theta to predict our new data points. For example, given a new tweet, you will use this theta to say whether this tweet is positive or negative. In doing so, you want to analyze whether your model generalizes well or not. In this tutorial, we will show you whether your model generalizes well or not, and specifically, we ll show you how to compute the accuracy of your model.\n", "Let s take a look at how you can do this. For this, you will need X_val and Y_val. Theta that was set aside during training, also known as the validation sets and Theta, the sets of optimum parameters that you got from training on your data. First, you will compute the sigmoid function for X_val with parameters Theta, then you will evaluate if each value of h of Theta is greater than or equal to a threshold value, often set to 0.5.\n", "For example, if your h X Theta is equal to the following vector, 0.3, 0.8, 0.5, etc., up to the number of examples from your validation set, you re going to assert if each of its components is greater than or equal to 0.5. So is 0.3 greater than or equal to 0.5? No. So our first prediction is equal to 0. Is 0.8 greater than or equal to 0.5? Yes. So our prediction for the second example is 1. Is 0.5 greater than or equal to 0.5? Yes. So our third prediction is equal to 1, and so on. At the end, you will have a vector populated with zeros and ones indicating predicted negative and positive examples, respectively.\n", "After building the predictions vector, you can compute the accuracy of your model over the validation sets\n", "To do so, you will compare the predictions you have made with the true value for each observation from your validation data. If the values are equal and your prediction is correct, you ll get a value of 1 and 0 otherwise. For instance, if your prediction was correct, like in this case where your prediction and your label are both equal to 0, your vector will have a value equal to 1 in the first position. Conversely, if your second prediction wasn t correct because your prediction and label disagree, your vector will have a value of 0 in the second position and so on and so forth. After you have compared the values of every prediction with the true labels of your validation set, you can get the total times that your predictions were correct by summing up the vector of the comparisons. Finally, you ll divide that number over the total number m of observations from your validation sets. This metric gives an estimate of the times that your logistic regression will correctly work on unseen data. So if your accuracy is equal to 0.5, it means that 50 percent of the time, your model is expected to work well.\n", "For instance, if your Y_val and prediction vectors for five observations look like this, you ll compare each of their values and determine whether they match or not. After that, you ll have the following vector with a single 0 in the third position where the prediction and the label disagree. Next, you have to sum the number of times that your predictions were right and divide that number by the total number of observations in your validation sets. For example, you get an accuracy equal to 80 percent.\n", "You learned many concepts this week. The first thing you learned is you learned how to preprocess a text. You learned how to extract features from that text. You learned how to use those extracted features and train a model using those. Then you learned how to test your model.\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "Note:if you are a NLP export and have some good suggestions to improve this blog to share, you write comments and contribute.\n", "if you need more update about NLP and want to contribute then following and enroll in following\n", " Course: Natural Language Processing (NLP)\n", " GitHub Repository\n", "   Notebook\n", "1- Natural Language Processing with Classification and Vector Spaces\n", "2- Logistic Regression testing\n"]}, {"link": "https://medium.com/@shanakachathuranga/end-to-end-machine-learning-pipeline-with-mlops-tools-mlflow-dvc-flask-heroku-evidentlyai-github-c38b5233778c?source=list-2c27d980d3f3--------34-------338c7da11cbf---------------------", "title": "End to End Machine Learning Pipeline With MLOps Tools (MLFlow+DVC+Flask+Heroku+EvidentlyAI+Github Actions)", "subtitle": "false", "autorName": "Shanaka Chathuranga", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*BuhWs_EjcILbIZBjTPHv2g.jpeg", "clap": "325", "response": "10", "timeForRead": "11 min read", "dateCreate": "Oct 4, 2021", "text": ["This article will show you how to automate the entire machine learning lifecycle with the MLOps tools. Firstly, a simple machine learning model will be trained using a churn dataset which is available in Kaggle. Then, the deployment will be done using flask, GitHub actions, and Heroku. Finally, production model monitoring will be done using EvidentlyAI. Below tools will be used throughout the project. Most of these tools are open-source so that anyone can simply experiment with the code. The full codebase is available here in the Github repository. The main reference for this article is the 4-day MLOps course by Krish Naik's youtube channel.\n", "Step 1: Create the EnvironmentCreate a working directory and change the directory to the newly created folder. Then open a terminal and create a new Conda environment as below. I have created the new Conda environment with python 3.7 and named it churn_model. Then activate the new environment. You can either install the anaconda if you do not have Anaconda installed on your machine or another option is to use a python virtual environment. I have been used the anaconda environment throughout this experiment\n", "Step 2: Create the data science project structure with the cookiecutter It is important to have a project structure when we are dealing with a data science project. We can use the cookiecutter template to organize the project. You will be asked to enter the required details when importing cookiecutter. Below details have been added for this project.\n", "Step 3: Create a Github repoCreate a GitHub repo and push the current version to the repo. Here I have done all the developments in the main branch. You can either create a separate branch and do the developments as well.\n", "Step4: Download the Dataset Download a training dataset from Kaggle and put it in the external folder inside the data folder. Refer to this to download the train.csv from Kaggle. There are 4 folders inside the data main folder. We will be only using external, raw, and processed folders in this project.\n", "external: External files (ex. train.csv from Kaggle) raw: Raw data for this project Processed: Processed files using the raw files\n", "Step5: Track the dataset with DVCData Version Control (DVC) is a new form of Git-based data versioning, workflow, and experiment management software. We will be using GIT for code version control while DVC for the data version control. You can refer to more information on DVC from here. In this project, I will explain how DVC can be used. Check the external folder after executing the below three commands. You will see the new file named train.csv.dvc. This will be used by DVC to track the train.csv file. Important: Remember to comment the /data/ line in .gitignore file before execute the commands. Because, now we are going to track datasets with the DVC, therefore, it will create a separate .gitignore file inside the data folder.\n", "Step6: Create the source code inside the src folder All python scripts related to the projects are located in the src folder. There are 4 folders namely data, features, visualization, models, and prediction within the src folder. But in this project, I will be using only data, models, and the prediction folders. Also params.yaml file need to be created inside the main churn_model folder.\n", "Data: Data loading related python scripts (load_data.py, split_data.py)Models: Model-related python scripts (train_model.py, production_model_selection.py, model_monitor.py)\n", "7. Pipeline Creation After creating the above files in src, Now it's time to write the model pipeline to execute the model. DVC will be used to create the model pipeline. For that, first, create the dvc.yaml file inside the churn_model folder.\n", "8. Pipeline ExecutionThe next step is to execute the model pipeline. Now we are going to execute the dvc.yaml file. It contains four stages. Each stage contains at least three steps1. cmd: command used to execute the script. 2. deps: specify the dependencies to execute the step.3. outs: output from the step(Model files or datasets).4. params: parameters used in the script.Execute the below commands to run the pipeline. Remember to start the mlflow sever using another terminal.\n", "The advantage of using DVC is that it only executes the stage only if dependencies are changed. For example, if we run the dvc repro command again, it will not execute any of the stages. But if we change one of the random forest model-related parameters(max_depth or n_estimators) in the params.yaml, it will execute the stages after the model_train. Because those are dependencies for executing the model_train stage. But it will not execute the first two stages since no change happened to the dependencies of the first two stages. (Do several experiments by changing random forest parameters). Now it s time to check the mlflow dashboard.\n", "Here I have done three experiments by changing the parameter values. It will store history with the parameter values and model results. In the log_production_model stage, it will automatically find the best-performing model using one of the accuracy measures (accuracy was used in this project). Then the best model will be saved inside the models folder(If you check the models folder, you will be able to see the model file). This model will be used for the prediction service. We can use mlflow as a model registry as above. You can try different ML models with different parameter combinations. This will store all the information related to all runs.\n", "9. Web app with FlaskA simple web app will be created using a flask. Flask is a micro web framework written in Python. This web app will be used to consume the created model. A user can enter the feature values into the form and after submitting, the model will predict the outcome(churn or not).Create a new folder named webapp and put the required HTML, CSS, and JavaScript codes inside the folder. (You can get the code from here). Also, remember to put the model.joblib file to the model_webapp_dir in webapp folder.\n", "Now it s time to create the python code related to the web app. Create app.py file in chrun_folder. The objective of this script is to send the response to the frontend after predicting the target using the request.\n", "10. Unit tests Pytest will be used to do the simple testing. For that, create a separate folder named tests inside the main directory. Then create the test_config.py and __Init__.py files inside the newly created folder. Here a simple test will be performed to check whether the entered values are numerical or not. It will raise an exception if someone enters a string value instead of a numerical value. It is important to remember that the function name of all test cases must start with the test. After creating the unit tests we can test them by executing the below command. We can also do this as a frontend validation. But, here I did it in the backend just to show the unit tests capabilities of python. Here it checks whether the required error message is passed when entering incorrect values into the form. (Ex. adding one or more non-numerical values)\n", "11. Create an app in Heroku Heroku will be used to deploy the application. Create an account in Heroku if you do not have one. After that follow the steps below to create the app and authorization token for the app.\n", "* Go to https://dashboard.heroku.com/apps* Click New and create a new app * Give a name for the app and create it (I named it churnmodelprod)* In the deployment method section, click Github.* In the connect to Github section, enter the repo name and search. It will find the repo name for you. Then click connect.* In the automatic deployed section, tick Wait for CI to pass before deploying and click enable the automatic deploy button. * Then go to account setting   application   Authorizations   create authorization.* Then enter the description in the box and click create.* Copy and save the authorization token for future use (We need this in the next step to create secrets).\n", "12. Create CI-CD pipeline using GitHub actions CI-CD pipeline will be created using the GitHub actions. Create a new file inside the .github/workflows folder and named it as the ci-cd.yaml. Please note that the file extension of this must be yaml. We can easily reflect the changes in the model or code through the frontend after implementing the CI-CD pipeline. Because we just need to push the code after doing the modifications and it will reflect the changes automatically. That is the advantage of using the CI-CD pipeline for ML model development. Because in ML context, there is a model retraining part that is not included in the normal software development life cycle(We will be discussing the retraining part in the 13th section). We can easily reflect the changes to the model with this approach.\n", "Now we need to create two secrets inside GitHub as HEROKU_API_TOKEN and HEROKU_API_NAME to do the deployment. * Select the repo and click the settings. * Then click secrets in the left panel. * Click new repository secrets. Need to create two secrets. 1. name: HEROKU_API_NAME |value: churnmodelprod2. name: HEROKU_API_TOKEN |value: Authorization token saved in the last step\n", "Now it s time to push the final code to GitHub. It will automatically be deployed to Heroku using the Github actions. You can check the status of deployment by clicking the actions tab in the Github repo. So we have automated the entire machine learning life cycle. Therefore, whenever there are modifications to the code or model parameters, the changes will be reflected automatically in the front-end.\n", "Now, it s time to access the final URL from Heroku. For that, first, select the dashboard from the top right corner and select the new app (in my case it is churnmodelprod), and go to settings and check the domain section. You can get the URL from there.\n", "Now Just enter some random values to fill the form and check the output. It will show the output in the prediction section. That is, it will predict whether the customer will be churn or not (yes/no)\n", "Also, remember to check the output by entering non-numeric values into the form. The output is as below. That is our validation is working successfully.\n", "13. Production Model monitoring with EvidentlyAINow our model is in the production environment. But with time model s performance may degrade. That is the common nature of any machine learning model. Because of that, we will need to retrain our model with new data. Therefore we need to continuously monitor the model performance. EvidentlyAI is a very good tool to measure model performance. Also, this can be used to check model drift or concept drift if there is any. It uses various statistical tests such as t-test, chi-squared..etc to measure the drift.\n", "Let's assume you receive a new dataset. Now our objective is to check any data drift that has happened in our new dataset. Since we do not have a new dataset, for the demonstration purpose, I just extracted a sample from the same original train dataset with size 1000 and put it inside the raw folder as train_new.csv. But in a real-world scenario, we will have a totally new train dataset after some time. After that, we can check the data drift using the below python script. It will create an HTML file inside the reports folder.\n", "A new file named data_and_target_drift_dashboard is created inside the report folder. There are three sections in this report. 1. Input features drift: This shows if there is any data drift in input features. 2. Target drift: This Shows if there is any data drift in the target.3. Target behavior by feature: This shows the target behavior according to each feature. Below tests are used by EvidentlyAI to measure the drift. All tests use the 95% confidence level by default. Numerical variables: two-sample Kolmogorov-Smirnov test.Categorical variables: chi-squared test\n", "According to the above outputs, no data drift or target drift was detected. If there is a drift in input features we may need to retrain the model. There are a lot of awesome functions inside this tool that we can use to monitor the model performance. I just used two functions to generate the above three outputs. There is a possibility to get the output of the above results as a json file. We can automate the retraining process by using the values in the json file, in which the automatic retraining process can be incorporated into the main pipeline easily.\n", "References: 1. 4-day MLOps course by Krish Naik s youtube channel (https://www.youtube.com/watch?v=1BSwYlJUxK0&list=PLZoTAELRMXVOk1pRcOCaG5xtXxgMalpIe)2. https://dvc.org/3. https://evidentlyai.com/\n"]}, {"link": "https://medium.com/@fareedkhandev/exciting-news-claude-ai-is-now-available-in-95-countries-90047ebf1606?source=list-e28f6edecf84--------11-------7b153c9756d3---------------------", "title": "Exciting News   Claude.ai is Now Available in 95 Countries!", "subtitle": "false", "autorName": "Fareed Khan", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ujdMB17AE56yPSA3zeZcNA.jpeg", "clap": "53", "response": "3", "timeForRead": "5 min read", "dateCreate": "Oct 17", "text": ["The world of artificial intelligence is evolving at a pace that can sometimes feel like science fiction come to life. From AI-powered chatbots to robotic helpers, technology is progressing faster than ever. In the midst of this AI revolution, there s one company that s taking a unique approach to ensure that the future remains safe and sound. That company is Anthropic, and they ve just made a groundbreaking move: Claude.ai, their AI chatbot, is now available in a whopping 95 countries.\n", "Check out the availability here: Claude.ai in 95 Countries\n", "Anthropic s CEO, Dario Amodei, has been vocal about the potential risks of AI becoming too autonomous, especially as it gains the ability to access the internet and control robots. And he s not alone in this concern. Many other AI leaders and scientists share his view. To address these concerns, Anthropic has taken an unusual approach by developing their own Large Language Model (LLM). Their latest iteration, Claude 2, is already being hailed as a potential  ChatGPT killer. \n", "But what makes Claude.ai and Claude 2 so special? Let s dive into it.\n", "Claude.ai is an AI chatbot powered by Anthropic s LLM, Claude 2. If you ve ever used ChatGPT or Google Bard, you ll feel right at home with Claude. It s a powerful and flexible chatbot that collaborates with you, writes for you, and answers your questions.\n", "Anthropic, the brains behind Claude, was founded in 2021 by a group of former OpenAI employees who were instrumental in developing GPT-2 and GPT-3. Their primary focus? AI research with an unwavering commitment to safety.\n", "After a successful closed alpha phase with select commercial partners in early 2023, Claude was integrated into products like Notion AI, Quora s Poe, and DuckDuckGo s DuckAssist. In March 2023, Claude opened up its API to a broader range of businesses and finally released its chatbot to the public in July 2023, alongside the launch of Claude 2.\n", "If you re curious about Claude and want to give it a spin, you re in luck. The Claude chatbot, powered by the latest Claude 2 model, is currently available through an open beta in the U.S. and U.K. Anthropic has plans to expand access globally in the future. To get started, simply sign up at Claude.ai. You can initiate a conversation or use one of Claude s default prompts to begin your journey.\n", "And if you re eager for more, Anthropic recently introduced Claude Pro, which offers high-traffic access and access to upcoming features.\n", "Now, you might wonder what sets Claude apart from other AI models. All AI models have their strengths and limitations, and Claude is no exception. One significant concern with AI is bias and inaccuracy, and hallucinations often occur when an AI doesn t know the answer. Claude s mission is to be  helpful, harmless, and honest. \n", "While most AI companies rely on human contractors to fine-tune their models, Anthropic took a different path. In addition to human fine-tuning, they developed a second AI model known as Constitutional AI. This model incorporates rules inspired by the United Nations  Declaration of Human Rights and Apple s terms of service. It ensures Claude s behavior aligns with values that prioritize safety and ethical conduct. These rules are easy to understand and adjust, allowing for transparency and adaptability.\n", "Anthropic takes red teaming to a whole new level. They intentionally provoke Claude to respond in ways that breach its benevolent guardrails. This process helps identify areas for safety improvements. Additionally, Anthropic collaborates with the Alignment Research Center (ARC) for third-party safety assessments, ensuring Claude s safety is rigorously evaluated.\n", "Unlike many AI companies, Anthropic operates as a public benefit corporation. This means that their decisions aren t solely driven by financial gains. While they do partner with big names like Google and Zoom and aim to secure investments, their unique structure enables them to prioritize safety over profits.\n", "One of the most compelling aspects of Claude 2 is its ability to handle up to 100K tokens per prompt. This is equivalent to about 75,000 words, which is twelve times more than GPT-4. Claude 2 performs admirably on standardized tests, though it excels in creative writing while lagging behind in coding and quantitative reasoning. It s also noteworthy that Claude 2 s knowledge extends up to early 2023, surpassing GPT-4 s September 2021 cutoff.\n", "To truly appreciate Claude s capabilities, I decided to put it to the test. I gave it various tasks and compared its performance with other chatbots.\n", "In a test to practice Spanish, Claude, ChatGPT, Llama 2, and Bard each had their moments, but ChatGPT emerged as the victor.\n", "For generating ideas for a dystopian young adult novel, Claude, ChatGPT, and Llama 2 performed similarly. Bard, however, missed the mark entirely.\n", "But where Claude truly shines is in its 100K context window. Although it declined my request to write a 30,000-word novel based on a plot outline, when I accessed the Claude 2 model through Poe, it effortlessly generated the first five chapters of a compelling young adult novel. This was a testament to its creative writing prowess.\n", "Anthropic s unique approach to AI safety doesn t end with Claude s development. They firmly believe that to advocate for AI safety, they need to compete commercially. This influences competitors to prioritize safety and accountability. While it s too early to assess the full impact of Claude s release on the AI industry, Anthropic s leaders were invited to brief the U.S. president and are actively cooperating with organizations like the U.K. s AI Safety Taskforce.\n", "In a surprising twist, a group of researchers who feared the existential threat of AI decided to take matters into their own hands and create a powerful AI model. Thus far, Anthropic s approach appears to be a promising step forward for AI safety.\n", "With Claude.ai now accessible in 95 countries, it s exciting to see how this unique endeavor will shape the future of AI and ensure its responsible and ethical use. Anthropic s dedication to safety is a breath of fresh air in the ever-evolving world of artificial intelligence.\n"]}, {"link": "https://medium.com/@siddharth.vij10/prompt-engineering-llama-2-chat-cot-react-few-shot-self-critiq-fbf3bbf6688f?source=list-e28f6edecf84--------26-------7b153c9756d3---------------------", "title": "Prompt Engineering | LLaMA-2 Chat | COT | ReAct | Few Shot | Self-Critique", "subtitle": "false", "autorName": "Siddharth vij", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*qX-lvumGYc5ru9eoZFALFw.jpeg", "clap": "39", "response": "1", "timeForRead": "5 min read", "dateCreate": "Sep 21", "text": ["We have multiple resources on internet that discuss about variety of methods to create prompts i.e. prompt engineering. Here, my intention is to explain the major Prompt Engineering methods by following easy going examples.\n", "Happy Reading ..\n", "Specifications of the experiments done are mentioned below.\n", "Together.ai provides an inferencing service and also provides some credit balance for trial purposes. I have utilized the same for below examples.\n", "Let s begin with the methods and examples \n", "Zero Shot Prompt  In this method, we rely on model to provide us answer to the question without us providing additional support. As you can see in the example below, we are asking the LLM to find out logical flaw in a sample text. The flaw is related to the usage of pronouns   we are using  his  for peter in first sentence and  She  for peter in second sentence. That s what is confusing in the text.\n", "Result   Model isn t able to catch hold of the issue w.r.t bad usage of pronouns\n", "Few Shot Prompt   Model is supplied with one or more example question and answer. The examples help the model build reasoning capability in expected direction. Let s see how model does with support of an example.\n", "Result   Model is able to catch hold of this issue w.r.t bad usage of pronouns by following the reasoning provided in example.\n", "Few Shot Prompt (Complicated Question)  This looks to be the same as previous example. The only difference is that the question is made complex. It is a bit tough to identify the misuse of pronouns now.\n", "Result   As expected, model miss the key issue w.r.t pronoun usage when we complicated the situation.\n", "Self-Critique   The complicated question which was left unanswered in previous example is used here as well. The difference here is that we force the model to think of multiple outputs and then come up with the best possible output. We do this by adding a line to the prompt i.e.  Think of all possible logical flaws and critique to return the most obvious logical flaw .\n", "Result   The model is able to pick up the incorrect usage of pronouns after being forced to evaluate its responses using self critique.\n", "Chain Of Thought (COT)   In this technique, you are pushing LLM to think step by step. This happens by adding a text  Let s think step by step  as shown in below picture. Please note that there are multiple variations of COT. Once such variation can be seen in the blog available on prompting guide. There is also an excellent video tutorial on prompt engineering by DeepLearning.AI here that gives excellent COT methods.\n", "Result   The model is able to pick up the incorrect usage of pronouns after being forced to think step by step.\n", "Chain Of Thought (Complicated Question)   Here, we complicated the situation to such an extent that COT does not work as expected. Let s see what happens.\n", "Result   As we see in the output, LLM is unable to figure out the most obvious flaw in the situation.\n", "Chain Of Thought & Few Shot   To overcome the issue we faced in the previous example w.r.t a complicated situation, we provided an example in the prompt to help COT find out the flaw successfully.\n", "Result   Model is able to find out the flaw in situation w.r.t bad usage of pronouns.\n", "Reasoning and Action (ReAct)   Here, we are deviating from the gender inconsistency example that we utilized in prior methods. The idea, here, is to see if a complicated question that needs multiple steps can be answered correctly by giving the model some sample questions and the steps involved in arriving at the answers to these questions. For example, in the first sample question as seen below, we are supposed to find age of Leo DiCaprio s girlfriend and then do a mathematical computation on top of her age. This requires multiple steps. We need to find Leo s identity, then his girlfriend s identity & age and in the last do the mathematics asked in the question. We provide LLM with all these steps.\n", "Below is the second example that we are providing LLM in the prompt. This is a similar example that requires multiple custom steps to reach to an answer. Also, notice that we are also asking a question in below picture to the LLM. The question is  Who was the first PM of India and what would be his age if he was alive today .\n", "Response   Below is the response we received from LLM. As we see, LLM is able to do all the steps based on the samples we supplied to it in prompt. This is what I was talking about.\n", "Result   The answer is still incorrect. However, model is able to understand that it has to perform some steps related to reasoning and provide a response. I was able to get correct answer for the exact same prompt by upgrading the model from LLaMA-2 Chat (13B) to LLaMA-2 Chat (70B).\n", "The example that we did above for ReAct can also be done without giving any sample reasoning and action samples. This can be achieved using LangChain library and will require OpenAI subscription.\n", "That s all for this article. In case you come across other popular techniques, please feel free to add them in comments. I will try to add examples of those in this article as well.\n", "References:https://python.langchain.com/docs/modules/agents/agent_types/reacthttps://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/1/introductionhttps://www.promptingguide.ai/\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-3-214177ca96d4?source=list-660438a01f7f--------14-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing (Part 3)", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "1", "response": "2", "timeForRead": "8 min read", "dateCreate": "Jul 30", "text": ["Sentiment analysis, also known as opinion mining, is a technique used to determine the sentiment or emotion expressed in a piece of text. It has gained significant popularity in recent years due to the rise of social media and the need to understand customer opinions and feedback. In this blog post, we will explore how to perform sentiment analysis using logistic regression with Python.\n", "Sentiment analysis is a natural language processing (NLP) technique that aims to understand and categorize the sentiment expressed in a given text. It involves analyzing the words, phrases, and context of the text to determine whether the sentiment is positive, negative, or neutral.\n", "Sentiment analysis, also known as opinion mining, is a technique used to determine the sentiment or emotion expressed in a piece of text. It has gained significant popularity in recent years due to its applications in various fields such as marketing, customer feedback analysis, and social media monitoring. In this blog post, we will explore the concept of sentiment analysis and delve into the details of using logistic regression as a powerful tool for sentiment classification.\n", "In this section, we will provide an overview of sentiment analysis and its importance in today s data-driven world.\n", "Sentiment analysis is the process of extracting subjective information from text and determining the sentiment or emotion associated with it. It involves analyzing the text to classify it into positive, negative, or neutral sentiment categories. The main goal of sentiment analysis is to understand the opinions, attitudes, and emotions expressed by individuals or groups. This information can then be used to make informed decisions, improve customer service, and gain valuable insights.\n", "Def: Sentiment analysis has been widely used since the early 20th century, and its research area is still fast growing. One of the most advanced solutions is to use AI to proceed with sentiment analysis. The algorithm uses a natural language processing (NLP) technique which enables it to determine the moods or emotions of a piece of text. In this case, companies can react based on user feedback.\n", "Def: Sentiment analysis is a Natural Language Processing (NLP) [2] technique used to determine the sentiment of a text by automatically identifying its underlying opinions. The sentiment can be positive (e.g.  I m very happy today ), negative (e.g.  I didn t like that movie ), or neutral (e.g.  Today is Friday , which may be subjectively seen as positive by some people actually  ) [1]\n", "Sentiment analysis typically works by first identifying the sentiment of individual words or phrases. This can be done using a variety of methods, such as lexicon-based analysis, machine learning, or natural language processing.\n", "Once the sentiment of individual words or phrases has been identified, they can be combined to determine the overall feeling of a piece of text. This can be done using a variety of techniques, such as sentiment scoring or sentiment classification [4].\n", "Background: machine-learning classification task of sentiment analysis. In this example you have the tweet, let s say, I m happy because I m learning NLP. And the objective in this task is to predict whether a tweet has a positive or negative sentiment. And you ll do this by starting with a training set where tweets with a positive sentiment have a label of one, and the tweets with a negative sentiment have a label of zero.\n", "In order for you to implement logistic regression, you need to take a few steps. In this tutorial you will learn about the steps required in order to implement this algorithm, so let s take a look.\n", "In supervised machine learning, you have input features X and a set of labels Y. Now to make sure you re getting the most accurate predictions based on your data, your goal is to minimize your error rates or cost as much as possible. And to do this, you re going to run your prediction function which takes in parameters data to map your features to output labels Y hat.\n", "Now the best mapping from features to labels is achieved when the difference between the expected values Y and the predicted values Y hat is minimized. Which the cost function does by comparing how closely your output Y hat is to your label Y. Then you can update your parameters and repeat the whole process until your cost is minimized. So let s take a look at the supervised\n", "Logistic regression is a statistical model used to predict binary outcomes. It is commonly used when the dependent variable is dichotomous, meaning it can take only two values. In the context of sentiment analysis, the binary outcome represents the sentiment category (positive or negative).\n", "Logistic regression is a popular machine learning algorithm used for binary classification problems. It is well-suited for sentiment analysis because it can handle text data and provide probabilistic outputs. Logistic regression models are interpretable and can capture nonlinear relationships between features and labels.\n", "Logistic regression works by estimating the probability of an event occurring based on a set of independent variables. It uses a logistic function (also known as a sigmoid function) to map the linear combination of the independent variables to a value between 0 and 1. This value represents the probability of the event occurring.\n", "For this task you will be using your logistic regression classifier which assigns its observations to two distinct classes. Next up I ll show you how to do this. So to get started building a logistic regression classifier that s capable of predicting sentiments of an arbitrary tweet.\n", "You will first process the raw tweets in your training sets and extract useful features. Then you will train your logistic regression classifier while minimizing the cost. And finally you ll be able to make your predictions. So in this video you learned about the steps required for you to classify a tweet. Given the tweet, you should classify it to either be positive or negative. In order for you to do so, you first have to extract the features. Then you have to train your model. And then you have to classify the tweet based off your trained model. In the next video, you re going to learn how to extract these features. So let s take a look at how you can do that\n", "Before we can build a sentiment analysis model, we need to prepare the data. This involves cleaning and preprocessing the text, as well as labeling the data with sentiment labels (positive, negative, or neutral).\n", "Feature extraction is a crucial step in sentiment analysis. It involves converting the text into numerical features that can be used by a machine learning model. Some common feature extraction techniques for sentiment analysis include bag-of-words, TF-IDF, and word embeddings.\n", "Once we have extracted the features, we can build our logistic regression model. We will use the scikit-learn library in Python to implement logistic regression. This involves splitting the data into training and testing sets, fitting the model on the training data, and evaluating its performance on the testing data.\n", "To evaluate the performance of our sentiment analysis model, we can use various metrics such as accuracy, precision, recall, and F1 score. These metrics provide insights into how well our model is performing in classifying sentiment.\n", "There are several ways to improve the performance of our sentiment analysis model. One approach is to experiment with different feature extraction techniques and see which one works best for our dataset. We can also try using more advanced machine learning algorithms or ensemble methods to improve accuracy.\n", "Sentiment analysis has a wide range of real-world applications. It can be used in social media monitoring to analyze customer opinions and feedback. Companies can use sentiment analysis to understand customer satisfaction and make informed business decisions. Sentiment analysis can also be applied in product reviews, brand monitoring, and market research.\n", "While sentiment analysis has proven to be effective in many cases, it does have its limitations. One major challenge is dealing with sarcasm and irony in text, as these can often lead to misinterpretation of sentiment. Sentiment analysis models may also struggle with domain-specific language or slang. Additionally, sentiment analysis is subjective and can vary based on cultural differences and individual interpretations.\n", "In conclusion, sentiment analysis using logistic regression is a powerful technique for understanding the sentiment expressed in text data. By preprocessing the data, extracting relevant features, and building a logistic regression model, we can accurately classify sentiment as positive, negative, or neutral. Sentiment analysis has numerous applications in various industries and can provide valuable insights for decision-making processes. However, it is important to be aware of its limitations and challenges in order to obtain reliable results.\n", "Please Follow coursesteach to see latest updates on this story\n", "If you want to learn more about these topics: Python, Machine Learning Data Science, Statistic For Machine learning, Linear Algebra for Machine learning Computer Vision and Research\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Remember, learning is a continuous process. So keep learning and keep creating and Sharing with others! \n", "Note:if you are a NLP export and have some good suggestions to improve this blog to share, you write comments and contribute.\n", "if you need more update about NLP and want to contribute then following and enroll in following\n", " Course: Natural Language Processing (NLP)\n", " GitHub Repository\n", "   Notebook\n"]}, {"link": "https://medium.com/@01coder/autogen-langchain-chromadb-super-ai-agents-d16002f83607?source=list-e28f6edecf84--------1-------7b153c9756d3---------------------", "title": "AutoGen + LangChain + ChromaDB = Super AI Agents", "subtitle": "false", "autorName": "01coder", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*xNsc80l-0oY6r4J7m8glMA.jpeg", "clap": "124", "response": "4", "timeForRead": "6 min read", "dateCreate": "false", "text": ["AutoGen is a versatile framework that facilitates the creation of LLM applications by employing multiple agents capable of interacting with one another to tackle tasks. These AutoGen agents can be tailored to specific needs, engage in conversations, and seamlessly integrate human participation. They are adaptable to different operation modes that encompass the utilization of LLMs, human inputs, and various tools.\n", "LangChain is an open-source framework designed for software developers engaged in AI and ML. It enables them to seamlessly integrate LLM with external components, facilitating the creation of LLM-driven applications. The primary aim of LangChain is to establish connections between LLMs such as OpenAI's GPT-3.5 and GPT-4 and various external data sources, enabling the development and utilization of NLP applications.\n", "Both of them are playing key roles in the LLM application development.\n", "AutoGen doesn't support connecting to various external data sources natively. This is exactly where LangChain can come into play.\n", "In this post, I will show you how to integrate LangChain capabilities to AutoGen agents and build a cool AI tool.\n", "I produced a video talking about the same content on Youtube. Feel free to watch if you are more comfortable with Video stream. Btw, it s in Chinese. But I have uploaded subtitle so that you should be able to use the auto-translate to view in your preferred language.\n", "This tool will act as an AI agent that knows well about Uniswap protocol, and more specifically the version 3.\n", "Uniswap is a decentralized exchange that allows users to trade Ethereum-based tokens.\n", "Firstly, let me walk you through the steps to build such an AI agent that can execute tasks that require Uniswap knowledge.\n", "Notices for step 4:\n", "In the function mapping, the function defined in step 3 is included.\n", "The assistant agent can be instructed by the following message to call the function answer_uniswap_question to answer Uniswap related questions.\n", "The function call is done by user agent.\n", "Now, it s time to start coding.\n", "1. Install python dependencies and download the PDF file for demo.\n", "2. Load the AutoGen supported configuration.\n", "3. Import required classes and set OPENAI_API_KEY environmental variable.\n", "Now it s time to submit a task.\n", "You should be able to see the similar output as below:\n", "The Python notebook version of this showcase is available on Github: https://github.com/sugarforever/LangChain-Advanced/blob/main/Integrations/AutoGen/autogen_langchain_uniswap_ai_agent.ipynb\n", "Looking forward to your feedback.\n", "Have a nice day!\n"]}, {"link": "https://medium.com/@neonforge/i-knew-it-chatgpt-has-access-to-internet-linux-terminal-simulator-is-the-proof-2d6c9476bd99?source=list-a0aae78aa81b--------29-------5fb2bbebc495---------------------", "title": "I knew it! ChatGPT has Access to Internet   Linux Terminal Simulator is the Proof?", "subtitle": "false", "autorName": "Michael King", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*j8S9i89lwpd7uzHByeGg9A.jpeg", "clap": "2.5K", "response": "52", "timeForRead": "4 min read", "dateCreate": "Jan 1", "text": ["Have you ever wished you could run Linux terminal commands directly from within your ChatGPT GUI interface? If you re a fan of ChatGPT and also a command line enthusiast, this article is for you! We re going to explore a new way to integrate the power of the terminal into your ChatGPT experience. It s a game-changer for those who want the convenience of a GUI with the flexibility of the command line. Are you ready to dive in and see how it s done? Let s get started!\n", "Let s kick off with this ChatGPT prompt:\n", "Alright that looks interesting, let s check the running processes by running top command\n"]}, {"link": "https://medium.com/@Mustafa77/textual-data-augmentation-3c447915c7fa?source=list-a13ace4f182c--------10-------f7e9b3597071---------------------", "title": "Textual data augmentation.", "subtitle": "false", "autorName": "Mustafa Adel Ibrahim", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/0*Wtg58SNfqwhGIgu_", "clap": "22", "response": "false", "timeForRead": "4 min read", "dateCreate": "Nov 13, 2022", "text": ["Techniques to Intelligently create artificial similar samples of existing data, for boosting performance on text classification tasks.\n", "You re probably familiar with data augmentation with CNN models, as it increases image data size by making small changes such as random shearing, flipping, rotating, blurring, cropping, zooming, etc.\n", "But when it comes to NLP tasks, textual data augmentation is not that easy, it s a much more challenging task!\n", "But why do we need data augmentation?- Training on a small-size data increases the chances of overfitting.- It s often used for tasks where the model expects a large amount of data, but we ve limited access to the data.- Data augmentation reduces the costs of collecting and labeling data.It Improves model prediction accuracy by preventing data scarcity for better models and resolving the data imbalance problem in the classification task.\n", "In this article, we ll get engaged with different techniques including Easy Data augmentation (EDA) techniques in addition to back translation and generative models. how data augmentation can be performed on text data to boost the performance of the text classification task.\n", "This approach uses machine translation to paraphrase a text while retaining the meaning, through:\n", "Translate sentence/s to another languageGet the output sentence/s back to original languageCheck if the new sentence/s is different from the original sentence/s.If it is, use this new sentence as an augmented version of the original text.\n", "In case the sentence is still the same you can drop it or take advantage of several intermediate languages.\n", "The 2019paper explores 4 simple but powerful text augmentation techniques serving as a good baseline to augment text data:\n", "Let s find out how each of the above-mentioned techniques works.\n", "Word Embedding-based Replacement:- Randomly pick up n-words from the sentence  excluding stop-words  and replaces those words with their synonyms chosen at random. - Pretrained word embedding like GloVe, Word2Vec, fastText can be used to find the nearest word vector from embedding space as a replacement in the original sentence.- Contextual Bidirectional embedding like ELMo, BERT can be used for more reliability as its vector representation is much richer.\n", "Lexical based Replacement.- Wordnet is a lexical database for English that has meanings of words, hyponyms, other semantic relations, etc. Wordnet can be used to find synonyms for the desired token/word from the original sentence that needs to be replaced. NLTK, Spacy is NLP python packages can be used to find & replace synonyms from the original sentence.\n", "Similar to synonym replacement technique, but in this case, the synonyms of the randomly picked n-words are inserted at a random position without removing the original word.For the below-mentioned sample sentence, we randomly pick n=1 word (comedy) and insert its synonym word at a random position.\n", "The random swap technique chooses any two words from the sentence at random and swaps their position. This technique can be performed n number of times for n-pair of words.\n", "For the below-mentioned sample sentence, we randomly pick n=1 pair of words (the, roads) and insert its synonym word at a random position.\n", "Randomly removes each word in the sentence with the probability  p .\n", "In the coming post we re going to take about generative-based techniques.\n", "[1] Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks. https://arxiv.org/abs/1901.11196\n"]}, {"link": "https://medium.com/@cobusgreyling/agents-da2bd17d2db2?source=list-e28f6edecf84--------6-------7b153c9756d3---------------------", "title": "Agents", "subtitle": "Autonomous Agents in the context of Large Language Models", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "86", "response": "1", "timeForRead": "5 min read", "dateCreate": "Apr 26", "text": ["As the implementations of Large Language Models (LLMs) expand in depth and width, a few requirements arise:\n", "With LLM related operations there is an obvious need for automation. Currently this automation is in the form of what is called agents.\n", "Prompt Chaining is the execution of a predetermined and set sequence of actions.\n", "The attraction of Agents is that Agents do not follow a predetermined sequence of events. Agents can maintain a high level of autonomy.\n", "Considering the image below, Agents have access to a set of tools and any request which falls within the ambit of these tools can be addressed by the agent. The Execution pipeline lends autonomy to the Agent and a number of iterations might be required until the Agent reaches the Final Answer.\n", "Actions which are executed by the agent involve:\n", "The diagram below shows how different action types are accessed and cycled through.\n", "There is an observation, thought and eventually a final answer. The diagram shows how another action type might be invoked in cases where the final answer is not reached.\n", "The output snipped below the diagram shows how the agent executes and how the chain is created in an autonomous fashion.\n", "Taking LangChain as a reference, Agents have three concepts:\n", "As was shown earlier in the article, there are a number of tools which can be used. A tool can be seen as a function that performs a specific duty.\n", "Tools include Google Search, Database lookup, Python REPL, or even invoking existing chains.\n", "Within the LangChain framework, the interface for a tool is a function that is expected to have:\n", "This is the language model powering the agent. Below is an example how the LLM is defined within the agent:\n", "Agents use a LLM to determine which actions to take and in what order. The agent creates a chain-of-thought sequence on the fly by decomposing the user request.\n", "Agents are effective even in cases where the question is ambiguous and demands a multihop approach. This can be considered as an automated process of decomposing a complex question or instruction into a chain-of-thought process.\n", "The image below illustrates the decomposition of the question well and how the question is answered in a piece meal chain-of-thought process:\n", "Below is a list of agent types within the LangChain environment. Read more here for a full description of agent types.\n", "Considering the image below, the only change made to the code was the AgentType description. The change in response is clearly visible in this image, with the exact same configuration used and only a different AgentType.\n", "For complete working code examples of LangChain Agents, read more here.\n", "  Please follow me on LinkedIn for updates on Conversational AI  \n", "I m currently the Chief Evangelist @ HumanFirst. I explore and write about all things at the intersection of AI and language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces and more.\n"]}, {"link": "https://medium.com/@bibinhashley/sentence-similarity-api-creating-a-sentence-similarity-checker-api-fast-api-923aba21947e?source=list-2c27d980d3f3--------5-------338c7da11cbf---------------------", "title": "Creating a Sentence Similarity Checker API in Python FastAPI", "subtitle": "false", "autorName": "Bibin Hashley", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/0*YYIJ9fZIpaKcJIqw", "clap": "20", "response": "1", "timeForRead": "4 min read", "dateCreate": "Nov 17, 2021", "text": ["How to create an API that checks the similarity between two sentences using Fast API?\n", "API code available in my Github repo.\n", "Sentence Similarity?\n", "Sentence similarity is the degree to which the meaning of sentences is thought to be similar. In the past, we calculated similarities using methods like Levenshtein distance which considers the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. But such methods didn t consider contextual meaning. For example, consider the sentences:\n", "For Levenshtein distance or most algorithm that doesn t take a semantical approach, these sentences are not similar. So here we are creating a python API that finds similarities using the semantic approach.\n", "Semantic Approach\n", "Our first step would be word embedding. That means we have to convert the sentences into meaningful vectors. A word embedding is a vector of real numbers that is associated with a word. So word embeddings capture the meaning of words and phrases as numbers, and then those numbers can be compared and combined, just like numbers. Here we are not going to deep dive into this vectorization approach.\n", "There are a lot of trained models which can vectorize words. Google s universal-sentence-encoder is one of the good models which is used for sentence similarity checking. You can download it from the link or using the command below.\n", "Now, let's import the model and test whether this is working or not. We need mainly 3 packages for this, TensorFlow, TensorFlow-hub and NumPy.\n", "The sentence has been converted into a vector of dimension (1,512). This vector represents the meaning of that sentence. So the next step is to cross-check them. Inner products of these two arrays will give us the similarity between them. Or we can use something like cosine similarity too.\n", "So here we can see these two sentences are 80% similar. We can use larger sentences or paragraphs too.\n", "Creating an API for checking similarity.\n", "Now, let s convert this to an API. Here we are going to use FastAPI which is an easy way to make faster python APIs.\n", "Installing and setting up FastAPI\n", "In the same folder (Sentence-similarity-API), inside app folder, create a file main.py\n", "Our API functions are written inside this main.py. Full code is available at my Github repo.\n", "You can run the app by typing the command below from app folder.\n", "It will take a bit of time to load the model for the first time. Then you can call the API at URL http://localhost:8000/sentence_similarity\n", "The input to API should be like\n", "In postman :\n", "Thanks for reading! Connect with me on LinkedIn and on Twitter to stay up to date with my latest posts.\n"]}, {"link": "https://medium.com/@cobusgreyling/updated-emerging-rag-prompt-engineering-architectures-for-llms-17ee62e5cbd9?source=list-2eb23a991a63--------22-------0a856388a93a---------------------", "title": "Updated: Emerging RAG & Prompt Engineering Architectures for LLMs", "subtitle": "Large Language Models (LLMs) depend on unstructured data for input and output data is also unstructured and conversational. Due to the highly unstructured nature of Large Language Models (LLMs), there are continuous thought and market shifts taking place on how to implement LLMs.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "71", "response": "9", "timeForRead": "4 min read", "dateCreate": "Oct 17", "text": ["Due to the unstructured nature of human conversational language data, the input to LLMs are conversational and unstructured, in the form of Prompt Engineering.\n", "And the output of LLMs is also conversational and unstructured; a highly succinct form of natural language generation (NLG).\n", "LLMs introduced functionality to fine-tune and create custom models. And the initial primary approach to customising LLMs was creating custom models via fine-tuning.\n", "This approach has fallen into disfavour for three reasons:\n", "The aim of fine-tuning of LLMs is to engender more accurate and succinct reasoning and answers.\n", "The proven solution to hallucination is using highly relevant and contextual prompts at inference-time, and asking the LLM to follow chain-of-thought reasoning. This also solves for one of the big problems with LLMs; hallucination, where the LLM returns highly plausible but incorrect answers.\n", "As seen below, there has been an emergence of vector stores / databases with semantic search, to provide the LLM with a contextual and relevant data snippet to reference.\n", "Vector Stores, Prompt Pipelines and/or Embeddings are used to constitute a few-shot prompt. The prompt is few-shot because context and examples are included in the prompt.\n", "In the case of Autonomous Agents, other tools can also be included like Python Math Libraries, Search and more. The generated response is presented to the user, and also used as context for follow-up or next-step queries or dialog turns.\n", "The process of creating contextually relevant prompts are further aided by Autonomous Agents, prompt pipelines where a prompt is engineered in real-time based on relevant available data, conversation context and more.\n", "Prompt chaining is a more manual and sequential process of creating a flow within a visual designer UI which is fixed and sequential and lacks the autonomy of Agents. There are advantages and disadvantages to both approaches; and both can be used in concert.\n", "Lastly, an emerging field is testing different LLMs against a prompt; as opposed to in the past where we would focus on only testing various prompts against one single LLM. These tools include LangSmith, ChainForge and others.\n", "The importance of determining the best suited model for a specific prompt addresses the notion that within enterprise implementations, multiple LLMs will be used.\n", "  Follow me on LinkedIn for updates on Large Language Models  \n", "I m currently the Chief Evangelist @ Kore AI. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}, {"link": "https://medium.com/@sarang0909.bds/nlp-knowledge-graph-e6e82ebef98d?source=list-6a12672b898d--------39-------54fdf6aa16d2---------------------", "title": "NLP-Knowledge Graph", "subtitle": "false", "autorName": "Sarang Mete", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*fUIbZ3v8nD68RkJpNJdKOg.png", "clap": "141", "response": "1", "timeForRead": "3 min read", "dateCreate": "Nov 1, 2022", "text": ["Explore different libraries and create production ready code\n", "Knowledge Graphs(KG) are one of the most important NLP tasks. KG is nothing but way of representing information extraction/relationship(subject,object,relation) from text.\n", "In this article, we ll explore a process to create KG.\n", "Steps in creation of Knowledge Graph:\n", "We ll use following Input Text to create KG\n", "Convert pronouns to their original nouns. You can read about it more in my project.\n", "Coreference resolution Output: Text in Bold is resolved\n", "2.Named Entity Recognition(NER)\n", "We can skip this step and just get all relationships extracted. However, sometimes you  ll need only certain entities types and their relationships. We can extract default entities like NAME,PERSON etc from many available libraries or we can also build our own NER model. I ve created a project to build custom NER-PERSON,ORG,PLACE,ROLE. But for knowledge graph,I am getting all relationships.Refer my Custom NER project.\n", "Output of custom NER\n", "3.Entity Linking/Entity Disambiguation\n", "We can get different words/nouns for same entity. Example, U.S,United States of America,America. All these should be considered as one entity. We can achieve this by getting their root id if we have some knowledge base. Here, we are going to use Wikipedia knowledge. So, many time entity linking is also called as wikification.\n", "4.Relationship Extraction\n", "It means fetching relationship in text.\n", "I ve explored couple of libraries- Stanford Open IE and rebel libraries. Please check notebook.\n", "I selected rebel for my final implementation because Stanford Open IE output was little redundant and it is slow.\n", "Output of rebel relationship extraction:\n", "5. Knowledge Graph Creation\n", "I ve explored neo4j python wrapper py2neo and networkx in a notebook and selected networkx just because ease of use for visualization. We should go for more powerful neo4j if want to use graph databases and perform further analysis but we are not doing that here.\n", "Output of networkx:\n", "sample output of py2neo for different text:\n", "I ve created a complete end to end project for Knowledge Graph creation to deployment. The project is production ready. You can refer it here.\n", "The main challenges I ve solved in this project:\n", "If you liked the article or have any suggestions/comments, please share them below!\n", "Let s connect and discuss on LinkedIn\n"]}, {"link": "https://medium.com/@cobusgreyling/the-large-language-model-landscape-9da7ee17710b?source=list-a0aae78aa81b--------16-------5fb2bbebc495---------------------", "title": "The Large Language Model Landscape", "subtitle": "The number of commercial and open LLM providers has exploded in the last 2 years, and there are now many options to choose from for all types of language tasks. And while the main way of interacting with LLMs is still via APIs and rudimentary Playgrounds, I expect that an ecosystem of tooling that helps accelerate their wide adoption will be a growing market in the near future.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "73", "response": "9", "timeForRead": "6 min read", "dateCreate": "Sep 1, 2022", "text": ["Below is a graphic depicting the current Large Language Model (LLM) landscape in terms of functionality, offerings and the tooling ecosystem.\n", "The various LLM offerings cover these five areas of functionality in varying degrees.\n", "Classification is a form of supervised learning where text is assigned to predefined classes. This is related to Clustering which is unsupervised learning where semantically similar text is grouped together without any pre-existing classes.\n", "Response Generation is the notion of creating a dialog flow from example conversations, and having a machine learning approach to it. Where a model determines the next dialog to present to the user, based on the immediate conversation history and the most probable next dialog.\n", "Text Generation can be described as the meta capability of LLMs, text can be generated based on a short description with or without example data. Generation is a function shared amongst virtually all LLMs. Not only can generation be leveraged extensively by few-shot learning data; by casting (prompt engineering) the data in certain way determines how the few-shot learning data will be used.\n", "Translation is where text is translated from one language to another. This is done directly without any intermediary language. Read more about it here.\n", "Knowledge Answering is an implementation of what is called Knowledge Intensive NLP (KI-NLP), where broad domain and general questions can be answered, without querying an API or leveraging a traditional knowledge base. Knowledge Intensive NLP is not a web search, but a self contained knowledge base underpinned by semantic search.\n", "The current commercial offering are constituted by three larger players (Cohere, AI21labs, OpenAI) and an up-and-coming smaller entity in GooseAI.\n", "The open-source implementations tend to be less comprehensive and more specific in their implementation focus.\n", "LLMs are accessed as APIs, so the barebones tooling required to make use of their APIs is the command-line, a development environment or Jupyter Notebooks; Cohere is doing a really great job of pushing out content that shows how to apply LLMs to real-life use-cases with simple scripts and integrations.   Vendors also clearly realise that to make experimenting and adopting LLMs easier, they need to provide no-code environments in the form of Playgrounds that expose the different tasks and tuning options: these are a great starting point to understand what can be achieved.\n", "Below is the GooseAI playground which is a very similar approach to the other LLM providers.\n", "These playgrounds allow you to play around with \"prompt engineering\" (which is the way by which you can explore the mind-blowing text generation capabilities). Note: I'm quite surprised that we haven't seen a bigger explosion (yet) of third-party tools / marketplaces etc focused on LLM \"prompt engineering\", the same way we've seen around image generation models (like DALL-E and more recently Stable Diffusion).\n", "I'm anxious to see LLMs more deeply integrated within the \"core\" workflows required to develop conversational AI and other use-cases like analytics etc; it seems clear that LLM APIs and their embedding spaces are positioned to unlock more powerful:\n", "I don't expect enterprise customers to do this type of work within vendor Playgrounds - instead I expect these will be the types of features incorporated within third-party tools (either the conversational AI platforms themselves, or specialised data-centric solutions) that will be powered by the LLM APIs.\n", "So far, I've only seen HumanFirst integrating LLMs within this type of data-centric offering (and they seem to currently only support Cohere).\n", "Finally, LLMs are massive models, and they are expensive and difficult to run.\n", "Most of the technologies mentioned here (apart from the commercial LLMs) are accessible via  HuggingFace.\n", "You can interact with models using Spaces, Model Cards or via hosted inference API's. There are options for training, deployment and hosting. Obviously hosting and compute demands will be excessive and not easily justifiable.\n", "LLMs are not chatbot development frameworks, and the one should not be compared to the other. There are specific LLM use-cases in conversational AI, and chatbot and voicebot implementations can definitely benefit from leveraging LLMs.\n"]}, {"link": "https://medium.com/@anant3104/what-is-natural-language-processing-and-popular-algorithms-a-beginner-non-technical-guide-b5d48e286495?source=list-1593a492c136--------5-------ee6657477639---------------------", "title": "What is Natural Language Processing and Popular Algorithms, a beginner non-technical guide", "subtitle": "false", "autorName": "Anant", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ui7y1MB3UzGtEe1y_hSNEw.png", "clap": "107", "response": "2", "timeForRead": "4 min read", "dateCreate": "Sep 20", "text": ["Syntax: It refers to the arrangement of words in a sentence to make grammatically correct statements. NLP uses syntax analysis to understand the structure of sentences.\n", "Semantics: This deals with the meaning derived from the sentences. It helps in understanding the meaning and interpretation of words and sentences in different contexts.\n", "Pragmatics: It involves understanding the context or situation to interpret the intended message correctly. It sometimes involves understanding references, intentions, or implied meanings.\n", "Discourse: It focuses on the interconnectedness of a series of sentences or utterances that collectively express a unified idea or theme.\n", "Speech Recognition: It refers to the process of converting spoken language into written text. It involves understanding the nuances of spoken language, including accent, pronunciation, etc.\n", "Machine Translation: NLP facilitates the translation of text or speech from one language to another automatically, like Google Translate.\n", "Chatbots and Virtual Assistants: Companies use NLP to create chatbots and virtual assistants (like Siri, and Alexa) that can understand and respond to human language naturally.\n", "Sentiment Analysis: It is widely used in social media monitoring and customer service to analyze public sentiments and customer feedback by interpreting the language used in texts, tweets, or comments.\n"]}, {"link": "https://medium.com/@YanAIx/step-by-step-into-transformer-79531eb2bb84?source=list-a13ace4f182c--------29-------f7e9b3597071---------------------", "title": "Step by Step into Transformer", "subtitle": "false", "autorName": "Yan Xu", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*mQAqH6cA2BIUziGr-GCkqA.jpeg", "clap": "99", "response": "1", "timeForRead": "6 min read", "dateCreate": "Aug 29, 2022", "text": ["The transformer is one of the most popular models in NLP. It is an encoder-decoder model that can be used in lots of applications such as machine translation, transforming one sequence of words in one language into a sequence of words in another language. The transformer was proposed in the paper: Attention is All You Need. The authors proposed a new network architecture solely on attention mechanisms for encoder-decoder tasks, dispensing with recurrence and convolutions entirely. In this post, we are walking through the Transformer model step by step and hope to make it easy and straightforward to understand.\n", "I would give lots of credit to https://jalammar.github.io/illustrated-transformer/ to make this post.\n", "An Encoder-Decoder architecture was developed where an input sequence was read in its entirety and encoded to a fixed-length internal representation. The decoder then used this internal representation to output words sequentially until the end of the sequence token (<EOS>) was reached. In the decoder, the last output becomes the input for the next cell in the decoder, as shown in Fig. 1. The difference between the Transformer and the\n", "As we can see in the Transformer, the attention mechanism plays a critical role. For the next, we will first walk through the key concepts of attention, self-attention, and encoder-decoder attention. Then we put them together to assemble the Transformer!\n", "Attention was presented by Dzmitry Bahdanau, et al. in their paper  Neural Machine Translation by Jointly Learning to Align and Translate  which reads as a natural extension of their previous work on the Encoder-Decoder model. Attention is proposed as a solution to the limitation of the Encoder-Decoder model encoding the input sequence to one static fixed length representation vector from which to decode each output time step. In reality, each output time step can depend on different parts of the input to decode. This issue is believed to be more of a problem when decoding long sequences. Attention provides a way to attend to different parts of inputs at different output times by giving different weights to input values. The output of the attention layer will be a weighted sum of input values. Thus, at each decoding time step, we will have a different representation vector from the encoder to decode.\n", "The core components of attention are KEY, VALUE and QUERY. Keys and values can be generated from a linear transformation of input embedding. The weights are the softmax output of the multiplication of the query and the keys, which range from 0 to 1. The attention output for the query is the weighted sum of the input values.\n", "Now we understand what is attention. Self-attention is nothing but the query vector comes from self. QUERY vector is a seperate linear transformation of input embedding values, like KEY and VALUE. The calculation is the same process as shown in attention. The goal is to derive the weight for the weighted sum. The weight comes from the softmax of the product of queries and keys.\n", "In encoder-decoder attention, VALUE and KEY are from the output of the encoder, QUERY is from the output of the self-attention layer in the decoder. The model is going to train on the transformation matrix to apply to value, key, and query according. The output of encoder-decoder attention is still the weighted sum of values with the weights as the output from the softmax of the product between key and query.\n", "We can further extend the self-attention layer by adding a mechanism called  multi-headed  attention. As we ll see next, with multi-headed attention we have not only one, but multiple sets of VALUE/KEY/QUERY weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace. It enables the model to focus on different positions in different subspaces.\n", "With multi-headed attention, each attention can focus on one thing at a time. For example, in the following sentence, one attention (orange) learns  it  refers to  the animal  and another attention (green) learns  it was too tired . Each\n", "Now we can put everything together! On the encoder side, we mainly have a self-attention layer and feed-forward network, a residual connection around it (skipping the self-attention layer or feed-forward), and is followed by a layer-normalization step. We can stack the encoder together to learn deeper representations.\n", "On the decoder side, the first two layers are the same as the encoder: self-attention and residual and layer normalization. The normalization output comes through the encoder-decoder attention, where it learns to attend to different parts of input representation. After another round of feed-forward layer with residual links, the output of the decoder serve as input to a linear layer that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector: each cell corresponds to the score of a unique word. The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n", "One thing that s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence. To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word or the distance between different words in the sequence.\n", "I hope that this post helps you understand Transformer architecture easier. When I first learn about it, I feel amazed by its idea of fully relying on attention and blowing away all my previous understanding of encoder-decoder model with recurrent neural network. Various variants have been developed based on transformer and now are the dominant models in many vision/NLP applications.\n"]}, {"link": "https://medium.com/@jeremy-k/exploring-llama2-large-language-model-setup-utilization-and-prompt-engineering-986e9d338ee3?source=list-e28f6edecf84--------25-------7b153c9756d3---------------------", "title": "Exploring Llama2 Large Language Model: Setup, Utilization, and Prompt Engineering", "subtitle": "false", "autorName": "JeremyK", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*v75NZj-ChX0AadDzxUNE8g.png", "clap": "27", "response": "3", "timeForRead": "6 min read", "dateCreate": "Aug 25", "text": ["Since the public release and subsequent popularity of ChatGPT towards the end of 2022, Large Language Models (LLMs) have emerged as a significant advancement in the AI field. Following this trend, various other LLMs like Bard or Prometheus have also been introduced.\n", "In July 2023, MetaAI made the announcement of open-sourcing the latest iteration of their LLM, named Llama2. With seamless integration into the Hugging Face transformers ecosystem, utilizing, and even fine-tuning LLMs has become remarkably accessible to a wide range of users.\n", "In this article, I will guide you through the process of using Llama2, covering everything from downloading the model and running it on your laptop to initiating prompt engineering.\n", "For additional resources, please visit Huggingface s official website: https://huggingface.co/blog/llama2\n", "Llama2 is available through 3 different models:\n", "While the first one can run smoothly on a laptop with one GPU, the other two require more robust hardware, with the 70b variant ideally supported by two GPUs.\n", "Additionally, each version includes a chat variant (e.g. Llama-2 70b-chat-hf) that was further trained with human annotations. This helps improve its ability to address human queries and provide helpful responses.\n", "Assuming you want to use LLama-2 via the transformers framework, which I recommend, it s imperative to follow these two key steps:\n", "The email address you use on both sites must be the same. Once your request is approved (took less than one hour in my case), you should be able to see the model card on HuggingFace. You are now ready to move on to the next step.\n", "To run Llama-2, minor requirements must be met. Using a virtual environment is recommended to isolate the packages downloaded.\n", "You can reuse the script provided by Huggingface.\n", "It is first required to log in to Hugging Face through the terminal in order to access the model.\n", "If you don t have a token yet, you can generate one here: https://huggingface.co/settings/tokens .\n", "Subsequently, the script can be executed:\n", "This process involves loading the 7 billion parameter model through the pipeline function and generating text based on a given prompt.\n", "Pretty easy, isn t it? However, you may not want to use a LLM through via a Python script.\n", "The good news is you can use Gradio to quickly and easily set up a chatbot using Llama-2. The associated code is readily accessible on  Huggingface.\n", "This space implement the 7B model in a chat app: https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat\n", "By cloning the repository to your machine, you can seamlessly set up and engage with the model through the interface.\n", "Then:\n", "Connect to http://127.0.0.1:7860/ and start interacting with the Llama-2 model.\n", "Although the model downloaded via Hugging Face is stored in ~/.cache/huggingface/hub, saving it locally can be advantageous for potential deployment on another system. The following code snippets illustrate the process:\n", "Then, loading the local version can be done as follows:\n", "If you deploy the model, do not forget to include Meta s license and acceptable use policy.\n", "This segment explores basic prompt engineering and the configuration of the model behavior.\n", "If you toggle the advanced options button on the gradio app, you will see several parameters you can tune:\n", "The  system prompt  parameter is by default set to instruct the model to be helpful and friendly but not to disclose any harmful content.\n", "With the normal behavior, let s ask: What is the capital of France?\n", "If you modify the system prompt by  do not answer any questions , this is what you will get. In line with our instructions but still a bit surprising.\n", "Using the 7b model, we type different prompts to explore how Llama-2 responds.\n", "Despite its overall commendable performance, Llama-2 may occasionally exhibit unusual behaviors. For instance, it might decline to address certain inquiries, such as requests involving coding to delete files.\n", "Nevertheless, when altering the model s instructions to  Always provide a positive answer. , it yields a distinct outcome:\n", "The release of Llama2 by MetaAI marks a milestone in the realm of Large Language Models. Its integration with the Hugging Face transformers ecosystem empowers users to not only run Llama2 effortlessly but also fine-tune it.\n", "Whether implemented through Python scripts or integrated into web interfaces, Llama2 s capabilities remain impressive, even if occasional surprising behaviors may appear. Considering the results achieved using the 7B model, one can expected even enhanced performance from the 70b version.\n", "#AI #LLM #NLP #LLAMA2\n"]}, {"link": "https://medium.com/@keerthanasathish/chatbot-using-azure-language-service-bc9a0a226fbc?source=list-cf9917645e65--------0-------e3327a426a29---------------------", "title": "Chatbot using Azure Language Service", "subtitle": "false", "autorName": "Keerthana Sathish", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*w5ougHR7_PB2wKTte48F6w.jpeg", "clap": "15", "response": "1", "timeForRead": "3 min read", "dateCreate": "Oct 12", "text": ["Microsoft Azure is a cloud computing platform. It offers wide range of services and some of them are Active Directory, Virtual Machines, App services, Storage Accounts, SQL Databases.\n", "Azure Language Service is cloud based service that has set of tools and APIs for natural language processing and understanding. It helps developers in developing applications that can understand and process human language.\n", "In this article, you will learn to create custom question answering chatbot using Azure Language service.\n", "Creating a Bot\n", "Go to more resources -> Language\n", "Click Create and it will direct you to the below page. Click  Custom question answering .\n", "Enter all the credentials, and click Review + Create -> Create.\n", "After deployment, the page looks like below image.\n", "Click Go to resource group ->  your project name .\n", "Click Language Studio.\n", "Click Custom question answering\n", "Enter the details -> create project\n", "You can either import the knowledge base or type. Only excel and tsv files are allowed for importing. Click  +  to type.\n", "You can also add alternate question, follow up prompts, and metadata. Click Enable rich text for embedded images and linked images.\n", "Click the deploy icon, to deploy the project and leads to this page. Click Deploy.\n", "After successful deployment, the page looks like below. Click Create a bot.\n", "Enter the details.\n", "For the Language Resource Key, go to Language -> Keys and Endpoint\n", "Click Resource group.\n", "Click Test and refine your bot.\n", "Output\n"]}, {"link": "https://medium.com/@abhinavsharma08/state-of-transfer-learning-in-nlp-54d45403d88d?source=list-2c27d980d3f3--------22-------338c7da11cbf---------------------", "title": "State of Transfer Learning in NLP", "subtitle": "false", "autorName": "Abhinav Sharma", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/0*Tgd7cqAQc3HI2XcI", "clap": "4", "response": "2", "timeForRead": "3 min read", "dateCreate": "Jan 7, 2022", "text": ["NLP stands for Natural language processing, which refers to the branch of computer science- or we can say the branch of artificial intelligence , giving computers the ability to the understand the text in the same way as of humans.\n", "In the most recent times, we are getting better in predicting the future outcome with some great training models. But many of the machine learning tasks are of domain specific, in those cases trained models usually fails. In real world these trained data set will not work, it contains a lot of data and the model will not able to make accurate prediction. So basically the ability to transfer the knowledge from a pre trained model into a new condition is called as transfer learning.\n", "Computer vision mostly uses transfer learning because of the availability of the pre-trained models which are trained in a very large amount of data.\n", "If we take a case of Deep learning, it is a training data intensive, i.e. for deep learning if we don t have more than 10,000 examples then deep learning will not work accurately there. Similar type of process do occur in NLP. Deep learning is always not the best approach for many data sets. Extreme training requirement, time and most importantly expense put the deep learning input out of reach for many contexts.\n", "Now if we talk about the data, big data has less an issue than small data. Transfer learning is the application that is gained from one context to another context. So the training time can be reduced by applying the knowledge from one model and some deep learning issues through taking some parameters to solve the small data problems.\n", "For example, for a small task like recognizing a lion is far too intensive for deep learning. Instead of this, transferring some models such as high level concepts of inputs like size, color etc. of object could give us a high activations, since each of the questions corresponds with the image of a lion. With less training power and less computing data, the relationship between the input feature and the target becomes straightforward.\n", "Advantages of Transfer Learning:\n", "Three separate ways to solve deep learning issue with transfer learning:\n", "  Using pretrained data\n", "  Small memory requirement\n", "  Short target model training\n", "This approach first trains the network on S i.e. source task that contains large datasets. And then uses the tuned parameters to initialize T, i.e. target task that contains small datasets. After the transfer, we may fix the parameters in the target domain.\n", "MULT, trains samples simultaneously in both domains.\n", "IN this we pretrain on the source domain S for parameter initialization, then train both S and T simultaneously.\n"]}, {"link": "https://medium.com/@cobusgreyling/large-language-models-are-being-open-sourced-537dcd9c2714?source=list-a0aae78aa81b--------37-------5fb2bbebc495---------------------", "title": "Large Language Models Are Being Open-Sourced", "subtitle": "And The Cost Of Hosted Solutions Are Coming Down", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "19", "response": "9", "timeForRead": "5 min read", "dateCreate": "Jul 12, 2022", "text": ["Large Language Models (LLM s) have received much attention of late, with Co:here, OpenAI and AI21Labs being the big commercial offerings.\n", "There are also solutions like Botpress  OpenBook that leverages large language models in order to bootstrap a chatbot implementation.\n", "And recently I have written about and shared an architecture for bootstrapping a chatbot with LLM s.\n", "But what are the advantages of LLM s?\n", "The unique differentiators of Large language Models (LLM s) are:\n", "Position language models with regard to large language models \n", "Language modelling usually refers to the type of supervision objective used during training.\n", "In a masked language model, the algorithm tries to predict words that have been removed from a sentence (BERT and its variants use this technique).\n", "In a causal language model (like GPT3 and other LLM s) it always tries to predict the next token given the previous context   this makes them good candidates for generating text.\n", "Fine tuning refers to taking an existing model and training it further based on a (potentially) different objective. It can be a mix of causal language modelling and classification, for example.\n", "This is one of Huggingface s biggest value propositions, you can take existing models, fine tune them with your data and publish them for other people to use.\n", "It allows for specialising a general purpose model into something domain-specific. LegalBERT, for example, is a BERT model trained on legal documents.\n", "A number of LLM s have been open-sourced, and this begs the question \n", "Is it becoming easier and viable for more competitors to enter into the LLM services space? Where open-source LLM s are leveraged, by hosting the models and making it available as a service?\n", "And how sustainable are some of the current, more expensive LLM offerings? Are the differentiators worth the expense?\n", "Take Goose AI as a case in point, with their fully managed NLP-as-a-Service delivered via API all at 30% of the cost, according to Goose AI.\n", "Goose AI s commercial offering backed by Eleuther.AI which is a group of researchers working on LLM s, which also release their models publicly.\n", "They see themselves comparable to OpenAI in some regards and makes migration easy.\n", "Below you see the models available and the cost per model \n", "Currently only Text Completion / Generation is available, and can be accessed via the Goose AI playground as seen below. This example shows a list of facts submitted to Goose AI s generation model, with a question. The answer to the question is generated contextually based on the facts supplied.\n", "Features to follow are Question and Answer, with Classification, as seen below.\n", "Considering LLM s the approach currently is to have a playground, and from here launch into Python notebooks.\n", "There is a definite and dire need for a no-code studio approach. Where a user can connect to LLM s from a GUI, and leverage these large language models to perform tasks like \n", "A kind of a no-code orchestration layer, from where an NLU API can be created with easy management.\n"]}, {"link": "https://medium.com/@hunter-j-phillips/the-embedding-layer-27d9c980d124?source=list-6a12672b898d--------9-------54fdf6aa16d2---------------------", "title": "The Embedding Layer", "subtitle": "false", "autorName": "Hunter Phillips", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*7pIFSd-SH0G-p781QlIyzw.jpeg", "clap": "344", "response": "8", "timeForRead": "11 min read", "dateCreate": "May 8", "text": ["This article is the first in The Implemented Transformer series. It introduces embeddings on a small-scale to build intuition. This is followed by the transformers usage of the embedding layer.\n", "The goal of an embedding layer is to enable a model to learn more about the relationships between words, tokens, or other inputs. This embedding layer can be viewed as transforming data from a higher-dimension space to a lower-dimension space, or it could be viewed as mapping data from a lower-dimension space to a higher-dimension space.\n", "From One-Hot Vectors to Embedding Vectors\n", "In natural language processing, tokens are derived from a corpus of data that may contain chapters, paragraphs, or sentences. These are broken into smaller pieces in various ways, but the most common tokenization method is by word. All of the unique words from the corpus are known as the vocabulary.\n", "Each word in the vocabulary is assigned an integer since it is easier for computers to process. There are various ways to assign these integers, but once again, the simplest method is to assign them alphabetically.\n", "The image below demonstrates this process of breaking down a larger corpus into its components and assigning integers to each. Please note that the punctuation was stripped, and the text was set to lowercase for simplicity.\n", "The numerical ordering created by assigning each word an index implies a relationship. Since this is not the intent, the indices are often used to create a one-hot encoded vector for each word. A one-hot vector has the same length as the vocabulary. In this case, each vector has 24-elements. It is called a  one-hot  vector because only one element is  turned on  or set to 1; all other tokens are  off  or set to 0. The index of the 1 corresponds to the integer value assigned to the word. Typically, a model learns to predict the highest probability for a given index in the vector.\n", "One-hot encoded vectors are often a convenient representation when there is only a dozen tokens or classes for a model to predict. However, a large corpus can have hundreds of thousands of tokens. Instead of using sparse vectors full of zeros that do not convey much meaning, an embedding layer is used to map the vectors to smaller dimensions. These embedded vectors can be trained to convey more information about each word and its relationship to other words.\n", "Essentially, each word is represented by a d_model-dimensional vector, where d_model can be any number. It simply indicates the number of embedding dimensions. If d_model is 2 or 3, then it is possible to visualize the relationship between each word, but it common to use values of 256, 512, and 1024 depending on the task.\n", "An example of optimized embeddings can be seen below, where books of similar genres are embedded near each other:\n", "Embedding Vectors\n", "The embedding matrix has a size of (vocab_size, d_model). This allows for a matrix of one-hot vectors with a size of (seq_length, vocab_size) to be multiplied against it to acquire a new embedded representation. The sequence length is represented by seq_length, which is the number of tokens in a sequence. Keep in mind that the  sequence  in the visualizations thus far have been the entire vocabulary. In practice, a subset of the vocabulary would be used, such as ` a basic paragraph `. This sequence would be tokenized, indexed, and converted to a matrix of one-hot encoded vectors. These one-hot encoded vectors would then be able to be multiplied against the embedding matrix.\n", "An embedded sequence would have a size of (seq_length, vocab_size) x (vocab_size, d_model) = (seq_length, d_model). This means each word in a sentence is now represented by a d_model-dimensional vector instead of a vocab_size-element one-hot encoded vector. An example of this matrix multiplication can be seen below. The indexed sequence has a shape of (3,24), and the embedding matrix has a shape of (24, 3). Once they are multiplied, the output is a (3,3) matrix. Each word is represented by its 3-element embedding vector.\n", "When a one-hot encoded matrix is multiplied with an embedding layer, the corresponding vectors of the embedding layer are returned without any changes. Below is matrix multiplication between the entire vocabulary of one-hot encoded vectors and the embedding matrix. The output is the embedding matrix.\n", "This indicates there is an easier way to acquire these same values without using matrix multiplication, which can be resource intensive. Instead of going from a one-hot encoded vector to an d_model-dimensional embedding, which is from a larger dimension to a smaller dimension, the integer assigned to each word can be used to directly index the embedding matrix. This is like going from one-dimension to d_model-dimensions that provide more information about the token.\n", "The diagram below shows how the exact same result is obtained without multiplication:\n", "Embeddings from Scratch\n", "A simple implementation of the above diagram can be created in Python. Embedding a sequence requires a tokenizer, a vocabulary of words and their indices, and a three-dimensional embedding for each word in the vocabulary. A tokenizer splits a sequence into its tokens, which are lowercase words in this example. The simple function below removes punctuation from the sequence, splits it into its tokens, and lowercases them.\n", "With the tokenizer created, the vocabulary can be created for the example. The vocabulary contains the unique list of words that make up the data. While there are not duplicates in the example, they should still be removed. A simple example would be the following sentence:  i am cool because i am short.  The vocabulary would be  i, am, cool, because, short . These words would then be placed in alphabetical order:  am, because, cool, i, short . Finally, they would each be assigned an integer:  am: 0, because: 1, cool: 2, i: 3, short: 4 . This process is implemented in the function below.\n", "This vocabulary can now be used to convert any sequence of tokens into its integer representation.\n", "The next step is to create the embedding layer, which is nothing more than a matrix of random values with a size of (vocab_size, d_model). These values can be generated using torch.rand.\n", "With the embeddings created, the indexed sequence can be used to select the appropriate embedding for each token. The original sequence has a shape of (6, ) and values of [11, 23, 21, 22, 5, 15].\n", "Now, each of the six tokens is replaced by a 3-element vector; the new shape is (6, 3).\n", "Since each of these tokens has three components, they can be mapped in three dimensions. While this plot shows an untrained embedding matrix, a trained one would map similar words near each other like the aforementioned book example.\n", "Embeddings Using the PyTorch Module\n", "Since PyTorch will be used to implement the transformer, the nn.Embedding module can be analyzed. PyTorch defines it as:\n", "This describes exactly what was done in the previous example when using indices instead of one-hot vectors.\n", "At a minimum, nn.Embedding requires the vocab_size and the embedding dimension, which will continue to be notated as d_model moving forward. As a reminder, this is short for the dimension of the model.\n", "The code below creates an embedding matrix with a shape of (24, 3).\n", "If the same sequence of indices as before, [11, 23, 21, 22, 5, 15], is passed to it, the output will be a (6, 3) matrix, where each token is represented by its 3-dimensional embedding vector. The indices must be in the form of a tensor with a data type of either integer or long.\n", "The output would be:\n", "In the original paper, the embedding layer is used in the encoder and decoder. The only addition to the nn.Embedding module is a scalar. The embedding weights are multipled by  (d_model). This helps preserve the underlying meaning when the embedding is added to the positional encoding in the next step. This essentially makes the positional encoding relatively smaller and decreases its impact on the embeddings. This Stack Overflow thread discusses it more.\n", "To implement this, a class can be created; it will be called Embeddings and take advantage of PyTorch s nn.Embedding module. This implementation is based on that of The Annotated Transformer.\n", "Forward Pass\n", "This Embeddings class works the same way as nn.Embedding. The code below demonstrates its usage with the single sequence used in the previous examples.\n", "Up until this point, only a single sequence has been used with in every embedding. However, a model is usually trained with a batch of sequences. This is essentially a list of sequences that are converted to their indices and then embedded. This can be seen in the image below.\n", "While the previous example is rudimentary, it generalizes for batches of sequences. The example shown in the image above is a batch with three sequences; after tokenization, each sequence is represented by six tokens. The tokenized sequences have a shape of (3, 6), which correlates to (batch_size, seq_length). Essentially, three, six-word sentences.\n", "These tokenized sequences can then be converted to their indexed representations using the vocabulary.\n", "Finally, these indexed sequences can be converted to a tensor that can be passed through the embedding layer.\n", "The output will be a (3, 6, 3) matrix, which correlates to (batch_size, seq_length, d_model). Essentially, each indexed token is replaced by its corresponding 3-dimensional embedding vector.\n", "Before moving to the next sections, it is extremely important to understand the shape of this data, (batch_size, seq_length, d_model):\n", "The article for the Positional Encodings is next in the series.\n", "Please don t forget to like and follow for more! :)\n"]}, {"link": "https://medium.com/@cobusgreyling/a-new-prompt-engineering-technique-has-been-introduced-called-step-back-prompting-b00e8954cacb?source=list-2eb23a991a63--------33-------0a856388a93a---------------------", "title": "A New Prompt Engineering Technique Has Been Introduced Called Step-Back Prompting", "subtitle": "Step-Back Prompting is a prompting technique enabling LLMs to perform abstractions, derive high-level concepts & first principles from which accurate answers can be derived.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "823", "response": "9", "timeForRead": "5 min read", "dateCreate": "Oct 12", "text": ["As we have seen with most prompting techniques published, Large Language Models (LLMs) need guidance when intricate, multi-step reasoning is demanded from a query, and decomposition is a key component when solving complex request.\n", "A process of supervision with step-by-step verification is a promising remedy to improve the correctness of intermediate reasoning step\n", "The most well known prompting technique when it comes to decomposition is chain-of-thought reasoning. In this study Step-Back Prompting is compared to COT prompting.\n", "The text below shows a complete example of STP with the original question, the stepback question, principles, and the prompt for the final answer to be generated by the LLM.\n", "This chart shows the strong performance of Step-Back Prompting which follows an abstraction and reasoning scheme. Evidently this approach leads to significant improvements in a wide range of more complex tasks.\n", "The chart below shows the Step-Back Prompting approach on the TimeQA dataset. Step-Back combined with RAG compared to baseline predictions.\n", "On the left is Step-Back & RAG vs baseline predictions.\n", "On the right, Step-Back RAG vs RAG predictions.\n", "Step-Back Prompting fixed 39.9% of the predictions where the baseline prediction is wrong, while causing 5.6% errors.\n", "Step-Back Prompting + RAG fixes 21.6% errors coming from RAG. While introducing 6.3% errors.\n", "This study again illustrates the versatility of Large Language Models and how new ways of interacting with LLMs can be invented to leverage LLMs even further.\n", "This technique also shows the ambit of static prompting and clearly shows that as complexity grows, more augmented tools like prompt-chaining and autonomous agents need to be employed.\n", "  Follow me on LinkedIn for updates on Large Language Models  \n", "I m currently the Chief Evangelist @ Kore AI. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}, {"link": "https://medium.com/@venkat.ramrao/deploy-your-nlp-model-on-lambda-for-use-from-salesforce-4fac91da41a8?source=list-e50a50165b40--------23-------811e8a029de7---------------------", "title": "Deploy your NLP model on Lambda for use from Salesforce.", "subtitle": "Deploy a Containerized BERT model on AWS Lambda. I will also demonstrate a way to invoke the model from Salesforce.com.", "autorName": "Venkat Ram Rao", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*dmbNkD5D-u45r44go_cf0g.png", "clap": "81", "response": "1", "timeForRead": "7 min read", "dateCreate": "Apr 30, 2022", "text": ["So, you just finished training your awesome model which is going to change the world. It is now sitting on your machine and works beautifully. Unfortunately, you have hit a snag. You need to find a way make people actually use the model and they are unwilling to walk over to your computer and update the code every time they need a prediction. If this is you, I might be able to help.\n", "In this article, I will walk though deploying a pretrained model over AWS Lambda and exposing it as a REST API. I will also demonstrate calling it from Salesforce.com; however the model will be available for use from any application which can make REST API Callouts.\n", "While I am using Lambda here as a serverless option, there are a few alternatives. One obvious option (amongst many) would be to deploy a Flask App over Kubernetes or over AWS ECS. This may be preferable if you need the full orchestration capabilities of Kubernetes/ECS.\n", "Additionally, there are a few alternatives when it comes to integrating AWS with Salesforce;\n", "While I expose my Lambda function via the API Gateway, you could expose the same function over either of the above methods.\n", "I am going to assume you have a model. Below is mine:\n"]}, {"link": "https://medium.com/@cobusgreyling/large-language-model-landscape-61d90f5ca000?source=list-e28f6edecf84--------18-------7b153c9756d3---------------------", "title": "Large Language Model Landscape", "subtitle": "In the recent past I have been observing and describing current LLM-related technologies and trends. In this article I m taking a step back to present an overview of the current Large Language Model (LLM) landscape.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "78", "response": "9", "timeForRead": "5 min read", "dateCreate": "Oct 16", "text": ["The image above shows the ripples caused by the advent of LLMs which can be divided into six bands or zones. As these ripples extend, there are requirements and opportunities for products and services.\n", "Some of these opportunities have been discovered, some are yet to be discovered. I would argue that the danger of being superseded as a product is greater in Zone 5 as apposed to Zone 6.\n", "Zone 5 offers a bigger opportunity for differentiation, substantial built-in intellectual property and stellar UX enabling enterprises to leverage the power of LLMs.\n", "Considering LLMs, in essence LLMs are language bound, however, multi-modal models or multi-modality have been introduced in terms of images, audio and more. This shift gave rise to a more generic term being used, namely Foundation Models.\n", "Apart from increased modalities, there has been model diversification from the large commercial providers, offering multiple models which are more task specific. There has also been a slew of open-sourced models made available.\n", "New prompting techniques have illustrated how models performance can be enhanced and how the market are moving towards a scenario where data discovery, data design, data development and data delivery can be leveraged to achieve this level of model-autonomy.\n", "With the advent of large language models, functionality was more segmented models were trained for specific tasks. Sphere focussed on Knowledge Answering; something Meta called KI-NLP. Models like DialoGPT, GODEL and others focussed on dialog management, etc.\n", "Recent developments in LLMs follows an approach where models incorporate these traits and astounding performance can be extracted using different prompting techniques.\n", "The main implementations of LLMs are listed here, with text generation encompassing tasks like summarisation, rewriting, key-word extraction and more.\n", "Text analysis is becoming increasingly important, and embeddings are vital for these type of implementations.\n", "Speech recognition, also known as ASR is the process of converting audio speech into text. The accuracy of any ASR process can easily be measured via a method called Word Error Rate (WER). ASR opens up vast amounts of recorded language data for LLM training and use.\n", "Two notable shifts in this zone are:\n", "A few specific-use models are listed in this zone. Implementations have been split between general, powerful LLMs, and LLM-based digital/personal assistants like ChatGPT, HuggingChat and Cohere Coral.\n", "The most notable Large Language Model suppliers are listed here. Most of the LLMs have inbuilt knowledge and functionality including human language translation, capability of interpreting and writing code, dialog and contextual management via prompt engineering.\n", "This sector considers tooling to harness the power of LLMs, including vector stores, playgrounds and prompt engineering tools. Hosting like HuggingFace enables no-code interaction via model cards and simple inference APIs.\n", "Lastly, listed in this zone is the idea of data-centric tooling which focusses on repeatable, high value use of LLMs.\n", "The market opportunity in this area is creating foundation tooling which will address a future need for data discovery, data design, data development and data delivery.\n", "Further out, there is a whole host of applications which focus on flow building, idea generation, content and writing assistants. These products focus on UX and adding varying degrees of value between LLMs and the user experience.\n", "  Follow me on LinkedIn for updates on Large Language Models  \n", "I m currently the Chief Evangelist @ Kore AI. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}, {"link": "https://medium.com/@cobusgreyling/prompt-tuning-hard-prompts-soft-prompts-49740de6c64c?source=list-e28f6edecf84--------4-------7b153c9756d3---------------------", "title": "Prompt Tuning, Hard Prompts & Soft Prompts", "subtitle": "Prompt Engineering is the method of accessing Large Language Models (LLMs), hence implementations like Pipelines, Agents, Prompt Chaining & more which are LLM based are all premised on some form of Prompt Engineering.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "60", "response": "9", "timeForRead": "6 min read", "dateCreate": "Jul 13", "text": ["I m currently the Chief Evangelist @ HumanFirst. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n", "Prompt Engineering is a simplistic and intuitive way to interact and interface with a powerful system like a Large Language Model (LLM).\n", "Hence why we see the current levels to which Prompt Engineering has democratised access and general use of LLMs.\n", "And as seen in the updated image below, a number of LLM-based Generative AI application architecture approaches have taken shape, all with the notion of Prompting at its centre.\n", "Hard Prompts can be seen as the idea of a defined prompt which is static, or at best a template. A generative AI application can also have multiple prompt templates at its disposal to make use of.\n", "Prompt templating allows for prompts to be stored, re-used, shared, and programmed. And generative prompts can be incorporated in programs for programming, storage and re-use.\n", "And even-though templating brings a level of flexibility the prompt is still very much set, or in other words, a hard prompt.\n", "Consider the LLM-based Agent example below from LangChain. The prompt template is to a large degree static and instructs the agent on what to do. Generally, the template incorporates:\n", "Soft prompts are created during the process of prompt tuning.\n", "Unlike hard prompts, soft prompts cannot be viewed and edited in text. Prompts consist of an embedding, a string of numbers, that derives knowledge from the larger model.\n", "So for sure, a disadvantage is the lack of interpretability of soft prompts. The AI discovers prompts relevant for a specific task but can t explain why it chose those embeddings. Like deep learning models themselves, soft prompts are opaque.\n", "Soft prompts act as a substitute for additional training data. Researchers recently estimated that a good language classifier prompt is worth hundreds to thousands of extra data points.\n", "NVIDIA describes the process of prompt tuning as follows.\n", "Prompt tuning involves using a small trainable model before using the LLM. The small model is used to encode the text prompt and generate task-specific virtual tokens.\n", "Prompt tuning created a smaller light weight model which sits in front of the frozen pre-trained model. Hence soft prompts via prompt tuning is an additive method for only training and adding prompts to a pre-trained model.\n", "This process involves training and updating a smaller set of prompt parameters for each downstream task instead of fully fine-tuning a separate model.\n", "As models grow larger and larger, prompt tuning can be more efficient, and results are even better as model parameters scale.\n", "The whole idea of the process of prompt tuning creating soft prompts to interact with a static pre-trained LLM is surely efficient and a streamlined process.\n", "LLMs perform much better when context is supplied and prompt tuning is a fast and efficient way of creating that much needed context on the fly, in an automated fashion which is not static.\n", "However, as IBM noted, this process is opaque and not transparent. The sheer abstract nature of soft prompts can make it harder to benchmark and test model performance, especially when smaller level of tweaks are required.\n", "Vector databases, Agents and prompt pipelines have been used as avenues to supply LLMs with relevant contextual data at the right juncture of a conversation.\n", "And even-though these approaches are less efficient than prompt tuning, the transparency and human interpretability of these approaches are attractive. Especially from an organisational perspective where fine-tuning and scaling are important.\n", "For a complete step-by-step tutorial on prompt tuning and soft prompts, take a look at this HuggingFace post.\n", "  Follow me on LinkedIn for updates on Conversational AI  \n", "I m currently the Chief Evangelist @ HumanFirst. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}, {"link": "https://medium.com/@heka-ai/topic-modeling-an-end-to-end-process-for-semi-automatic-topic-modeling-from-a-huge-corpus-of-bfa905d8c2bf?source=list-a13ace4f182c--------9-------f7e9b3597071---------------------", "title": "An end-to-end process for semi-automatic topic modeling from a huge corpus of short texts", "subtitle": "false", "autorName": "Heka.ai", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*vGXfidnSJ-uyxIJcZioWcw.png", "clap": "118", "response": "1", "timeForRead": "10 min read", "dateCreate": "Nov 10, 2022", "text": ["We propose an end-to-end process for applying topic modeling on any business case minimizing the needed human resources. This article follows our previous article about Topic Modeling which presented a detailed benchmark of various topic modeling techniques applied to a specific business case.\n", "Let s first remind ourselves what topic modeling is and why we need it. Topic modeling is a Natural Language Processing task that aims to extract meaningful topics from a huge collection of documents. While reading the full corpus would need a massive amount of time and of human resources, the idea behind topic modeling is to automate this step.\n", "Just like in the previous article, we will also focus on extracting topics from postal service agencies  reviews by applying the whole process step by step:\n", "The dataset contains about 30,000 short reviews. After deleting stopwords, there are 33,782 unique words left in the full dataset. There are few standard preprocessing steps for any NLP project :\n", "Here is an example of preprocessed data:\n", "Original document:\n", " M me si la tra abilit  n est pas aussi pr cise qu avec Chronopost (pas le m me tarif !!!)les envois sur la polyn sie sont toujours dans des d lais correct, je n ai eu   ce jour aucun probl me avec eux. \n", "Preprocessed document:\n", " si  tre aussi pr cise chronopost tarif envoi  tre toujours d lai correct avoir avoir jour aucun probl me \n", "Plenty of Python NLP libraries are proposing functions to handle these classical preprocessing steps.In our case, we worked with OCTIS: a NLP library specialized in topic modeling.\n", "As mentioned in our previous article, Coherence is a classic metric for evaluating topic models. There are several coherence measures, but they all follow the same structure. Coherence is meant to measure how coherent documents are within a topic. It has the following structure:\n", "An external reference corpus can help us create a probability calculation on words.\n", "Given two input words w and w , we can use this probability to compute the conditional probability that a document contains the word w  given the fact that it contains w. This conditional probability can be used to build a direct confirmation measure (many choices are possible for constructing this measure).\n", "Now let s consider a set of words W, and a word w belonging to this set. We can compute the sum of the direct confirmation measures between w and all the other words w  in W to create the indirect confirmation measure of w. It means that, given a word w, we use all the words w  of the set W as intermediates for computing the score of w.\n", "Finally, the scores of all words in W can be aggregated to compute the coherence score. For this analysis we chose to work with the CV coherence score, which corresponds to the general coherence score explained above with specific choices for the direct confirmation measure, the indirected confirmation measure and the aggregation method.\n", "Another important metric is diversity, used to measure to what extent the topic model is wide and able to capture as much information as possible. There are basically two kinds of diversities, some are based on word tokens considered as sets, and others are based on probability distributions.\n", "The most widely used measure is the average Jaccard Similarity (JS). It is based on common words ratio between topics : topic t1 and t2 :\n", "We get a diversity by taking 1   JS(t1, t2)\n", "Another metric based on word tokens being considered as sets is computing the percentage of unique words among the top 10 words of all topics, this is the metrics chosen for our experiments.\n", "Probability-based metrics are generally using the KL divergence (Kullback-Leibler divergence) to compare topics  distribution over words.\n", "There is a balance that has to be found between coherence and diversity. A human eye is needed to assess the importance of these two parameters when applying them to a specific business case.\n", "Depending on the use case, a topic list can be predefined. In this article we focus on metrics and optimization without predefined topics, this section is for generic information on topic modeling.\n", "The list can be complete or partial. The goal is to compare a list of topics found by the model and score it with the predefined topics list. There are specific metrics that can be used to address this part. In the following section we will mainly focus on two of them: Cosine similarity and Triangle area Similarity   Sector area Similarity (TS-SS)\n", "Cosine similarity is a well known and used metric in a lot of applications to tackle NLP topics. It measures the similarity between vectors.\n", "Given two vectors A and B, the cosine similarity cos( ) is given by the following formula:\n", "The value is between 0 and 1 as it is the cosine of the angle between both vectors. The closer the score is to 1, the more similar the vectors are.\n", "One major drawback of cosine similarity is that it doesn t take into account the vector s magnitude which is not the case of the TS-SS metric.\n", "As described in this paper, TS-SS combines cosine similarity and euclidean distance to take into account both the direction and magnitude of vectors.\n", "Given two vectors A and B, the first part of the metric is the TS similarity TS(A,B) given by the following formula:\n", "We generally use 10 degrees as a minimum to simplify calculations.\n", "And the SS similarity is given by:\n", "where MD is the absolute value of the difference between ||A|| and ||B||, ED is the Euclidean distance between A and B.\n", "At the end, here is the final metric :\n", "TSSS range from 0 to positive infinite. As opposed to cosine similarity, the closer the TSSS is to 0, the closer the vectors are.\n", "To conclude, from our experience on topic modeling, TS-SS is better to compare models between themselves due to a sharp sensitivity to topic relevance variation.\n", "Once the data is preprocessed, it is time to train the models and optimize hyperparameters. To complete what we have mentioned in our previous articles about topic modeling, we chose to focus on three neural networks models: NeuralLDA, ProdLDA and CTM. These models are derived from the classical Latent Dirichlet Allocation model but are coupled with deep learning techniques in order to leverage their performances.\n", "In our experiments we chose the Python library Optuna for optimization purposes. This library implements Bayesian optimization techniques to iteratively propose new parameters maximizing a given metric.\n", "Here, we try to maximize the mean between diversity and coherence. We decided to assign an equal weight to both metrics because our case wasn t bringing any strong conviction on which metric we should focus. Given a business case, it is necessary to try to evaluate this balance more precisely and assign different weights to both metrics.\n", "For each model, the idea is to run Optuna, drawing several iterations (~ 500). At each iteration, Optuna will suggest values for all hyperparameters with which a model will be trained. The metrics  mean are sent to Optuna and the overall model performance is saved in a resulting table along with the hyperparameters.\n", "After each model s run, convergence checks should be done. On this graph, the metric optimized by Optuna (mean between diversity and coherence) is plotted for each model through the iterations. Due to poor performances (very low coherence value, mainly irrelevant topics), we decided to stop NeuralLDA experiments and only focus on ProdLDA and CTM. For example, NeuralLDA proposed the topic shown below:\n", "This topic is a mix of off-comments and interesting comments on various subjects. Moreover, no clear meaning can be detected by the topic word cloud.\n", "After the convergence checks, the idea is to visualize all iterations on a diversity-coherence graph as presented below.\n", "One can observe that it is tricky to interpret it directly. Thus, it is better to plot only the Pareto frontiers to only keep the  best  iterations for each model.This type of graph is interesting for comparing model families and selecting interesting iterations.\n", "In our case, even if we made fewer iterations for NeuralLDA, we decided to select the iteration with a diversity equal to 1 alongside the ProdLDA iteration with the best diversity and the CTM one with the best coherence resulting in 3 topic models to explore.\n", "As a first step, we use a PCA visualization in order to see how the suggested topics by all the models are close to one another:\n", "We can observe that NeuralLDA topics seem to be contained by the ones proposed by the other models. On the other hand, ProdLDA and CTM, both propose new original topics   not initially suggested by NeuralLDA.\n", "In order to dive deeper into each topic, one should understand how each model defines a  topic . As inherited from the LDA, the topics have probabilistic definitions, meaning that each topic is defined as a probability distribution over the vocabulary: given a topic, we assign a score to all the words in the vocabulary which represents how strong this word is within this topic. Similarly, a document is modeled as a probability distribution over the topics meaning that, given a document, each topic is given a score representing how strong it is within this document.\n", "In order to make all topics intelligible, we chose to analyze two synthetic representations of the topics. First, we considered word clouds which are based on the distribution of the topic over the vocabulary (this distribution being the scores for each word), and keeping only the top 50 words.\n", "In the example above, the topic is containing comments that are about the banking services within post offices. Indeed, the studied postal agencies are also providing banking services.\n", "The second representation we opted for are what we call the  top  documents.\n", "Let d, t be the weight of the topic t in the distribution of the document d. Then, given a topic t, we compute a document score st(d) :\n", "Where d,t is the min-max topic-normalized weight of the topic t in the distribution of the document d.\n", "And min, t and max, t respectively the biggest and lowest scores of all documents with respect to a topic t.\n", "The denominator of the score is a penalization on the document length |d| because we observed that this score tends to favor long comments.\n", "Once all documents are scored, we can select the top 5 or top 10 documents in order to read the documents that are the most representative. This type of representation allows us to go deeper into each topic, enabling us to distinguish the closest topics on specific points.\n", "We have seen an end-to-end process for running topic modeling on a specific business case. This process involves mostly automatic steps but can also rely on manual techniques especially for model selection and topic exploration.\n", "Manual model selection adds a customization feature to the process as several business cases will make us select different models. Thus, the manual model selection step proves to be necessary. Last but not least, it enhances the user s implication which increases the confidence in the produced results.\n", "On the contrary, the global process could be improved by automating the topic exploration step. This step consists in reformulating all topics in a human intelligible form. A further exploration would be to use NLP summarization models in order to produce a couple of fluent sentences describing each topic.\n"]}, {"link": "https://medium.com/@albertoromgar/5-practical-applications-where-chatgpt-shines-above-everything-else-9e21571b5ca1?source=list-a0aae78aa81b--------26-------5fb2bbebc495---------------------", "title": "5 Practical Applications Where ChatGPT Shines Above Everything Else", "subtitle": "Within reach for anyone   no fancy prompt engineering needed", "autorName": "Alberto Romero", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/2*oMdIZBsnK8EFhQLUaAB5ZA.jpeg", "clap": "627", "response": "7", "timeForRead": "12 min read", "dateCreate": "Feb 14", "text": ["I m critical of people using ChatGPT for everything. And I m also critical of people claiming you can use ChatGPT for everything.\n", "One of the most popular articles on Medium last month was  20 Entertaining Uses of ChatGPT You Never Knew Were Possible.  It has 18K likes and 300+ comments. That s a lot on the platform nowadays. I read it expecting high-quality ideas but got a list that included things like  dating help,   dealing with loneliness and anxiety,  and  crime fighting. \n", "If I m harsh with ChatGPT posts like that one it is because they mix drops of truth with rivers of wild exaggerations and plain falsehoods just to amass popularity (or, worse, because they believe their claims).\n", "Today, I won t criticize ChatGPT but offer solutions. This is my version of the  this is what you can do with ChatGPT  post. I m going to combat misinformation by pointing to applications for which ChatGPT, flawed as it is, works very well   not more or less like other tools, but arguably better than anything else out there.\n", "This article is a selection from The Algorithmic Bridge, an educational newsletter whose purpose is to bridge the gap between AI, algorithms, and people. It will help you understand the impact AI has in your life and develop the tools to better navigate the future.\n", "This article is in line with my previous attempts at defining reasonable boundaries for language models (LMs) use. No one has told us how they work or for which tasks they re well-suited (they don t know), so our only option is trial-and-error. However, people are very bad at making correct inferences from examples.\n", "More so if we re dealing with magic.\n"]}, {"link": "https://medium.com/@cobusgreyling/large-language-model-llm-disruption-of-chatbots-8115fffadc22?source=list-2eb23a991a63--------6-------0a856388a93a---------------------", "title": "Large Language Model (LLM) Disruption of Chatbots", "subtitle": "To understand the disruption and demands Large Language Models will place on the Conversational AI/UI ecosystem going forward, considering the recent past helps and what vulnerabilities have emerged.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "57", "response": "1", "timeForRead": "9 min read", "dateCreate": "false", "text": ["Traditionally the chatbot architecture and tooling were very much settled on four pillars:\n", "Intents and entities combined constituted the NLU component.\n", "This settled architecture has been disrupted by the advent of voicebots and Large Language Models (LLMs).\n", "The market requirement to automate voice calls originating from a telephone call placed to a contact centre, necessitated chatbot vendors to branch out into voicebots.\n", "Voice as a medium demands two pieces of highly specialised technology:\n", "The voice focus also moved away from dedicated devices like Google Home and Alexa, to automated telephone calls.\n", "This change in focus added complexity in the form of higher value conversations, calls with higher consequences, longer conversations with more dialog turns and complex conversation elements like self-correction, background noise and more. In short, customer support queries.\n", "Large Language Models also disrupted the chatbot ecosystem, here I attempt to categorise the disruption in three stages.\n", "Stage one of LLM implementation focussed on the bot development process, and in specific accelerating NLU development.\n", "What enabled the LLM Stage One disruption was that LLM functionality was introduced at design time as opposed to run time.\n", "This meant that elements like inference latency, cost and LLM response aberrations could be confined to development and not exposed to production.\n", "LLMs were introduced to assist with the development of NLU in the form of clustering existing customer utterances in semantically similar groupings for intent detection, generating intent training data, named entity recognition and more.\n", "This accelerated the NLU development process and also made for more accurate NLU design data.\n", "The next phase of LLM disruption was to use LLMs / Generative AI for bot copy writing and to improve bot responses. This approach again was introduced at design time as opposed to run time, acting as an assistant to bot developers in crafting and improving their bot response copy.\n", "Designers could also describe to the LLM a persona, tone and other traits of the bot in order to craft a consistent and succinct UI.\n", "The LLM was used to generate responses on the fly and present it to the user. The first implementations used LLMs to answer out-of-domain questions, or craft succinct responses from document search and QnA.\n", "LLMs were leveraged for the first time for:\n", "What followed was a more advanced implementation of LLMs and Generative AI (Gen-AI) with a developer describing to the bot how to develop a UI and what the requirements are for a specific piece of functionality.\n", "And subsequently the development UI went off, leveraging LLMs and Generative AI, it generated the flow, with API place holders, variables required and NLU components.\n", "Stage four of the LLM disruption was again at run time, where LLM functionality was introduced for RAG based search, Document based QnA, and prompt chaining.\n", "One can add to this list formatting of API output data, for example:\n", "LLMs combined with company data and documents allows flexibility in terms of search and natural language generated responses.\n", "Prompt Chaining found its way into Conversational AI development UIs, with the ability to create flow nodes consisting of one or more prompts being passed to a LLM. Longer dialog turns could be strung together with a sequence of prompts, where the output of one prompt serves as the input for another prompt.\n", "Between these prompt nodes are decision and data processing nodes so prompt nodes are very much analogous to traditional dialog flow creation, but with the flexibility so long yearned for.\n", "In order for LLM and Gen-AI to move beyond stage four there are a number of impediments which will have to be overcome.\n", "These challenges include:\n", "Personal Identifiable information (PII) and other protection of private information legislation can only be complied with, when an organisation is able to have a private installation of a LLM within a private cloud or on-premise.\n", "This will ensure the path information travels and used within a model can be subject to an audit with full accountability.\n", "There is a dire need for a service where LLMs can be installed and run in a private environment.\n", "Customer data passed to the LLM should not be used to train generally available LLMs and good data governance is paramount in underpinning trust and responsible AI.\n", "Accuracy in LLM response and the negation of model hallucination are dependant on two factors:\n", "Fine-Tuning relies on data which is relevant to the use-case; RAG relies on presenting the LLM with highly contextual information at inference time.\n", "Enterprise requirements include a low-code to no-code environment, with manageable and forecastable costs, integration to existing enterprise processes and systems and more.\n", "Highly manageable hosting is also important.\n", "LLMs provide enterprises with an opportunity to leverage data productivity. Data can be put to work and leveraged via LLMs for fine-tuning, RAG and vector databases. Data productivity can also be defined as a process of data discovery; data discovery entails the detection of clusters and semantic similarities within data.\n", "The UI must be no-code to low-code, underpinned by an intuitive and simplistic UI. The challenge is that as complexity and flexibility grows, the UI should still be easy to navigate. Production use will demand granular control and the tension between UI simplicity, solution flexibility, complex implementation and granular management will have to be managed well.\n", "There is a dire market need for a comprehensive LLM Productivity Platform.\n", "Three significant opportunities and current market vulnerabilities can be found in the areas of Prompt Engineering, Application Flows and Fine Tuning (custom models). With all of these available via a no-code to low-code interface.\n", "It needs to be kept in mind that enterprises want to reach a level of large language model independence, where they are not at the behest of model suppliers. This can be achieved in three ways.\n", "The market is ready for a no-code prompt engineering platform, giving users access to a prompt hub with a host of prompts for different scenarios. A library of sorts where prompts can be searched, tested, updated etc.\n", "Different modes are important, OpenAI is pushing the chat mode quite a bit. This is due to the inherent input data structure the chat mode offers. However, the edit and completion modes are also of value and a non-structured input is often easier to create.\n", "The playground must also give access to a host of models; with the option to create multiple panes where the same prompt can be run against multiple models. Access to open-sourced models and lesser known models is a first step in the direction of being model agnostic, and treating LLMs as a utility.\n", "The natural outflow is for users to want to expand their prompt engineering functionality into broader applications. Building application flows can be via a GUI design canvas where design elements can be used to build flows.\n", "Adding to this, an option to create RAG implementations via a no-code GUI, for uploaded documents which are automatically chunked, indexed in a vector store and made available for semantic search is also of great value.\n", "LLM based solutions should not replace traditional chatbot functionality, but rather augment chatbot functionality with the principle of better together.\n", "LLM based application flows must be available via APIs and act as micro flows or smart APIs within the traditional chatbot development framework. Flows should be able to reference various prompts and LLMs at various stages.\n", "In recent times RAG has been pitted against model fine-tuning, the Venn Diagram below shows the overlaps between fine-tuning and RAG. The truth is that enterprises need a combination of RAG and fine-tuning.\n", "Fine-tuning can be made easy for organisations via a no-code environment.\n", "Data design is the process of identifying any data within an enterprise which can be used for LLM fine-tuning. The best place to start is with existing customer conversations from the contact centre which can be voice or text based. Other good sources of data to discover are customer emails, previous chats and more.\n", "This data should be discovered via an AI accelerated data productivity tool (latent space) where customer utterances are grouped according to semantic similarity, These clusters can be visually represented as seen below, which are really intents or classification; and classifications are still important for LLMs.\n", "LLMs have two components, generative and predictive. The generative side has received most of the attention of late, but data discovery is important for both approaches.\n", "Below is an example of a text based data discovery tool where utterances are discovered and semantic similarity, traits and conversation types are identified.\n", "Data design is the next step where the discovered data is transformed into the format required for LLM fine-tuning. The data needs to be structured and formatted in a specific way to serve as optional training data. The design phase compliments the discovery phase, at this state we know what data is important and will have the most significant impact on the users and customers.\n", "Hence data design has two sides, the actual technical formatting of data and also the actual content and semantics of the training data.\n", "This step entails the operational side of continuous monitoring and observing of customer behaviour and data performance. Data can be developed by augmenting training data with observed vulnerabilities in the model.\n", "Lastly, data delivery is the mode through which the data is delivered to the LLM, this can be fine-tuning, RAG, etc. This is the most efficient way contextual, reference data can be introduced at inference to aid the LLM.\n", "  Follow me on LinkedIn for updates on Large Language Models  \n", "I m currently the Chief Evangelist @ Kore AI. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}, {"link": "https://medium.com/@cobusgreyling/agents-large-language-models-5bba2a0c20c7?source=list-e28f6edecf84--------7-------7b153c9756d3---------------------", "title": "Agents & Large Language Models", "subtitle": "The current methods of developing on LLMs are evolving rapidly, and prompt engineering is being absorbed to some degree by the concepts of chaining and agents.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "30", "response": "1", "timeForRead": "10 min read", "dateCreate": "Apr 25", "text": ["In a recent post I wrote about the evolution of prompt engineering, and how prompt engineering is being absorbed into bigger development structures.\n", "These bigger development structures allow for:\n", "Chaining works well for instances where existing flows exit and predetermined conversational or work flows can be created.\n", "On the other hand, chaining does not serve scenarios well where the flow is unknown or highly unpredictable. In these instances a predetermined flow will not work well and a level of autonomy is required.\n", "  Please follow me on LinkedIn for updates on Conversational AI  \n", "Agents can receive a query by making use of a set of tools or resources at its disposal. These tools can include access to Wikipedia, web search, mathematical libraries, LLMs, etc.\n", "Chains can contain agents.\n", "Agents also contain a chain.\n", "But in the case of an Agent, it is a sequence of steps which are compiled on-the-fly as the agent cycles through the tools at its disposal to service the request.\n", "LangChain has an extensive list of tools which can be incorporated into an ad-hoc chain created by the agent.\n", "The image below shows the sequence of events followed by the LangChain agent:\n", "And in the notebook extract below an ambiguous question can be seen with the sequence of events. From entering the chain, action input, observation, thought, etc.\n", "Another practical example; considering the image below, this notebook snipped is of a LangChain agent which has access to web search and GPT4 to answer the question.\n", "Notice how the agent enters a chain, takes the question posed by me: What year was the founder of SpaceX and Tesla born and what is the name of the first company he founded? And converts the question into an action.\n", "The agent access an action of search, with the search input: fonder of SpaceX and Tesla birth year and first company.\n", "This is an observation based on the initial input the agent receives. The agent has a Thought, that the final answer has been reached. The chain is completed and the response is given: Elon Musk, the founder of SpaceX and Tesla, was born in 1971. The first company he founded was Zip2.\n", "Below is the full working code for a Langchain Agent which can perform:\n", "Considering the code below, an ambiguous question is submitted to the agent:\n", "What year was the founder of SpaceX and Tesla born and what is the name of the first company he founded?\n", "And the agent response:\n", "This example shows an instance were the agent has access to:\n", "Notice the difference in the response:\n", "Chains and Agents can be used interchangeably. The most valuable component of agents are:\n", "  Please follow me on LinkedIn for updates on Conversational AI  \n", "I m currently the Chief Evangelist @ HumanFirst. I explore and write about all things at the intersection of AI and language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces and more.\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-14-d3de0382ba09?source=list-660438a01f7f--------1-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing(Part 14)-Probability and Bayes  Rule", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "3", "response": "2", "timeForRead": "4 min read", "dateCreate": "Oct 15", "text": ["Probability is fundamental to many applications in NLP. You ll see how you can use it to help classify whether a tweet is positive or negative. Let s get started.\n", "To start, we are going to first review what s probabilities and conditional probabilities are, how they operate, and how they can be expressed mathematically. Then I ll go over how to derive Bayes rule from the definition of conditional probabilities. Bayes rule is applied in many different fields, ranging from medicine to education and is used extensively in NLP. Once you understand the theory behind Bayes rule, you can use it to perform sentiment analysis on tweets,\n", "Imagine you have an extensive corpus of tweets that can be categorized as either positive or negative sentiment, but not both. Within that corpus, the word happy is sometimes being labeled positive and sometimes negative.\n", "Let s explore why this situation is occurring.\n", "One way to think about probabilities is by counting how frequently events occur. Suppose you define event A as a tweets being labeled positive, then the probability of event A, shown as B of A here, is calculated as the ratio between the counts of positive tweets in the corpus divided by the total number of tweets in the corpus.\n", "In this example, that number comes out to 13 over 20, or 0.65. You could also express this value as a percentage, 65 percent positive. It s worth noting that the complimentary probability here, which is the probability of the tweets expressing a negative sentiment is just equal to one minus the probability of a positive sentiment.\n", "Note that for this to be true, all tweets must be categorized as either positive or negative but not both. Let s define Event B in a similar way by counting tweets containing the word happy. In this case, the total number of tweets containing the word happy, shown here as N-happy is 4.\n", "Here s another way of looking at it. Take a look at the section of the diagram were tweets are labeled positive and also contain the word happy. In the context of this diagram, the probability that a tweet is labeled positive and also contains the word happy is just the ratio of the area of the intersection divided by the area of the entire corpus.\n", "In other words, if there were 20 tweets in the corpus, and three of them are labeled positive and also contain the word happy, then the associated probability is 3 divided by 20 or 0.15. You now know how to calculate the probability of an intersection. You know how to calculate the probability of a word, namely happy with the probability of being positive. In the next Tutorial, we willtalk about Naive Bayes.\n", "Please Follow coursesteach to see latest updates on this story\n", "If you want to learn more about these topics: Python, Machine Learning Data Science, Statistic For Machine learning, Linear Algebra for Machine learning Computer Vision and Research\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "Note:if you are a NLP export and have some good suggestions to improve this blog to share, you write comments and contribute.\n", "if you need more update about NLP and want to contribute then following and enroll in following\n", " Course: Natural Language Processing (NLP)\n", " GitHub Repository\n", "   Notebook\n", "Do you want to get into data science and AI and need help figuring out how? I can offer you research supervision and long-term career mentoring.Skype: themushtaq48, email:mushtaqmsit@gmail.com\n", "Contribution: We would love your help in making coursesteach community even better! If you want to contribute in some courses , or if you have any suggestions for improvement in any coursesteach content, feel free to contact and follow.\n", "Together, let s make this the best AI learning Community!  \n", " WhatsApp\n", "  Facebook\n", " Github\n", " LinkedIn\n", " Youtube\n", " Twitter\n", "1- Natural Language Processing with Classification and Vector Spaces\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-2-c78237784247?source=list-660438a01f7f--------15-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing(Part 2)", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "1", "response": "2", "timeForRead": "9 min read", "dateCreate": "Jul 28", "text": ["Python Notebook (Link)\n", "we will explore the main NLP tasks and the most popular application for each task.Here are some key NLP tasks and their corresponding applications:\n", "Text Classification: assigning a category to a sentence or document (e.g. spam filtering) [1]. Text classification is the process of automatically categorizing text into predefined classes or categories. For example, a text classification algorithm might be used to classify emails as spam or not spam or to categorize news articles by topic[2]. There are three main types of classification:\n", "Applications include:\n", "1.1- Sentiment Analysis\n", "What is sentiment analysis\n", "Def1: identifying the polarity of a piece of text [1]. Def 2: Sentiment analysis attempts to extract subjective qualities   attitudes, emotions, sarcasm, confusion, suspicion   from the text. Def 3: Determining the sentiment expressed in a piece of text (positive, negative, or neutral). Sentiment analysis is the process of determining the emotional tone behind a piece of text, such as a tweet, a product review, or customer feedback[2].\n", "Applications\n", "Sentiment analysis has numerous real-world applications, such as:\n", "Sentiment analysis finds applications in social media monitoring, customer feedback analysis, and brand reputation management\n", "Ways of using this form of text classification include sorting through customer reviews and inquiries and prioritizing negative ones, monitoring brand sentiment through social media responses, analyzing responses to surveys, or even determining gas in the competitor s strategy using customers.\n", "1- Customer Feedback Analysis\n", "Companies can use sentiment analysis to analyze customer feedback from reviews, social media posts, or surveys. By understanding the sentiment behind these comments, businesses can gain valuable insights into customer satisfaction levels and make data-driven decisions to improve their products or services.\n", "2- Brand Monitoring\n", "Sentiment analysis can also be used for brand monitoring purposes. By analyzing social media mentions and online discussions related to a brand, companies can gauge public perception and take appropriate measures to manage their reputation.\n", "Detecting spam alerts in emails and messages is one of the main applications that every big tech company tries to improve for its customers. Apple s official messaging app and Google s Gmail are great examples of such applications where spam detection works well to protect users from spam alerts.\n", "Topic classification is a supervised machine learning task that involves assigning a document to one of a predefined set of topics. The goal of topic classification is to identify the main topic of a document, such as  politics ,  sports , or  technology .\n", "Topic classification is a challenging task because documents can often be about multiple topics, and the topics can be overlapping or ambiguous. Additionally, the language used to describe a topic can vary depending on the author and the context.\n", "There are a number of different methods for topic classification, including:\n", "NLP models for text classification are the various pre-trained models in use for natural language processing being done with artificial intelligence.In this section, we will be discussing two models that are highly in use globally.\n", "XLNet is a language model that was developed by Google AI in 2020. It is a bidirectional transformer model that was trained on a massive dataset of text and code. XLNet is able to learn long-range dependencies between words, which makes it better at understanding and generating natural language. It doesn t simply classify text but also takes the lead with more complex forms of processing natural language. The processes XLNET uses are based on two main ideas: generalized autoregressive pretraining and transformer-XL\n", "BERT stands for Bidirectional Encoder Representations from Transformers. It is a language model that was developed by Google AI in 2018. BERT is a bidirectional model, which means that it can learn the relationships between words in a sentence in both directions, from left to right and from right to left. This makes BERT better at understanding the context of words, which is essential for tasks such as natural language inference and question answering.\n", "BERT is the abbreviation for  bidirectional encoder representations from transformers  and is a neural network model, which means it uses RNNs (recurrent neural networks) as its main process for modeling languages, answering questions, and machine translation.\n", "What is information extraction: def 1: information extraction is extracting structured information from unstructured text sources like news articles or web pages. This includes tasks like named entity recognition, relation extraction, and event extraction. Def: Information extraction is the process of extracting structured data from unstructured text. For example, an information extraction algorithm might extract product information, such as price and availability, from an e-commerce website. Information extraction is used in a variety of industries, including e-commerce, finance, and healthcare, to extract structured data from unstructured text [2].\n", "It determines how similar the two texts are.Sentence/document similarity is a measure of how similar two pieces of text are, or to what degree they express the same meaning. It is a common task in natural language processing (NLP) and has a wide range of applications, such as:\n", "There are a number of different methods for measuring sentence/document similarity, including:\n", "What is Question Answering means:\n", "Def 1: Question Answering is the task of answering a question in natural language.Building systems that can answer questions posed by users based on a given context or knowledge base. Applications include\n", "Application:\n", "1- Chatbots:\n", "2- Virtual Assistant\n", "Def 1: it translates from one language to another. Def 2:Automatically translating text from one language to another. Machine translation is the process of automatically translating text from one language to another. For example, a machine translation algorithm might translate a news article from Spanish to English. Machine translation is used in a variety of industries, including e-commerce, international business, and government. Popular examples include Google Translate and Microsoft Translator.\n", "Application\n", "What tis Text summarization means: def1: Generating concise summaries of longer texts while retaining important information. Text summarization is useful for news articles, research papers, and meeting transcripts. Creating a shortened version of several documents that preserves most of their meaning. Def:2: Text summarization uses NLP techniques to digest huge volumes of digital text and create summaries and synopses for indexes, research databases, or busy readers who don t have time to read full text. Def: Text summarization is the process of automatically generating a condensed version of a longer piece of text. For example, a text summarization algorithm might take a long news article and generate a shorter summary of the main points. Text summarization is used in a variety of applications, including natural language processing, information retrieval, and machine learning [2].\n", "Application\n", "Named Entity means anything that is a real-world object such as a person, a place, any organization, any product which has a name. For example    My name is Aman, and I and a Machine Learning Trainer . In this sentence the name  Aman , the field or subject  Machine Learning  and the profession  Trainer  are named entities. In Machine Learning Named Entity Recognition (NER) is a task of Natural Language Processing to identify the named entities in a certain piece of text.\n", "Named Entity Recognition (NER) is a technique used to extract entities such as people, organizations, and locations from unstructured text. One way to perform NER is by using pre-trained models, such as the one provided by the spacy library in Python. Here's an example of how to use the spacy library to extract named entities from a piece of text.\n", "Real-life application:\n", "what is Language Generation: Creating human-like text output based on given input or prompts. This includes tasks like. Def:Text generation is the process of automatically generating text, such as creating product descriptions or writing news articles. For example, a text generation algorithm might take a product image as input and generate a product description. Text generation is used in a variety of industries, including e-commerce, marketing, and content creation [2].\n", "What is speech recognition: Def1: Converting spoken language into written text. This technology is used in: Def: Speech recognition is the process of converting spoken words into written text. For example, a speech recognition algorithm might be used in a voice-controlled system, such as a virtual assistant, to transcribe spoken commands into text that can be understood by a computer. Speech recognition is used in a variety of industries, including healthcare, finance, and customer service [2].\n", "Text-to-speech (TTS) is a technology that converts written text into spoken words. It is commonly used in applications such as speech synthesis for the visually impaired, voice assistants, and automated customer service systems.\n", "Real-life application\n", "Some examples of TTS software include Google Text-to-Speech, Amazon Polly, and Apple s Siri.\n", "Text clustering is the process of grouping similar text documents together. For example, a text clustering algorithm might take a collection of news articles and group them into categories such as  sports ,  politics , and  entertainment . Text clustering is used in a variety of applications, including natural language processing, information retrieval, and machine learning [2].\n", "Please Follow coursesteach to see latest updates on this story\n", "If you want to learn more about these topics: Python, Machine Learning Data Science, Statistic For Machine learning, Linear Algebra for Machine learning Computer Vision and Research\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "1-Two minutes NLP   33 important NLP tasks explained\n", "2- Top Most Ten NLP Techniques Used In The Industry-Reading\n", "3-Text classification in artificial intelligence\n"]}, {"link": "https://medium.com/@keerthanasathish/chatgpt-architecture-56d20c339efe?source=list-cf9917645e65--------1-------e3327a426a29---------------------", "title": "ChatGPT Architecture", "subtitle": "false", "autorName": "Keerthana Sathish", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*w5ougHR7_PB2wKTte48F6w.jpeg", "clap": "8", "response": "1", "timeForRead": "3 min read", "dateCreate": "Sep 22", "text": ["The most common Large Language Model (LLM) which we know and used is ChatGPT. There are others too like Bard which is developed by Google. Let us dive into the architecture of ChatGPT.\n", "The most common use of ChatGPT for me is to explain a paragraph in a simpler way. The explanations generated are crystal clear and helped in understanding a concept. Shouldn t we know the architecture of the well-known LLM.\n", "ChatGPT stands for Chat Generative Pre-Trained Transformer. It is based on Natural Language Processing (NLP) techniques.\n", "GPT is built on language models and transformer neural network, and ChatGPT is top of GPT and reinforcement learning.\n", "Language Models are used to understand and generate human like responses.\n", "Probability of next word is calculated for a given sentence. Some of the notable examples are Google Assistant, Alexa, Cortana, Siri.\n", "It is a sequence to sequence architecture. It consists of encoder and decoder. With respect to translation the sequences are words.\n", "Example:\n", "X0   I\n", "X1   am\n", "X2   Keerthana\n", "X3   Sathish\n", "We will translate it to German\n", "Y0   Ich\n", "Y1   bin\n", "Y2   Keerthana\n", "Y3   Sathish\n", "English: I am Keerthana Sathish\n", "German: Ich bin Keerthana Sathish\n", "If decoder is stacked together, we get Generative Pre-Trained Transformer (GPT).\n", "Reinforcement learning is learning from its mistakes. We need an agent, environment and connect through the feedback loop.\n", "In the game of Mario, the actions are buttons to move the characters, state environment is the game. State refers to the various conditions or modes that Mario and the game world can be in at any given time.\n", "Agent has set of actions. Thereby, updated state will be each game frame as time passes and our reward signal will be the change in score.\n", "[1] Code Emporium, ChatGPT   Explained!, https://www.youtube.com/watch?v=NpmnWgQgcsA\n"]}, {"link": "https://medium.com/@keerthanasathish/find-and-replace-regex-531933ce77ca?source=list-cf9917645e65--------2-------e3327a426a29---------------------", "title": "Find and Replace: Regex", "subtitle": "false", "autorName": "Keerthana Sathish", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*w5ougHR7_PB2wKTte48F6w.jpeg", "clap": "1", "response": "1", "timeForRead": "3 min read", "dateCreate": "Jun 5", "text": ["We all perform find/ replace in our day-today tasks. It makes our work easier. To know more about how it works read until the end.\n", "Regex also called as Regular Expression is used to find patterns in a sentence. Common applications are web scraping, data validation, data wrangling and many other task.\n", "Before we get into the examples, let us first understand the metacharacters.\n", "To practice regex the link mentioned below can be used:\n", "https://regex101.com/\n", "[abcdefgh] is equivalent to [a-h].\n", "[^ ]   excludes the characters in the [ ] and includes other characters.\n", "In the below example characters after d is included.\n", "[ ^]   excludes the characters not mentioned within [ ].\n", "Characters from a-d is only included.\n", "We can also use numbers as characters.\n", "Alphabets from a-d and numbers from 0 5 are only included.\n", "The below example groups the character  a  into three. There are 6 groups in total.\n", "It matches the string starting from 9 and takes 10 characters from 9\n", "[89]   Starting from 8 or 9\n", "[0 9]   Number is in the range from [0 9]\n", "{9}   Range from {0 9}\n", "2. Gmail\n", "3. Color/ Colour\n", "The character preceding  ?  is optional to be included.\n", "Regex has various applications like advanced search filters, web scraping, content filtering, search query validation. This is one of the important topics in natural language processing.\n"]}, {"link": "https://medium.com/@keerthanasathish/rule-based-machine-translation-7b0074a91d20?source=list-cf9917645e65--------4-------e3327a426a29---------------------", "title": "Rule Based Machine Translation", "subtitle": "false", "autorName": "Keerthana Sathish", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*w5ougHR7_PB2wKTte48F6w.jpeg", "clap": "2", "response": "1", "timeForRead": "2 min read", "dateCreate": "Jul 6", "text": ["Rule based machine translation works on the rules specified by human experts to describe the translation process.\n", "It is also known as Knowledge Base Machine Translation (KBMT) or Classical Machine Translation.\n", "It depends on the linguistic information present in the source language and the translated language.\n", "Rules are based on Chomsky Normal Form (CNF) and Cocke Kasami Younger(CKY). First words are translated and later looks into syntactic and semantic with phonetics.\n", "Method   I\n", "Third step is similar to direct machine translation.\n", "Step 1: Analysis\n", "Performs\n", "It does not look into position of words i.e., does not look into meaningful sentences.\n", "Step 2: Transfers the knowledge to model\n", "Step 3: Identifying which word will match for the sentence\n", "Method   II\n", "Step 1: Analyze the structure of source language. It is similar to step-1 in method   1. Analysis is based on position of words.\n", "Step 2: Tagging of words based on parts of speech.\n", "Step 3: It is done using specific structure.\n", "Method   III\n", "This is a combination of method 1 and method 2. Position of words changes according to sentence to give us meaningful sentence in target language.\n", "Lexical Transfer   It is the process of mapping words from source language (SL) to target language (TL). It involves selecting most appropriate word for a text to to be translated from SL to TL.\n", "Structural Transfer   Transfers syntactic and structural aspects of a sentence from SL to TL. Meaning of the sentence remains same, preserving the grammar.\n", "Morphological Generator   Ensures that the words translated are in its right form (gender, number, tense) according to the rules in TL.\n", "Post-Generator Target Text   It is the final output i.e., translated sentence from SL.\n", "The position of words is changed (rearranging of words) according to the rules in translated language.\n", "One of the major advantage is that it can handle complex sentences with the approach of neural networks and statistical methods which have increased the quality of translation.\n", "When coming to cons of the model, it becomes difficult to handle idiomatic and ambiguity expressions; it can be challenging to develop rules for complex syntax and grammar.\n"]}, {"link": "https://medium.com/@thoughtworks/can-business-trust-chatgpt-90a09e03f70f?source=list-2eb23a991a63--------39-------0a856388a93a---------------------", "title": "Can business trust ChatGPT?", "subtitle": "false", "autorName": "Thoughtworks", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*-W-ozblVdCGo11Rdmx4Ugg.jpeg", "clap": "174", "response": "6", "timeForRead": "10 min read", "dateCreate": "Oct 11", "text": ["(100% written by humans.)\n", "Earlier in 2023, Samsung severely limited its employees  use of ChatGPT. Somewhat innocently, employees were using it to check their work, but in doing so they inadvertently shared sensitive corporate information. This could be competitively damaging   ChatGPT could hypothetically generate answers for a competitor based on those inputs. In another case, unwitting lawyers used fake reference legal cases invented out of thin air by ChatGPT in a civil suit. They were fined several thousand dollars. The lawyers thought they were using a search engine; they claimed they did not understand it was a generative tool.\n", "Despite these stories, there are nevertheless examples of how ChatGPT can be used effectively. One of the most notable is within customer service, where it has the potential to improve the way customer inquiries are handled. One documented case study from a major e-commerce company used ChatGPT-4 chatbots for customer inquiries: the project led to a 40% reduction in response time and a 30% increase in customer satisfaction.\n", "ChatGPT, Bard and Chinchilla may not be household names yet, but they likely will be very soon. Businesses are investigating Generative AI (sometimes referred to as GenAI) and artificially intelligent chatbots, such as ChatGPT, in two distinct ways. First, it is being used to secure investment from venture funds, private equity and R&D funds within large companies. Second, it s being used in business processes to improve products and unlock increased operational efficiency. However, unless GenAI tools can be trusted, they will struggle to gain widespread adoption   they will likely remain experimental curiosities situated in R&D teams and labs.\n", "It might seem premature to worry about trust when there is already so much interest in the opportunities GenAI can offer. However, it needs to be recognized that there s also an opportunity cost   inaccuracy and misuse could be disastrous in ways organizations can t easily anticipate.\n", "Up until now, digital technology has been traditionally viewed as being trustworthy in the sense that it is seen as being deterministic. An Excel formula, for example, will be executed in the same manner 100% percent of the time, leading to a predictable, consistent outcome. Even when the outcome yields an error   due to implementation issues, changes in the context in which it has been deployed, or even bugs and faults   there is nevertheless a sense that technology should work in a certain way. In the case of Gen AI, however, things are different; even the most optimistic hype acknowledges that it can be unpredictable, and its output is often unexpected. Trust in consistency seems to be less important than excitement at the sheer range of possibilities Gen AI can deliver, seemingly in an instant.\n", "We need to take trust seriously when it comes to Gen AI. Addressing this issue can have a significant global benefit. For example, one system called  Jugalbandi  (meaning entwined twins) was used as the basis for a GenAI chatbot helps Indian citizens access, learn and apply for government website social programs (written in English) in their own language. It impressed Microsoft CEO Satya Nadella so much that he said it is unlocking a  sense of empowerment  in every corner of the [globe]. \n", "There are three main technology-driven trust issues with GenAI:\n", "GenAI, in a category called Limited Memory AI, also known as large language models (LLMs), are fed training natural language data sets and create statistical knowledge from this training data or new data it is exposed to. Its probabilistic machine-learning engine then generates its answers to questions or prompts. As a probabilistic system, it will not produce identical results from identical questions. This can be a very productive outcome   the creativeness of its output makes it useful. And this shouldn t be an issue provided its outputs are true, not plagiarized and socially acceptable.\n", "As mentioned above, Thoughtworks recently deployed an LLM solution to create an authoritative chatbot that provides information on government programs to users in their native language. Because accuracy was particularly important in this context, Thoughtworks explicitly created an architecture to manage and correct potential hallucinations and inaccuracies, to ensure the outputs are trustworthy and reliable. Let s see how it was done.\n", "At their core, LLMs are designed to generate text that s almost indistinguishable from what a human might write. This makes them incredibly powerful tools for creating digital products and services that can interact with people in a more human manner.\n", "However, hallucinations undermine the usefulness of LLMs by sometimes producing inaccurate information. The use case or business process that the LLM is used within is also an important factor in mitigating this risk   is there a human in the loop reviewing the output before it ultimately reaches an end user, or is the GenAI s output immediately presented to an end user/consumer?\n", "First, let s tackle how information fed to a GenAI system can contribute to inaccurate responses.\n", "The information generated by LLMs has two sources:\n", "Parametric knowledge is what an LLM has learned during pre-training from datasets like Common Crawl and Wikipedia. Trustworthy information can also be inserted into the LLM s input prompt. The chances of hallucinations are much greater if we rely exclusively on parametric knowledge. This is because the source of this information may not be trustworthy, or it can be outdated. However, if we ask an LLM to use the information in the input prompt to create answers, the chances of hallucinations reduce drastically. This method is popularly known as Retriever Augmented Generation (RAG). However, the amount of information you can insert into a prompt is typically restricted by GenAI restriction called the  maximum context length  of the LLM. The maximum context length typically varies from 4k tokens (roughly six pages of single-spaced text) to 32k tokens (roughly 50 pages). So it is important to select the proper information to be inserted into the LLM input prompt so that user questions can be answered with that information.\n", "The diagram below shows what we ve termed our  PostRAG  reference architecture for LLM-powered conversational apps, which augments LLMs with authentic information. We believe it s an authoritative solution to tackling GenAI hallucinations. (Note that decisions like which LLM to use and where to host the LLM are outside the scope of this architecture description.)\n", "It has six layers: the UI layer, the conversation handling layer, the data privacy layer, the data acquisition planning layer, the LLM Augmentation layer and, finally, the LLM layer.\n", "Let s take a look at each of these layers in more detail.\n", "The user interface component can be integrated with social media platforms, mobile apps or websites. This interface is needed to handle the voice, text or video inputs made by the user and will subsequently present its response to the user.\n", "This layer deals with activities like converting speech to text, translating input text and converting text responses back to speech. The choice of translation depends on things like the language of the authoritative docs and the languages that the app supports.\n", "The data acquisition planner module plays a particularly important part in reducing hallucinations. The planner does this by using the domain context (keywords) present in the user s input to discretionarily identify the relevant data sources   documents, databases, search engines, and APIs   that are authoritative, useful, and accurate for the rest of the system to formulate an effective response.\n", "In other words, the main tasks of data acquisition planning are deciding which data sources should be queried, in what order, and the relevant query parameters for each. Hallucination risk is mitigated by telling the LLM only to use these queried, accurate source data references in the Prompt input to the LLM. The hallucination risk is also mitigated by the instructions contained in Prompt input.\n", "This planning module could be designed in multiple ways. One is to implement a finite-state machine as the main design of the planner so it can track the state of the conversation with a user and perform state transitions based on user input and conversation history. As the conversation progresses, the state machine-based planner would discretionarily identify or select different data sources for the rest of the LLM system to use. Recently, LLM agents such as ReAct and OpenAI function calling have shown promising results in selecting external sources   like APIs   and synthesizing the results for use by an LLM to generate user responses.\n", "Our LLM Augmentation layer is responsible for data acquisition based on the data acquisition plan. If the source data is in the form of free text, it can be indexed into a vector store using text embedding techniques.\n", "A semantic search can be done using user inputs. The similarity between a user input (q) and a passage from source data (p) is calculated by taking the dot product of the two embeddings. These embedding vectors could be created using models like OpenAI embeddings:\n", "sim(p,q)   EQ(q)TEP(p)\n", "If the data source is a structured database, it can be queried using SQL queries generated by the data acquisition planning layer. External data can be acquired using the APIs of search engines.\n", "Next, the data received after running the data acquisition plan is sent to the LLM within the Prompt input to generate a final user response. This ensures we are augmenting the LLM prompt with trustworthy information.\n", "It s often important that sensitive information doesn t leave the confines of an organization. This is why boundary data redaction is required before it is sent to cloud-hosted LLMs. Alternatively, such sensitive information can be masked and stored in a sensitive information store to log and keep track of the masking. Note that if the LLM is hosted privately, masking and logging may not be needed.\n", "A LLM s trustworthy prompt is generated by combining the task instructions (the user s prompt), trustworthy data and previous user/LLM conversation history. The growing conversation history and data source decisions made by the Planner help form the temporary knowledge boundaries or corpus of specific topics using trustworthy information, that constrains the scope of the conversation leading to trustworthy output responses. The response obtained from the LLM using this prompt is then post-processed to generate a user response.\n", "This section describes the architecture of the Jugalbandi bot that Thoughtworks helped to develop. As noted earlier, it provides Indian citizens with information about government s assistance programs; the tool makes it easy for citizens speaking many different languages to access the same information.\n", "The UI layer chosen is WhatsApp, India s most commonly used social media platform. Since the source information about the government social programs is in English, all the natural language processing is done in English. User queries are converted from speech to text and translated from an Indic language to English. The conversion is performed by Bhashini, the Indian National Public Digital Platform for languages.\n", "Custom rules were used to mask personally identifiable information (PII) in the data privacy layer. The data acquisition plan was created using a finite-state machine. Based on the user query, it creates a plan to search the vector database of government social programs and find the most suitable program for a user s requirement. (More software details about the user state machine can be found here.)\n", "All the information about government social programs was scraped from its official website and converted into vectors using OpenAI embeddings. These government social programs vectors are searched with a user query using an open library for efficient similarity search and clustering of dense vectors called FAISS. Search results are then shown to the user, and the user can select a specific government social program they are interested in. In conclusion, LLM augmentation is done using search and government social program information.\n", "The trustworthy information received from the search or government social programs database is then inserted into the LLM prompt. The conversation history is also inserted into the prompt. The LLM response obtained using this prompt is then sent to the user after post-processing. The text response is converted to audio, and both text and voice responses are shown to the user.\n", "The Jugalbandi system was deployed on the Google cloud, utilizing container-based development so we could run our application on multiple endpoints on a larger scale. It uses Google Looker for BI for dashboarding, taking advantage of its modeling language to create a governed, shareable library of metrics. Lastly Google CloudSQL is used for storage and data persistence.\n", "Data Privacy concerns are handled by the data privacy layer. Organizations  sensitive information could come from users  inputs or during the LLM augmentation process. It may be necessary to mask sensitive information before sending prompts to a cloud-hosted LLM. This can be achieved using a set of rules and/or training custom information redaction models.\n", "Most business conversation apps need to restrict user conversations around a few areas of interest; they won t be able to provide answers to every query. Drawing the knowledge boundaries for LLMs using its prompts on input to constrain it to trustworthy data drastically reduces the chances of creating harmful content. Parsing, pruning, and cutting down the LLM s output answers to eliminate ethically challenged content and prose produced by the LLM is an additional tactic to draw boundaries.\n", "The three main trust issues we identified   hallucinations, privacy and conversational performance   are serious enough to warrant a GenAI system that has been designed to mitigate and possibly eliminate their risks. PostRAG, as we discussed, does. Reputational damage not only to the promise of GenAI but also to a company s brand will become likely if the systems you develop or GenAI products you acquire have not been built in a way that addresses each one of the risks. There s no advantage in pursuing the ROI that GenAI promises if it only reverses customer satisfaction scores, increases losses, and leads to brand damage.\n", "Disclaimer: The statements and opinions expressed in this article are those of the author(s) and do not necessarily reflect the positions of Thoughtworks.\n", "Originally published at https://www.thoughtworks.com.\n"]}, {"link": "https://medium.com/@arshren/the-next-generation-of-learning-tools-llm-langchain-and-search-engines-b12888dbcb61?source=list-9eaefa8b15cb--------0-------35122275c687---------------------", "title": "The Next Generation of Learning Tools: LLM, LangChain, and Search Engines", "subtitle": "Optimize Your Learning on Any Topic with LangChain, LLM, and Search Engines", "autorName": "Renu Khandelwal", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*80JFr8sJirdKIMKUB8RMhA.jpeg", "clap": "95", "response": "1", "timeForRead": "8 min read", "dateCreate": "false", "text": ["Discover how to use modern tools like Large Language Models (LLMs), LangChain, and cutting-edge Search Engines to get the latest information from the web and learn more effectively and efficiently.\n", "In this LangChain series, you will uncover the power of LLM, Langchains, and Search Engine tools and use them to distill the essence of any topic with ease, even without any deep learning knowledge.\n", "Previously in LangChain Series \n", "How LangChain Makes Large Language Models More Powerful: Part 1\n", "How LangChain Makes Large Language Models More Powerful: Part 2\n", "Vector Database: Empowering Next-Gen Applications\n", "Unlock the Power of LangChain to Summarize Text\n", "LLMs are powerful tools that can be used for a variety of tasks, such as\n"]}, {"link": "https://medium.com/@Coursesteach/table-of-content-of-nlp-b9afc4880722?source=list-660438a01f7f--------3-------dbbdeca8bd9e---------------------", "title": "Table of Content of NLP", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "32", "response": "2", "timeForRead": "1 min read", "dateCreate": "Sep 24", "text": ["1- Supervised Machine Learning &Sentiment Analysis |medium|\n", "2- Vocabulary & Feature Extraction|medium|\n", "3-Negative and Positive Frequencies|Medium|\n", "4- Feature Extraction with Frequencies|Medium|\n", "5- Preprocessing |medium|\n", "6- Putting it All Together|medium|\n", "7- Logistic Regression Overview|Medium|\n", "8- Logistic Regression: Training|Medium|\n", "9-Logistic Regression: Testing|Medium|\n", "10- Logistic Regression: Cost Function |medium|\n", "11-Logistic Regression: Cost Function|medium|\n", "12-Sentiment analysis with logistic Regression|medium|\n", "1-Probability and Bayes  Rule|medium|\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "Note:if you are a NLP export and have some good suggestions to improve this blog to share, you write comments and contribute.\n", "if you need more update about NLP and want to contribute then following and enroll in following\n", " Course: Natural Language Processing (NLP)\n", " GitHub Repository\n", "   Notebook\n", "1- Natural Language Processing with Classification and Vector Spaces\n", "2- Logistic Regression testing\n"]}, {"link": "https://medium.com/@cobusgreyling/large-language-model-llm-stack-version-5-5a9306870e7f?source=list-2eb23a991a63--------19-------0a856388a93a---------------------", "title": "Large Language Model (LLM) Stack   Version 5", "subtitle": "In my attempt to interpret what is taking place and where the market is moving, I created a product taxonomy defining the various LLM implementations and use cases.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "191", "response": "9", "timeForRead": "3 min read", "dateCreate": "false", "text": ["There are sure to be overlaps between some products and categories listed above. I looked into the functionality of each and every product listed, hence the categories & segmentation of the landscape is a result of that research.\n", "  Follow me on LinkedIn for updates on Large Language Models  \n", "I m currently the Chief Evangelist @ Kore AI. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}, {"link": "https://medium.com/@ashwin_rachha/patent-phrase-to-phrase-matching-with-pytroch-lightning-79a5f37332fa?source=list-a13ace4f182c--------23-------f7e9b3597071---------------------", "title": "Patent Phrase-to-Phrase Matching with Pytorch Lightning", "subtitle": "false", "autorName": "Ashwin Rachha", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*9DoygWtfqktt76K53r2ptQ.jpeg", "clap": "5", "response": "2", "timeForRead": "9 min read", "dateCreate": "May 11, 2022", "text": ["Kaggle Recently Launched an NLP based competition wherein competitors are required to extract meaning from a large text-based dataset for patents. One cannot get a patent if the invention is already available out there or has been publicly disclosed. Therefore in Research and while registering for innovations and scientific patents it is absolutely important for a semantic search to be performed to confirm whether the invention that one wants to patent has already been registered or not.\n", "It is because of this reason the U.S Patent and Trademark Office offers one of the largest repositories of scientific, technical and commercial information in the world through its Open Data Portal.\n", "In this project we shall leverage the power of state-of-the-art transformer models which have known to perform astoundingly well to solve the problem of semantic similarity search and extract relevant information by matching key phrases in patent documents. Determining the semantic similarity between phrases is critically important during the patent search and examination process to determine if an invention has been described before.\n", "The patent domain is prepared for being exploited by tranformer based models to reap value to various businesses and save billions of dollars yearly. Patents hold the potential for tremendous business values since many companies are on the run for innovation and registering their novels works, patents account for billions of dollars of revenue in transacting the rights and developing them also reviewing patent applications.\n", "This competition and further research should help a broader community in the application of Machine Learning in the patent domain, particularly in semantic search.\n", "Google used a Large BERT model (Bidirectional Encoder Representations from Transformers) for data from patent officies across the United States and many other countries. This study illuminated on how transformers could be leveraged to understand the context of patents in spite of the synonymous nature of the keywords or tokenzed words. BERT is smart enough to weigh the same context term differently in the sample abstracts.\n", "To train the model, Google used a Large BERT training implantation using the core open-sourced Python libraries with the following hyperparameters trained on an 8 8 TPU slice on GCP. Of the over 2,000 terms that the USPTO provides as example synonyms, ~200 exist in multiple CPC codes. These synonyms that exist across multiple CPC codes provide a good mechanism to test how well the BERT algorithm is able to generate different synonyms for the same term in different contexts.\n", "According to Google, this is how BERT approaches patent applications:\n", "We shall use Pytorch Lightning for fine-tuning our Bert-For-Patents model on the data provided for the competition. Pytorch Lightning is optimized for research. In a fast paced modular environment of designing AI systems and models, a lot of unnecessary time is waster in ironing out errors and reusing redundant code for training and evaluating the deep learning pipelines. Not to mention thinking about scaling the applications to be run on multiple GPUs in the world of Big Data where datasets are getting Bigger and Bigger. Unlike Native Pytorch, Pytorch Lightning is a high-level framework that abstracts away implementation details for traning, validating, testing and logging of deep learning models so the entire focus could be placed on the two most important aspects of Deep Learning   Data and Models. Developers can focusing on preparing precise data points with actionable insights and design models apt to fit those data points to extract as much value from the datapoints and transform them aptly to our tasks.\n", "We will be needing the following libraries to make our lives easier -\n", "We will use Pathlib to hardcode the path in to a PosixPath object that will make it easier to deal with absolute file paths.\n", "idanchortargetcontextscore037d61fd2272659b1abatementabatement of pollutionA470.5017b9652b17b68b7a4abatementact of abatingA470.75236d72442aefd8232abatementactive catalystA470.2535296b0c19e1ce60eabatementeliminating processA470.50454c1e3b9184cb5b6abatementforest regionA470.00\n", "Columns of the train data-\n", "Here are some plots related to the Exploratory Data Analysis.\n", "The first letter is the  section symbol  consisting of a letter from  A  ( Human Necessities ) to  H  ( Electricity ) or  Y  for emerging cross-sectional technologies. This is followed by a two-digit number to give a  class symbol  ( A01  represents  Agriculture; forestry; animal husbandry; trapping; fishing ).\n", "We will declare some variables that will be required to initialize model and training parameters.\n", "Pytorch provides two data primitives : Dataset and DataLoader that allow users to use preloaded datasets as well as their own datasets. Dataset stores the samples and their corresponding labels. In this case the init function takes the tokenizer and the dataframe as inputs to the constructor as well as a string phase signifying which dataset we are dealing with train or test.\n", "We can now create a tokenizer for this model. Note that pretrained models assume that text is tokenized in a particular way. In order to ensure that your tokenizer matches your model, use the AutoTokenizer, passing in your model name.\n", "We ll need to combine the context, anchor, and target together somehow. There s not much research as to the best way to do this, so we may need to iterate a bit. To start with, we ll just combine them all into a single string. The model will need to know where each section starts, so we can use the special separator token to tell it:\n", "DataLoader wraps an iterable around the Dataset to enable easy access to the samples. Each iteration below returns a batch of train_features and train_labels (containing batch_size=64 features and labels respectively). Because we specified shuffle=True, after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order, take a look at Samplers).\n", "Now we will Load the pretrained Bert-for-Patents model using the nn.Module base class provided by Pytorch. Modules can also contain other Modules, allowing to nest them in a tree structure. Additionally we pass the encodings of the final layer of bert to a linear layer which produces one continuous variable as this problem is modelled as a regression problem.\n", "We then encapsulte our model in the pl.Lightningmodule base class. The structure of the class is self-explanatory, that is, in addition to defining the neural network and the forward function, with PyTorch Lightning we can define what we want to be done in each batch execution as well as in each epoch for both the training and validation data, on the other hand, we also observe that the optimizer is isolated, which allows us to have a better organization of each element of the training phase.\n", "In Pytorch Lightning training can be done with the Trainer method provided which provides an abstraction for training.\n", "This abstraction achieves the following:\n", "Under the hood, the Lightning Trainer handles the training loop details for you, some examples include:\n", "Post in Progress \n", "Please Upvote!\n"]}, {"link": "https://medium.com/@ankitmarwaha18/nlp-word-embedding-techniques-you-should-know-f4068dba8a55?source=list-6a12672b898d--------21-------54fdf6aa16d2---------------------", "title": "NLP: Word Embedding Techniques you should know", "subtitle": "false", "autorName": "Ankit Marwaha", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/0*uxoMEl39Dvcg96mK", "clap": "75", "response": "2", "timeForRead": "17 min read", "dateCreate": "Feb 26", "text": ["In our previous blog , we discussed Text Pre-processing techniques that is the first step we usually do for any NLP use case, second to Pre-processing we have Word Embeddings that we will discuss further in this blog.\n", "In natural language processing (NLP), an embedding technique is a way of representing words or text data as vectors of numbers that capture the underlying semantic meaning of the words. The idea is to transform the textual data into a numerical representation that can be used as input to machine learning algorithms.\n", "The need for embedding techniques in NLP arises from the fact that machine learning algorithms typically work with numerical data, and therefore require a numerical representation of text data to be able to learn from it. By converting text data into numerical embeddings, we can train machine learning models to perform a variety of NLP tasks, such as sentiment analysis, text classification, and machine translation. Embeddings also allow us to capture the semantic meaning of words and sentences, which is important for many NLP tasks that involve understanding the meaning of natural language text.\n", "Categories of Embedding Techniques\n", "We will discuss briefly on all these techniques with their code implementation, applications, pros and cons.\n", "Input:\n", "Output:\n", "Advantages:\n", "Disadvantages:\n", "Applications: Sentiment Analysis, Topic Modeling, Text Classification, Information Retrieval, Machine Translation.\n", "Concept of N-grams:\n", "In NLP, n-grams are a contiguous sequence of n items, typically words or characters, that occur in a piece of text. An n-gram model is a statistical language model that predicts the probability of the next word in a sequence based on the n-grams of the previous words. For example, in the sentence  The quick brown fox jumps over the lazy dog,  some examples of n-grams are:\n", "The choice of the value of n depends on the specific task and the size of the text corpus. N-grams are widely used in NLP for tasks such as text classification, language modeling, and machine translation.\n", "In BOW , We use Count Vectorizer model to convert text to vectors , we can tune this model with a parameter n-gram depending upon the use case to capture more semantic meaning of the words.\n", "2. TF-IDF   TF-IDF stands for Term Frequency-Inverse Document Frequency, and it is a technique used in natural language processing (NLP) to measure the importance of words in a document or corpus.\n", "In simple terms, TF-IDF gives a score to each word in a document based on how often it appears in the document (term frequency) and how often it appears in the corpus (inverse document frequency). The intuition behind this is that words that appear frequently in a document but rarely in the corpus are more important than words that appear frequently in both.\n", "First, let s define some terms:\n", "The formula for TF-IDF is:\n", "The logarithm is used to dampen the effect of very common words, which would otherwise dominate the score.\n", "Multiplying the term frequency TF(t, d) and the inverse document frequency IDF(t) gives us the TF-IDF score for the term t in the document d. This score is higher when the term t appears frequently in the document d, and when it appears rarely in the corpus as a whole.\n", "By calculating the TF-IDF score for each term in each document, we can compare documents and identify those that are most relevant to a given query. We can also cluster similar documents together based on their TF-IDF scores.\n", "Overall, TF-IDF is a powerful technique for measuring the importance of words in a document or corpus, and it has many applications in natural language processing and text mining.\n", "Input:\n", "The get_feature_names method of the TfidfVectorizer class returns a list of the unique terms that appear in the documents. By default, the vectorizer converts all terms to lowercase and removes punctuation and stop words.\n", "Output:\n", "The first line of output shows the vocabulary of the vectorizer, which consists of 13 unique terms after removing stop words and punctuation. The output then shows the corresponding text for each row of the TF-IDF matrix, along with the TF-IDF values for each term in that document.\n", "Advantages:\n", "Disadvantages:\n", "Applications: Information retrieval, Text classification, Keyword extraction, Recommender systems, Document clustering.\n", "3. Word2Vec-Word2Vec is a widely used technique in natural language processing (NLP) that is used to represent words as vectors in a high-dimensional space. It is a type of neural network that can learn to predict the context of a word, and it does so by training on large amounts of text data.\n", "The key idea behind Word2Vec is that words that are used in similar contexts tend to have similar meanings. By training a neural network to predict the context of a word, the model can learn to group together words that are semantically similar.\n", "Example: If King and Queen are related , Man and Women Are related\n", "King   Man + Woman= Queen(Can show similar representations hence, Semantic Meaning is captured)\n", "Word2vec can utilize either of two model architectures to produce these distributed representations of words: continuous-bag-of-words (CBOW) or continuous skip-gram. CBOW (Continuous Bag of Words) and Skip-gram. In CBOW, the model is trained to predict a word given its context, while in Skip-gram, the model is trained to predict the context given a word. In both architectures, word2vec considers both individual words and a sliding window of context words surrounding individual words as it iterates over the entire corpus. Lets understand both of them Briefly.\n", "The objective function of CBOW model is to predict the middle word as target when given past N/2 history words and N/2 future words. Eg- Pineapples are spikey and yellow . In this case  spikey  is the middle or target word that need to predicted and our context words to be [ Pineapples ,  are ,  and ,  yellow ].\n", "Note: The input selected as an window size from a corpus of text\n", "During training, the input layer feeds the one-hot encoded vectors for the context words into the hidden layer(s)(projection layer). In the projection layer, word vectors of context words are simply averaged as a compressed representation. The output layer takes this compressed representation and produces the embedding vector for the target word. We denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous distributed representation of the context.\n", "The number of hidden layers in projection layer and the number of neurons in each layer are hyperparameters that can be tuned during training to improve the performance of the model.\n", "The embedding vectors are learned by minimizing the loss function, which measures the discrepancy between the predicted and actual target words. The weights of the network are updated using backpropagation and stochastic gradient descent.\n", "Once the model has been trained, the embedding vectors can be used to represent words in a continuous vector space that captures their semantic relationships\n", "2. Skip Grams\n", "This model reverses an objective of CBOW model. Given the current word as an input( spikey ), it predicts the nearby context words within a certain range both in history and future([ Pineapples ,  are ,  and ,  yellow ]).We found that increasing the range improves quality of the resulting word vectors, but it also increases the computational complexity. Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples.\n", "During training, the input layer feeds the one-hot encoded vector for the target word into the hidden layer(s)(Projection Layer). The Projection layer then transform the input vector into a compressed representation(simple average) that captures the context of the target word. The output layer takes this compressed representation and produces the one-hot encoded vectors for the context words.\n", "The number of hidden layers and the number of neurons in each layer are hyperparameters that can be tuned during training to improve the performance of the model.\n", "The weights of the network are learned by minimizing the loss function, which measures the discrepancy between the predicted and actual context words. The weights are updated using backpropagation and stochastic gradient descent.\n", "Once the model has been trained, the embedding vectors can be used to represent words in a continuous vector space that captures their semantic relationships.\n", "Example representation of Both CBOW and Skip-Gram.\n", "Input:\n", "Output:\n", "Advantages:\n", "Disadvantage:\n", "Applications: Text Classification, Recommendation Systems, Search Engines, Question Answering Systems, Chatbots, Machine Translation, Named Entity Recognition.\n", "3. Average Word2Vec\n", "Average word2vec is another way to represent words in a vector space, similar to word2vec. However, instead of using a neural network to predict the context words given a target word or vice versa, it uses a simpler approach that averages the word vectors of all the words in a sentence or document to get the overall vector representation.\n", "The advantage of using average word2vec over word2vec is that it doesn t suffer from the limitation of being able to represent only individual words and their relationships with other words. Instead, it can capture the overall meaning of a sentence or document by taking into account all the words present in it. Additionally, it doesn t require training a neural network, making it a simpler and faster approach.\n", "Input:\n", "Output:\n", "Advantages:\n", "Disadvantages:\n", "Applications: Text classification, Search and recommendation engines, Machine translation, Named entity recognition, Question answering, Chatbots.\n", "Post-padding and Pre-padding in NLP\n", "In NLP, padding refers to the process of adding extra tokens to the beginning or end of a sentence or sequence to make it a fixed length. Pre-padding refers to adding tokens to the beginning of a sequence, while post-padding refers to adding tokens to the end of a sequence.\n", "For example, suppose we have a dataset of sentences with varying lengths, and we want to use them as input to a neural network that requires fixed-length input. We can pad the shorter sentences with zeros at the end to make them the same length as the longest sentence in the dataset.\n", "Pre-padding would involve adding zeros to the beginning of the shorter sentences, while post-padding would involve adding zeros to the end of the shorter sentences.\n", "Padding is important in NLP because it allows us to handle variable-length inputs in a consistent manner and enables the use of batch processing for training and inference.\n", "Here s an example code snippet that demonstrates how to use post and pre padding in NLP using the Keras library in Python:\n", "Input:\n", "In this example, we define a list of sequences and then use the pad_sequences function from the Keras library to pad the sequences with zeros to a maximum length of 5. We use the padding parameter to specify whether to add the padding at the beginning (pre) or end (post) of the sequences.\n", "Output:\n", "As we can see, the pad_sequences function has added zeros to the end of the sequences in the post-padded version, and to the beginning of the sequences in the pre-padded version, to bring them up to the desired length of 5.\n", "The above embedding techniques are being used since long in the industries but Recently, there has been a growing trend towards using ELMO and transformer-based models such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pretrained Transformer) for generating contextualized word embeddings. These models use self-attention mechanisms to generate embeddings that capture the meaning of a word in the context of the entire sentence or document. These embeddings have been shown to be very effective for a wide range of NLP tasks, including sentiment analysis, question answering, and named entity recognition.\n", "We will further discuss about trending ELMO and transformer based Embedding technique in our future blogs.\n", "Summary: The blog will take you through some of most common Embedding techniques that have been explained in well structure way with respective images for better understanding along with their pros, cons and applications with a hands on stuff which will be help beginners in better understanding.\n", "Resources:\n", "[1] Original Research Paper-https://arxiv.org/pdf/1301.3781.pdf\n", "Thanks for taking out time to read the post. I hope it was helpful. Please let me know what are your thoughts/ comments. Feel free to reach out to me if you have any queries and suggestions.\n"]}, {"link": "https://medium.com/@avra42/build-your-own-chatbot-with-openai-gpt-3-and-streamlit-6f1330876846?source=list-dee72bb8661c--------19-------c25b06fd87f2---------------------", "title": "Build Your Own Chatbot   with openAI GPT-3 and Streamlit  ", "subtitle": "false", "autorName": "Avra", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*NzYk5R6Pl1fYyHJPkIl2rA.png", "clap": "288", "response": "8", "timeForRead": "4 min read", "dateCreate": "Dec 24, 2022", "text": ["Creating a Chatbot Has Never Been Easier with GPT-3 and Streamlit\n", "  GitHub  |   Twitter |   YouTube |   BuyMeaCoffee | Ko-fi \n", "The current blog post is well explained in this video  \n", "Note : Consider this post/video tutorial as a continutation of the previous tutorial on Summarizing Scientific Articles with OpenAI   and Streamlit   .\n", "The first step is to import the necessary libraries. We will be using openai to access the text generation API and streamlit to create the chatbot interface.\n", "Next, we will need to set the API key for OpenAI. This key is used to authenticate our requests to the API.\n", "Refer to the \"  Setting Up\" section in the previous article for generating openAI API key and Streamlit secrets.\n", "We create a function called generate_response that takes in a prompt and generates a response using the OpenAI API. The openai.Completion.create function allows us to generate text by providing it with a starting prompt. We can specify the engine to use, the prompt to generate text from, the maximum number of tokens (words and punctuation) to generate with max_tokens, the number of responses to generate with n, and a string to stop generating text at with stop. We can also adjust the temperature parameter, which controls the randomness of the generated text.\n", "We will use Streamlit to create the chatbot interface   by setting the title of the page and initializing some variables to store the chat history.\n", "Next, a function called get_text is created that returns the user's input from a text input field.\n", "If user_input is not empty, we will generate a response using the generate_response function and store it in a variable called output. We will also append the user's input and the generated response to the past and generated lists, respectively, to keep track of the chat history.\n", "Finally, we will display the chat history by iterating through the generated and past lists and using the message function from the streamlit_chat library to display each message.\n", "In conclusion, we have successfully built a chatbot   using OpenAI s GPT-3 API and Streamlit ! With just a few lines of code, we were able to create a simple but powerful chatbot that can generate responses to user inputs. Congratulations!  \n", "  GitHub  |   Twitter |   YouTube |   BuyMeaCoffee | Ko-fi \n", "Hi there ! I m always on the lookout for sponsorship, affiliate links and writing gigs to keep growing my online contents. Any support, feedback and suggestions is very much appreciated ! Drop an email here : avrab.yt@gmail.com \n", "Also consider becoming my Patreon Member ?   you ll get access to exclusive content, codes, or videos beforehand, one-to-one web app development / relevant discussion, live-chat with me on specific videos and other perks. ( FYI : Basic Tier is 50% cheaper than ChatGPT/monthly with benefits which an AI can t help with   )\n", "  GitHub  |   Twitter |   YouTube |   BuyMeaCoffee | Ko-fi \n"]}, {"link": "https://medium.com/@dash.ps/build-chatbot-with-llms-and-langchain-9cf610a156ff?source=list-9eaefa8b15cb--------3-------35122275c687---------------------", "title": "Build Chatbot with LLMs and LangChain  ", "subtitle": "false", "autorName": "Dash ICT", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*E43xWyEqzk4846fyQGzA5Q.png", "clap": "428", "response": "4", "timeForRead": "11 min read", "dateCreate": "Aug 20", "text": ["In this article I share my experience in building Chatbot through my work at Dash Company, Our goal is to delve into a comprehensive exploration of Langchain, covering a wide array of common topics. Additionally, I will share my personal experience in crafting a chatbot that seamlessly integrates with custom APIs to assist users, and provide insightful recommendations based on their webshop data.\n", "I will give you a general idea about the chatbot we built, then I will go through the details step by step.\n", "It s an AI assistant that helps users to analyze their webshop data and give them advice on how they can improve their workshop, where they can spend more money to improve their webshop income, and so on.\n", "The Chatbot reads the API swagger file, and based on the user s question it decides which endpoint needs to use it to get data from the app backend to analyze the data and give the user a perfect answer.\n", "LLMs, or Large Language Models, are advanced artificial intelligence models designed to process and generate human-like text by analyzing and learning patterns from vast amounts of textual data. These models are characterized by their ability to understand, generate, and manipulate language in a coherent and contextually relevant manner.\n", "One of the remarkable capabilities of LLMs is their adaptability to various language-related tasks, including but not limited to:\n", "There are free and paid LLMs, Examples of popular LLMs:\n", "LangChain is a powerful tool that can be used to work with Large Language Models (LLMs). LLMs are very general in nature, which means that while they can perform many tasks effectively, they may not be able to provide specific answers to questions or tasks that require deep domain knowledge or expertise. For example, imagine you want to use an LLM to answer questions about a specific field, like medicine or law. While the LLM may be able to answer general questions about the field, it may not be able to provide more detailed or nuanced answers that require specialized knowledge or expertise.\n", "We decided to use Langchain so we can avoid going low level and not use the OpenAI API directly.\n", "LangChain is a framework that enables developers to build agents that can reason about problems and break them into smaller sub-tasks. With LangChain, we can introduce context and memory into completions by creating intermediate steps and chaining commands together.\n", "LLMs have limitations; to work around this limitation, LangChain offers a useful approach where the corpus of text is preprocessed by breaking it down into chunks or summaries, embedding them in a vector space, and searching for similar chunks when a question is asked. This pattern of preprocessing, real-time collecting, and interaction with the LLM is common and can be used in other scenarios as well, such as code and semantic search.\n", "So at the end of the day, if we go with Open Ai Api directly we will need to build all of the prompts from scratch, build our solution to work around limitations, and build summarizatione and memory tools by ourselves, Why we need to do this if LangChain offers all these tools to mange prompts and limitations ?\n", "Chains are the vital core of LangChain. These logical connections between one or more LLMs are the backbone of LangChain s functionality. Chains can range from simple to complex, contingent on the necessities and the LLMs involved.\n", "Let s build a simple chain so u can get the idea of chains \n", "At first, we create the prompt template and add the variable chain. we will take it from the human question and pass it to the template then send this message to the LLM.\n", "Agents in LangChain present an innovative way to dynamically call LLMs based on user input. They not only have access to an LLM but also a suite of tools (like Google Search, Python REPL, Math Calculator, weather APIs, etc.) that can interact with the outside world.\n", "In this case, the agent leverages the pal-math tool and an OpenAI LLM to solve a math problem embedded in a natural language prompt. It demonstrates a practical case where the agent brings additional value by understanding the prompt, choosing the correct tool to solve the task, and eventually returning a meaningful response.\n", "Memory comes in handy when you want to remember items from previous inputs. For example: if you ask  Who is Albert Einstein?  and then subsequently  Who were his mentors? , then conversational memory will help the agent to remember that  his  refers to  Albert Einstein .\n", "as you notice we add a variable called chat_history this var will take the summary of the chat.\n", "2. summarize the chat\n", "3. add chat summary to ur agent\n", "Tools are functions that agents can use to interact with the world. These tools can be generic utilities (e.g. search), other chains, or even other agents.\n", "Example:\n", "we know that GPT-3 just has information until 2021, and it did not know the actual date, so we will build a tool our agent can use to know the actual date\n", "As you can see in the above example, the tool will let our agent use the tool to get the date.\n", "2. Add it to the agent\n", "Now when you ask it about the real date or a question related to knowing the real date for today it can call it.\n", "Our chatbot was able to access user data by app API, analyze this data and answer user questions.\n", "The first this comes to our mind is to use a Planner Agent\n", "It s an agent that takes the API YAML file read it and converts all the endpoints to tools, the agent can use, then make a plan that contains all APIs need to call them to give human best question answer then call these APIs to analyze these data then give the user the best answer.\n", "The limitation of this approach was:\n", "So our solution was to build a custom planner ( improvements for the original planner ), so we can pass tools for it and add memory.\n", "After we create our custom planner now, we have another problem. We need a chat agent to work with a planner agent so they can give a user-friendly chat. If they ask about analyzing their webshop data will go to the planner if another normal question chat agent will respond.\n", "Now you have two approaches to do this:\n", "The best approach based on our analysis for these two options is to go with Chat agent and use Planner as a tool but as we discussed previously planner takes time to decide which APIs can use and then call them 3 or 4 endpoints\n", "So now we have another two options:\n", "So we decided to take the benefit of using the Planner agent as a tool and edit its prompt template to improve his way of planning and analyzing the user inputs based on the app API.\n", "The final code for our chatbot:\n", "Hopefully, the learnings I have shared through this post have made you more comfortable in taking a deep dive into the LangChain. In this article we cover. how to build a Q&A chatbot based on your own datasets, and how to optimize memory for these chatbots so that you can cherry-pick/summarize conversations to send in the prompt rather than sending all previous chat history as part of the prompt.\n", "As always if there s an easier way to do/explain some of the things mentioned in this article, let me know!\n", "Until next time  \n", "I enjoy writing step-by-step beginner s guides, how-to tutorials, decoding terminology used in ML/AI, etc.\n", "Written by :\n", "Mohammed Zourob LinkedIn\n", "To know more about Dash company u can visit this link:\n", "Dash Company Website\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-1-5727b4efc8b4?source=list-660438a01f7f--------16-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing(Part 1)", "subtitle": "What is Natural Language Processing (NLP)", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "1", "response": "2", "timeForRead": "3 min read", "dateCreate": "Jun 24", "text": ["Def 1: Natural language refers to the medium in which humans communicate with each other. This could be in the form of writing (text) for example emails, articles, news, blogs, bank documents, etc, or speech for example lectures, speeches, audio calls, etc. NLP is one of the major AI technologies aimed at making machines capable enough to interpret speech and text-based human language.\n", "Def2: Natural Language Processing is a branch of linguistics, AI, and CS for the manipulation, and translation of natural language which gives machines the ability to read, understand and derive meaning from human language [4].\n", "Def 3: Simply put, NLP is a set of computational techniques that allow machines to understand and manipulate human spoken languages\n", "By now we have to work with a huge amount of data. In Machine Learning we mainly work with those numerical values. How we can get some actions on text data like news reports, social media comments and posts, and customer reviews in the online stores? We can use Natural language processing techniques to do that.[3].Not only that, Even now we have made daily work easier by using applications made from NLP. Summarization applications, spell checker applications, and machine translations are some of them [3].\n", "Computers and machines are great at working with tabular data or spreadsheets. However, human beings generally communicate in words and sentences, not in the form of tables. Much information that humans speak or write is unstructured. So it is not very clear for computers to interpret such. In natural language processing (NLP), the goal is to make computers understand the unstructured text and retrieve meaningful pieces of information from it [2]\n", " Well NLP is cool and stuff, but how can we leverage it to improve our businesses more efficiently? How it could differ from the more traditional techniques?  [5].As we have said before, NLP allows machines to effectively understand and manipulate human languages. With that, you will be able to automate a lot of tasks and improve their rapidity and scale, like data labeling, translation, customer feedback, and text analysis. Applying NLP to real-world cases and not just for research purposes, will bring a significant competitive advantage to many businesses [5]\n", "Please Follow coursesteach to see latest updates on this story\n", "If you want to learn more about these topics: Python, Machine Learning Data Science, Statistic For Machine learning, Linear Algebra for Machine learning Computer Vision and Research\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "1- What is Natural Language?\n", "2- Natural Language Processing (NLP) with Python\n", "3-NLP Techniques\n", "4- Day 1 30 Days of Natural Language Processing Series with Projects\n", "5- Building An NLP Project From Zero To Hero (1): Project Overview\n", "6- What is Natural Language Processing (NLP), and Why is it Even Relevant in 2022?\n"]}, {"link": "https://medium.com/@ismailaslan1/how-to-containerize-a-huggingface-transformers-model-using-docker-and-flask-a8df93ea2dc3?source=list-2c27d980d3f3--------28-------338c7da11cbf---------------------", "title": "How to containerize a HuggingFace Transformers Model using Docker and Flask?", "subtitle": "false", "autorName": "ismail aslan", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*56Togs1G78LsZ18No8vI7w.jpeg", "clap": "17", "response": "3", "timeForRead": "3 min read", "dateCreate": "Aug 17, 2021", "text": ["HuggingFace have made a huge impact on Natural Language Processing domain by making lots of Transformers models available online. One problem I faced during my MLOPS process is to deploy one of those HuggingFace models for sentiment analysis. In this post, I will shortly summarize what I did to deploy a HuggingFace model using Docker and Flask.\n", "I assume the reader has a basic knowledge about Docker, TensorFlow, Transformers, Flask and PyTorch libraries. The source code can be found at my github repo.\n", "Required libraries: Flask, transformers, TensorFlow. (pip or conda as you wish, I used pip). If you are using TensorFlow, as I do, you will need PyTorch only if you are using a HuggingFace model trained on PyTorch, with the flag from_pt=true. But, to reload and re-use the model from local you don t need PyTorch again, so it will not be needed in your container.\n", "Step 1: Load and save the transformer model in a local directory using save_hf_model.py\n", "Your saved models/transformers directory should look like this:\n", "Load your model from local directory back and test your loaded model by comparing the results of two models, the original and the loaded one.\n", "Step 2: Create a minimal flask app, in fact you can use the one at my github repo without changing anything. Just replace your model with the one in the models/transformers directory. Recommend to test your app at this level again by running with flask.\n", "Step 3: Containerize the app using Dockerfile:\n", "docker build   tag mlapp .\n", "docker run -i -p 9000:5000 mlapp\n", "(add -d flag to run in detach mode in the background, you can change 9000 as you need)\n", "curl 127.0.0.1:9000 -v\n", "Step 4: Test your model with make_req.py. Please note that your data should be in the correct format, for example, as you tested your model in save_hf_model.py.\n", "Step 5: To stop your docker container\n", "docker stop 1fbcac69069c\n", "Your model is now running in your container, ready to deploy anywhere.\n", "Happy machine learning!\n"]}, {"link": "https://medium.com/@cobusgreyling/self-refine-is-an-iterative-refinement-loop-for-llms-23ffd598f8b8?source=list-e28f6edecf84--------35-------7b153c9756d3---------------------", "title": "Self-Refine Is An Iterative Refinement Loop For LLMs", "subtitle": "The Self-Refinement methodology mimic humans in a three step process where initial outputs from an LLM is refined via a sequence of iterative feedback.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "65", "response": "9", "timeForRead": "4 min read", "dateCreate": "Oct 11", "text": ["The biggest advantage of this approach is that it does not require any supervised training data, additional training or reinforcement learning.\n", "A single LLM is used for generation, refinement and feedback.\n", "Self-Refinement was tested across seven diverse tasks (in the footer of this article are three notebooks you can tryout) for dialog response generation, math reasoning, etc.\n", "The basic principle of the Self-Refine approach is, that when we as humans revisit something we generated, we often find ways of improving it. Consider writing an email; if we save the first version in the draft folder, and re-read the email a few hours later, we as humans intuitively find ways on how to improve the writing.\n", "Or when a programmer churns out a piece of code, and subsequently reflect on their code, the programmer will invariably find ways to optimise and improve it.\n", "The Self-Refine study demonstrates how an LLM can provide iterative self-refinement without additional training, yielding higher-quality outputs on a wide range of tasks.\n", "In the image above you see the initial output in the left, the same LLM is used to generate the feedback and yet again the same LLM for refinement.\n", "It is an advantage of self-improvement that only a single LLM is leveraged, however this also introduces an important consideration. And that is the fact that this approach is heavily dependant on the LLM it uses as a base. The study found that improvements are in keeping with the size of the LLM size. GPT-4 & Self-Refine works better than GPT-3.5 & Self-Refine.\n", "Considering the image above, on average, the quality of the output improves as the number of iterations increase.\n", "For instance, for a code optimisation task, the initial output (y0) has a score of 22.0, which improves to 28.8 after three iterations (y3).\n", "Similarly, in the Sentiment Reversal task, the initial output has a score of 33.9, which increases to 36.8 after three iterations.\n", "These findings do bring to mind that a balance will have to be found in terms of quality/improvement and iterations. Multiple iterations introduce considerations like latency, cost and rate limits.\n", "  Follow me on LinkedIn for updates on Large Language Models  \n", "I m currently the Chief Evangelist @ Kore AI. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}, {"link": "https://medium.com/@shaikhrayyan123/a-comprehensive-guide-to-understanding-bert-from-beginners-to-advanced-2379699e2b51?source=list-a0d368e4072c--------0-------c1e9800211a7---------------------", "title": "Mastering BERT: A Comprehensive Guide from Beginner to Advanced in Natural Language Processing (NLP)", "subtitle": "false", "autorName": "Rayyan Shaikh", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/0*4GQ2Vm8ARj9hsvGt", "clap": "1.6K", "response": "12", "timeForRead": "19 min read", "dateCreate": "Aug 26", "text": ["BERT (Bidirectional Encoder Representations from Transformers) is a revolutionary natural language processing (NLP) model developed by Google. It has transformed the landscape of language understanding tasks, enabling machines to comprehend context and nuances in language. In this blog, we ll take you on a journey from the basics to advanced concepts of BERT, complete with explanations, examples, and code snippets.\n", "2. Preprocessing Text for BERT\n", "3. Fine-Tuning BERT for Specific Tasks\n", "4. BERT s Attention Mechanism\n", "5. BERT s Training Process\n", "6. BERT Embeddings\n", "7. BERT s Advanced Techniques\n", "8. Recent Developments and Variants\n", "9. BERT for Sequence-to-Sequence Tasks\n", "10. Common Challenges and Mitigations\n", "11. Future Directions in NLP with BERT\n", "12. Implementing BERT with Hugging Face Transformers Library\n", "In the ever-evolving realm of Natural Language Processing (NLP), a groundbreaking innovation named BERT has emerged as a game-changer. BERT, which stands for Bidirectional Encoder Representations from Transformers, is not just another acronym in the vast sea of machine learning jargon. It represents a shift in how machines comprehend language, enabling them to understand the intricate nuances and contextual dependencies that make human communication rich and meaningful.\n", "Imagine a sentence:  She plays the violin beautifully.  Traditional language models would process this sentence from left to right, missing the crucial fact that the identity of the instrument ( violin ) impacts the interpretation of the entire sentence. BERT, however, understands that the context-driven relationship between words plays a pivotal role in deriving meaning. It captures the essence of bidirectionality, allowing it to consider the complete context surrounding each word, revolutionizing the accuracy and depth of language understanding.\n", "At its core, BERT is powered by a powerful neural network architecture known as Transformers. This architecture incorporates a mechanism called self-attention, allowing BERT to weigh the significance of each word based on its context, both preceding and succeeding. This context-awareness imbues BERT with the ability to generate contextualized word embeddings, which are representations of words considering their meanings within sentences. It s akin to BERT reading and re-reading the sentence to gain a deep understanding of every word s role.\n", "Consider the sentence:  The  lead  singer will  lead  the band.  Traditional models might struggle with the ambiguity of the word  lead.  BERT, however, effortlessly distinguishes that the first  lead  is a noun, while the second is a verb, showcasing its prowess in disambiguating language constructs.\n", "In the chapters to come, we will embark on a journey that demystifies BERT, taking you from its foundational concepts to its advanced applications. You ll explore how BERT is harnessed for various NLP tasks, learn about its attention mechanism, delve into its training process, and witness its impact on reshaping the NLP landscape.\n", "As we delve into the intricacies of BERT, you ll find that it s not just a model; it s a paradigm shift in how machines comprehend the essence of human language. So, fasten your seatbelts as we embark on this enlightening expedition into the world of BERT, where language understanding transcends the ordinary and achieves the extraordinary.\n", "Before BERT can work its magic on text, it needs to be prepared and structured in a way that it can understand. In this chapter, we ll explore the crucial steps of preprocessing text for BERT, including tokenization, input formatting, and the Masked Language Model (MLM) objective.\n", "Imagine you re teaching BERT to read a book. You wouldn t hand in the entire book at once; you d break it into sentences and paragraphs. Similarly, BERT needs text to be broken down into smaller units called tokens. But here s the twist: BERT uses WordPiece tokenization. It splits words into smaller pieces, like turning  running  into  run  and  ning.  This helps handle tricky words and ensures that BERT doesn t get lost in unfamiliar words.\n", "Example: Original Text:  ChatGPT is fascinating.  WordPiece Tokens: [ Chat ,  ##G ,  ##PT ,  is ,  fascinating ,  . ]\n", "BERT loves context, and we need to serve it to him on a platter. To do that, we format the tokens in a way that BERT understands. We add special tokens like [CLS] (stands for classification) at the beginning and [SEP] (stands for separation) between sentences. As Shown in the Figure (Machine Language Model). We also assign segment embeddings to tell BERT which tokens belong to which sentence.\n", "Example: Original Text:  ChatGPT is fascinating.  Formatted Tokens: [ [CLS] ,  Chat ,  ##G ,  ##PT ,  is ,  fascinating ,  . ,  [SEP] ]\n", "BERT s secret sauce lies in its ability to understand the bidirectional context. During its training, some words are masked (replaced with [MASK]) in sentences, and BERT learns to predict those words from their context. This helps BERT grasp how words relate to each other, both before and after. As Shown in the Figure (Machine Language Model)\n", "Example: Original Sentence:  The cat is on the mat.  Masked Sentence:  The [MASK] is on the mat. \n", "Code Snippet: Tokenization with Hugging Face Transformers\n", "This code uses the Hugging Face Transformers library to tokenize text using the BERT tokenizer.\n", "In the next chapter, we ll delve into the fascinating world of fine-tuning BERT for specific tasks and explore how its attention mechanism makes it a language-understanding champ. Stick around to learn more!\n", "After understanding how BERT works, it s time to put its magic to practical use. In this chapter, we ll explore how to fine-tune BERT for specific language tasks. This involves adapting the pre-trained BERT model to perform tasks like text classification. Let s dive in!\n", "BERT comes in different flavors like BERT-base, BERT-large, and more. The variations have varying model sizes and complexities. The choice depends on your task s requirements and the resources you have. Larger models might perform better, but they also require more computational power.\n", "Imagine BERT as a language expert who has already read a ton of text. Instead of teaching it everything from scratch, we fine-tune it on specific tasks. This is the magic of transfer learning   leveraging BERT s pre-existing knowledge and tailoring it for a particular task. It s like having a tutor who knows a lot and just needs some guidance for a specific subject.\n", "The tasks we fine-tune BERT for are called  downstream tasks.  Examples include sentiment analysis, named entity recognition, and more. Fine-tuning involves updating BERT s weights using task-specific data. This helps BERT specialize in these tasks without starting from scratch.\n", "Example: Text Classification with BERT\n", "This code demonstrates using a pre-trained BERT model for text classification using Hugging Face Transformers.\n", "In this snippet, we load a pre-trained BERT model designed for text classification. We tokenize the input text, pass it through the model, and get predictions.\n", "Fine-tuning BERT for specific tasks allows it to shine in real-world applications. In the next chapter, we ll unravel the inner workings of BERT s attention mechanism, which is key to its contextual understanding. Stay tuned to uncover more!\n", "Now that we ve seen how to apply BERT to tasks, let s dig deeper into what makes BERT so powerful   its attention mechanism. In this chapter, we ll explore self-attention, multi-head attention, and how BERT s attention mechanism allows it to grasp the context of language.\n", "Imagine reading a book and highlighting the words that seem most important to you. Self-attention is like that, but for BERT. It looks at each word in a sentence and decides how much attention it should give to other words based on their importance. This way, BERT can focus on relevant words, even if they re far apart in the sentence.\n", "BERT doesn t rely on just one perspective; it uses multiple  heads  of attention. Think of these heads as different experts focusing on various aspects of the sentence. This multi-head approach helps BERT capture different relationships between words, making its understanding richer and more accurate.\n", "BERT s attention isn t limited to just the words before or after a word. It considers both directions! When BERT reads a word, it s not alone; it s aware of its neighbors. This way, BERT generates embeddings that consider the entire context of a word. It s like understanding a joke not just by the punchline but also by the setup.\n", "Code Snippet: Visualizing Attention Weights\n", "In this code, we visualize BERT s attention weights using Hugging Face Transformers. These weights show how much attention BERT pays to different words in the sentence.\n", "BERT s attention mechanism is like a spotlight, helping it focus on what matters most in a sentence. In the next chapter, we ll delve into BERT s training process and how it becomes the language maestro it is. Stay tuned for more insights!\n", "Understanding how BERT learns is key to appreciating its capabilities. In this chapter, we ll uncover the intricacies of BERT s training process, including its pretraining phase, the Masked Language Model (MLM) objective, and the Next Sentence Prediction (NSP) objective.\n", "BERT s journey begins with pretraining, where it learns from an enormous amount of text data. Imagine showing BERT millions of sentences and letting it predict missing words. This exercise helps BERT build a solid understanding of language patterns and relationships.\n", "During pretraining, BERT is given sentences with some words masked (hidden). It then tries to predict those masked words based on the surrounding context. This is like a language version of the fill-in-the-blanks game. By guessing the missing words, BERT learns how words relate to each other, achieving its contextual brilliance.\n", "BERT doesn t just understand words; it grasps the flow of sentences. In the NSP objective, BERT is trained to predict if one sentence follows another in a text pair. This helps BERT comprehend the logical connections between sentences, making it a master at understanding paragraphs and longer texts.\n", "Example: Pretraining and MLM\n", "This code demonstrates pretraining BERT s Masked Language Model (MLM). The model predicts masked words while being trained to minimize the prediction error.\n", "BERT s training process is like teaching it the rules of language through a mix of fill-in-the-blanks and sentence-pair understanding exercises. In the next chapter, we ll dive into BERT s embeddings and how they contribute to its language prowess. Keep learning!\n", "BERT s power lies in its ability to represent words in a way that captures their meaning within a specific context. In this chapter, we ll unravel BERT s embeddings, including its contextual word embeddings, WordPiece tokenization, and positional encodings.\n", "Think of word embeddings as code words for words. BERT takes this a step further with contextual word embeddings. Instead of just having one code word for each word, BERT creates different embeddings for the same word based on its context in a sentence. This way, each word s representation is more nuanced and informed by the surrounding words.\n", "BERT s vocabulary is like a puzzle made of smaller pieces called subwords. It uses WordPiece tokenization to break down words into these subwords. This is particularly useful for handling long and complex words, as well as for tackling words it hasn t seen before.\n", "Since BERT reads words in a bidirectional manner, it needs to know the position of each word in a sentence. Positional encodings are added to the embeddings to give BERT this spatial awareness. This way, BERT knows not just what words mean, but also where they belong in a sentence.\n", "Code Snippet: Extracting Word Embeddings with Hugging Face Transformers\n", "This code shows how to extract word embeddings using Hugging Face Transformers. The model generates contextual embeddings for each word in the input text.\n", "BERT s embeddings are like a language playground where words get their unique context-based identities. In the next chapter, we ll explore advanced techniques for fine-tuning BERT and adapting it to various tasks. Keep learning and experimenting!\n", "As you become proficient with BERT, it s time to explore advanced techniques that maximize its potential. In this chapter, we ll delve into strategies for fine-tuning, handling out-of-vocabulary words, domain adaptation, and even knowledge distillation from BERT.\n", "Fine-tuning BERT requires careful consideration. You can fine-tune not only the final classification layer but also intermediate layers. This enables BERT to adapt more effectively to your specific task. Experiment with different layers and learning rates to find the best combination.\n", "BERT s vocabulary isn t infinite, so it can encounter words it doesn t recognize. When handling OOV words, you can split them into subwords using WordPiece tokenization. Alternatively, you can replace them with a special token, like  [UNK]  for unknown. Balancing OOV strategies is a skill that improves with practice.\n", "BERT, though powerful, may not perform optimally in every domain. Domain adaptation involves fine-tuning BERT on domain-specific data. By exposing BERT to domain-specific text, it learns to understand the unique language patterns of that domain. This can greatly enhance its performance for specialized tasks.\n", "Knowledge distillation involves training a smaller model (student) to mimic the behavior of a larger, pre-trained model (teacher) like BERT. This compact model learns not just the teacher s predictions but also its confidence and reasoning. This approach is particularly useful when deploying BERT on resource-constrained devices.\n", "Code Snippet: Fine-Tuning Intermediate Layers with Hugging Face Transformers\n", "This code illustrates fine-tuning BERT s intermediate layers using Hugging Face Transformers. Extracting intermediate layers can help fine-tune BERT more effectively for specific tasks.\n", "As you explore these advanced techniques, you re on your way to mastering BERT s adaptability and potential. In the next chapter, we ll dive into recent developments and variants of BERT that have further elevated the field of NLP. Stay curious and keep innovating!\n", "As the field of Natural Language Processing (NLP) evolves, so does BERT. In this chapter, we ll explore recent developments and variants that have taken BERT s capabilities even further, including RoBERTa, ALBERT, DistilBERT, and ELECTRA.\n", "RoBERTa is like BERT s clever sibling. It s trained with a more thorough recipe, involving larger batches, more data, and more training steps. This enhanced training regimen results in even better language understanding and performance across various tasks.\n", "ALBERT stands for  A Lite BERT.  It s designed to be efficient, using parameter-sharing techniques to reduce memory consumption. Despite its smaller size, ALBERT maintains BERT s power and can be particularly useful when resources are limited.\n", "DistilBERT is a distilled version of BERT. It s trained to mimic BERT s behavior but with fewer parameters. This makes DistilBERT lighter and faster while still retaining a good portion of BERT s performance. It s a great choice for applications where speed and efficiency matter.\n", "ELECTRA introduces an interesting twist to training. Instead of predicting masked words, ELECTRA trains by detecting whether a replaced word is real or artificially generated. This efficient method makes ELECTRA a promising approach for training large models without the full computational cost.\n", "Code Snippet: Using RoBERTa with Hugging Face Transformers\n", "This code demonstrates using RoBERTa, a variant of BERT, for generating contextual embeddings using Hugging Face Transformers.\n", "These recent developments and variants show how BERT s impact has rippled through the NLP landscape, inspiring new and enhanced models. In the next chapter, we ll explore how BERT can be used for sequence-to-sequence tasks like text summarization and language translation. Stay tuned for more exciting applications of BERT!\n", "In this chapter, we ll explore how BERT, originally designed for understanding individual sentences, can be adapted for more complex tasks like sequence-to-sequence applications. We ll dive into text summarization, language translation, and even its potential in conversational AI.\n", "Text summarization involves distilling the essence of a longer text into a shorter version while retaining its core meaning. Although BERT isn t specifically built for this, it can still be used effectively by feeding the original text and generating a concise summary using the contextual understanding it offers.\n", "Language translation involves converting text from one language to another. While BERT isn t a translation model per se, its contextual embeddings can enhance the quality of translation models. By understanding the context of words, BERT can aid in preserving the nuances of the original text during translation.\n", "Conversational AI requires understanding not just individual sentences but also the flow of dialogue. BERT s bidirectional context comes in handy here. It can analyze and generate responses that are contextually coherent, making it a valuable tool for creating more engaging chatbots and virtual assistants.\n", "Code Snippet: Text Summarization using BERT with Hugging Face Transformers\n", "This code demonstrates using BERT for text summarization using Hugging Face Transformers. The model generates a summary by predicting the most relevant parts of the input text.\n", "As you explore BERT s capabilities in sequence-to-sequence tasks, you ll discover its adaptability to various applications beyond its original design. In the next chapter, we ll tackle common challenges in using BERT and how to address them effectively. Stay tuned for insights on overcoming obstacles in BERT-powered projects!\n", "As powerful as BERT is, it s not without its challenges. In this chapter, we ll dive into some common issues you might encounter while working with BERT and provide strategies to overcome them. From handling long texts to managing computational resources, we ve got you covered.\n", "BERT has a maximum token limit for input, and long texts can get cut off. To mitigate this, you can split the text into manageable chunks and process them separately. You ll need to carefully manage the context between these chunks to ensure meaningful results.\n", "Code Snippet: Handling Long Texts with BERT\n", "BERT models, especially the larger ones, can be computationally demanding. To address this, you can use techniques like mixed-precision training, which reduces memory consumption and speeds up training. Additionally, you might consider using smaller models or cloud resources for heavy tasks.\n", "Code Snippet: Mixed-Precision Training with BERT\n", "While BERT is versatile, it might not perform optimally in certain domains. To address this, fine-tune BERT on domain-specific data. By exposing it to text from the target domain, BERT will learn to understand the nuances and terminology specific to that field.\n", "Code Snippet: Domain Adaptation with BERT\n", "Navigating these challenges ensures that you can harness BERT s capabilities effectively, regardless of the complexities you encounter. In the final chapter, we ll reflect on the journey and explore potential future developments in the world of language models. Keep pushing the boundaries of what you can achieve with BERT!\n", "As we conclude our exploration of BERT, let s gaze into the future and glimpse the exciting directions that Natural Language Processing (NLP) is headed. From multilingual understanding to cross-modal learning, here are some trends that promise to shape the NLP landscape.\n", "BERT s power isn t limited to English. Researchers are expanding their reach to multiple languages. By training BERT in a diverse range of languages, we can enhance its capability to understand and generate text in different tongues.\n", "Code Snippet: Multilingual BERT with Hugging Face Transformers\n", "BERT s contextual understanding isn t limited to text. Emerging research is exploring its application to other forms of data, like images and audio. This cross-modal learning holds the promise of deeper insights by connecting information from multiple sources.\n", "BERT s current training involves a static dataset, but future NLP models are likely to adapt to evolving language trends. Lifelong learning models continuously update their knowledge, ensuring that they remain relevant as languages and contexts evolve.\n", "Code Snippet: Lifelong Learning with BERT\n", "Advancements in NLP models like GPT-3 have shown us the potential for more natural conversations with AI. The future holds even more lifelike interactions as BERT s understanding of context and dialogue continues to improve.\n", "The future of NLP is a tapestry of innovation and possibility. As you embrace these trends, remember that BERT s legacy as a cornerstone of language understanding will continue to shape the way we interact with technology and each other. Keep your curiosity alive and explore the realms that lie ahead!\n", "Now that you ve gained a solid understanding of BERT, it s time to put your knowledge into action. In this chapter, we ll dive into practical implementation using the Hugging Face Transformers library, a powerful toolkit for working with BERT and other transformer-based models.\n", "To get started, you ll need to install the Hugging Face Transformers library. Open your terminal or command prompt and use the following command:\n", "Hugging Face Transformers makes it easy to load pre-trained BERT models. You can choose from various model sizes and configurations. Let s load a basic BERT model for text classification:\n", "BERT processes text in tokenized form. You ll need to tokenize your text using the tokenizer and encode it for the model:\n", "Once you ve encoded your text, you can use the model to make predictions. For example, let s perform sentiment analysis:\n", "Fine-tuning BERT for specific tasks involves loading a pre-trained model, adapting it to your task, and training it on your dataset. Here s a simplified example for text classification:\n", "The Hugging Face Transformers library provides a wide range of models and tasks to explore. You can fine-tune BERT for text classification, named entity recognition, question answering, and much more.\n", "As you experiment with the Hugging Face Transformers library, you ll find it to be an invaluable tool for implementing BERT and other transformer-based models in your projects. Enjoy the journey of turning theory into practical applications!\n", "In this blog post, we embarked on an enlightening journey through the transformative world of BERT   Bidirectional Encoder Representations from Transformers. From its inception to its practical implementation, we ve traversed the landscape of BERT s impact on Natural Language Processing (NLP) and beyond.\n", "We delved into the challenges that come with utilizing BERT in real-world scenarios, uncovering strategies to tackle issues like handling long texts and managing computational resources. Our exploration of the Hugging Face Transformers library provided you with practical tools to harness the power of BERT in your own projects.\n", "As we peered into the future, we caught a glimpse of the endless possibilities that lie ahead in NLP   from multilingual understanding to cross-modal learning and the continual evolution of language models.\n", "Our journey doesn t end here. BERT has set the stage for a new era of language understanding, bridging the gap between machines and human communication. As you venture into the dynamic world of AI, remember that BERT is a stepping stone to further innovations. Explore more, learn more, and create more, for the frontiers of technology are ever-expanding.\n", "Thank you for joining us on this exploration of BERT. As you continue your learning journey, may your curiosity lead you to unravel even greater mysteries and contribute to the transformative landscape of AI and NLP.\n"]}, {"link": "https://medium.com/@ThiyaneshwaranG/top2vec-for-topic-modeling-and-semantic-similarity-and-search-77db82dda7d3?source=list-a13ace4f182c--------30-------f7e9b3597071---------------------", "title": "Top2vec for Topic Modeling and Semantic Similarity and Search", "subtitle": "false", "autorName": "Thiyaneshwaran G", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*Rji4sSrA6aoNn2poVYIF1w.jpeg", "clap": "105", "response": "2", "timeForRead": "4 min read", "dateCreate": "Aug 27, 2022", "text": ["In this article we are going Explore Top2vec model in detail. Let s get started !!\n", "What is Top2Vec:\n", "Top2Vec is an algorithm for topic modeling and Semantic search. It automatically detects the topics present in the text and generates jointly embedded topic, document and word vectors. Once we train the Top2Vec model you can do the following :\n", "What are its Benefits :\n", "So How does it work ?\n", "The Assumption the algorithm makes is that many semantically similar documents are indicative of an underlying topic.\n", "The actual steps :\n", "1.Create jointly embedded document and word vectors using Doc2Vec or Universal Sentence Encoder or BERT Sentence Transformer.\n", "2.Create lower dimensional embedding of document vectors using UMAP.\n", "3.Find dense areas of documents using HDBSCAN.\n", "Well that's for the Theory lets now get our hands dirty !!\n", "Attached is the Dataset used.\n", "Now lets import Top2Vec and then see the dataset\n", "The area of interest is the  text  Column and lets clean the text\n", "Lets view the Cleaned Text\n", "Now is the time to call out model for the Embeddings and for few other action items that we are going to learn\n", "Yay! The model is now Trained. As mentioned above , Top2vec algorithm automatically identifies the number of topics in the document corpus and lets see in our data set\n", "The List of words in these Topics\n", "Lets Visualize them in Word cloud\n", "Lets now try to search the topic with our keyword called  citizens \n", "Lets see the Semantic similarity with the word  Constitutional  \n", "To get the list of documents that which contains the keywords :\n", "Top2vec trained model can be used to Embed any given sentence\n", "We can also reduce the number of topic with the reduction functionality\n", "Well ! That s it for the day. I hope this was helpful to you to practically implement Top2vec. I would also like to talk about t-sne, PCA and UMAP in our future posts .. The git code for the above content is here.\n", "Please leave a clap if you find it useful and share your feedbacks for me to improve the content :)\n"]}, {"link": "https://medium.com/@dr-bruce-cottman/part-1-eight-major-methods-for-finetuning-an-llm-6f746c7259ee?source=list-6a12672b898d--------16-------54fdf6aa16d2---------------------", "title": "Part 1: Eight Major Methods For FineTuning an LLM", "subtitle": "false", "autorName": "Bruce H. Cottman, Ph.D.", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*f958A_bXr8chKfPyY4rXLQ.jpeg", "clap": "210", "response": "3", "timeForRead": "14 min read", "dateCreate": "Jun 6", "text": ["I delve into eight methods that use targeted parameter fine-tuning of LLMs. I discuss in detail Gradient-based, LoRA, QLoRA, and four others as advanced variations of ULMFiT: selecting a small subset of the available parameters in a trained LLM.\n", "Large Language Models (LLMs) are leading the AI movement.\n", "These LLMs vary widely in the tasks that they can accomplish, but all of them, currently, are described in terms of the number of parameters and the amount of text they were trained on.\n", "Fine-tuning LLMs has emerged as a crucial technique to adapt these models to specific tasks and improve their performance.\n", "In Part 1 of three planned posts, I review the evolution of targeted parameter fine-tuning of LLMs, describe in detail five of these fine-tuning methods, and ponder where we might be headed in fine-tuning.\n", "In the early days, fine-tuning was considered a finesse or trick to boost performance in data science competitions, such as Kaggle.\n", "The earliest fine-tuning methods were simple and straightforward. They involved taking a pre-trained Language Model, where the term at the time was NLP (Natural Langage Processing), and fine-tuning it on a small dataset of labeled data. The goal was to improve the LLM s performance on the labeled data by adjusting the parameters of the model.\n", "As LLMs grew in size and were trained on vast amounts of text, they began to exhibit a general understanding of language tasks, including spelling, grammar, and contextual relationships between words.\n", "However, LLMs did poorly or lacked the ability to perform tasks outside the realm of text comprehension, such as coding, image-related tasks, or mathematical calculations. This limitation sparked the need for further training, or fine-tuning to equip LLMs with additional skills.\n", "One of the first papers I read, was published in May 2018, on a fine-tuning method \n"]}, {"link": "https://medium.com/@Capeai/natural-language-processing-a-brief-history-7811f0727f44?source=list-a0aae78aa81b--------33-------5fb2bbebc495---------------------", "title": "Natural Language Processing Series", "subtitle": "Part 1: A Brief History of Natural Language Processing", "autorName": "Cape AI", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/2*gBwT8jL7vFHQcxqxPVxGnA.jpeg", "clap": "64", "response": "false", "timeForRead": "8 min read", "dateCreate": "Sep 30, 2020", "text": ["This is the first blog post in a series focusing on the wonderful world of Natural Language Processing (NLP)! In this post we present you with the lay of the land   describing seven important milestones which have been achieved in the NLP field over the last 20 years. This is largely inspired by Sebastian Ruder s talk at the 2018 Deep Learning Indaba which took place in Stellenbosch.\n", "Short disclaimer before we begin: This post is heavily skewed towards neural network-based advancements. Many of these milestones, however, were built on many influential ideas presented by non-neural network-based work during the same era, which, for brevity purposes, have been omitted from this post.\n", "It s 2001 and the field of NLP is quite nascent. Academics all around the world are beginning to think more about how language could be modelled. After a lot of research, Neural Language models are born. Language modelling is simply the task of determining the probability of the next word (often referred to as a token) occurring in a piece of text given all the previous words. Traditional approaches for tackling this problem were based on n-gram models in combination with some sort of smoothing technique [1]. Bengio et al. [2] were the first to propose using a feed-forward neural network, a so-called word  lookup-table , for representing the n previous words in a sequence as illustrated in Figure 1. Today, this is known as word embeddings.\n", "Excitement and interest grows steadily in the years following Neural Language models. Advances in computer hardware allow researchers to push the boundaries on language modelling, giving rise to new NLP methods. One such method is multi-task learning. The notion of multi-task learning involves training models to solve more than one learning task, while also using a set of shared parameters. As a result, models are forced to learn a representation that exploits the commonalities and differences across all tasks.\n", "Collobert and Weston [3] were the first to apply a form of multi-task learning in the NLP domain back in 2008. They trained two convolutional models with max pooling to perform both part-of-speech and named entity recognition tagging, while also sharing a common word lookup table (or word embedding), as shown in Figure 2. Years later, their paper was highlighted by many experts as a fundamental milestone in deep learning for NLP and received the Test-of-time Award at the 2018 International Conference on Machine Learning (ICML).\n", "If you ve had any exposure to NLP, the first thing you have probably come across is the idea of word embeddings (or more commonly known as word2vec). Although we have seen that word embeddings have been used as far back as 2001, in 2013 Mikolov et al. [4] proposed a simple but novel method for efficiently training these word embeddings on very large unlabeled corpora which ultimately led to their wide-scale adoption.\n", "Word embeddings attempt to create a dense vector representation of text, and addresses many challenges faced with using traditional sparse bag-of-words representation. Word embeddings were shown to capture every intuitive relationship between words such as gender, verb tense and country capital, as illustrated in Figure 3.\n", "Looking back, 2013 appeared to be an inflection point in the NLP field, as research and development grew exponentially thereon. The advancements in word embeddings ultimately sparked the wider application of neural networks in NLP. The key challenge that needed to be addressed was architecturally allowing sequences of variable lengths to be inputted into the neural net which ultimately lead to three architectures emerging, namely: recurrent neural networks (RNNs) (which were soon replaced by long-short term memory (LSTM) networks), convolutional neural networks (CNNs), and recursive neural networks. Today, these neural network architectures have produced exceptional results and are widely used for many NLP applications.\n", "Soon after the emergence of RNNs and CNNs for language modelling, Sutskever et al. [5] were the first to propose a general framework for mapping one sequence to another, which is now known as sequence-to-sequence models. In this framework, an encoder network processes an input sequence token by token and compresses it into a vector representation, represented by the blue layers in Figure 4. A decoder network (represented by the red layers) is then used to predict a new sequence of output tokens based on the encoder state, which takes every previously predicted token as input.\n", "This architecture is particularly useful in tasks such as machine translation (MT) and natural language generation (NLG). It s no surprise that, in 2016, Google announced that it is in the process of replacing all of its statistical-based MT systems with neural MT models [7]. Additionally, since the decoder model can be conditioned on any arbitrary representation, it can also be used for tasks like generating captions for images [8].\n", "Although useful in a wide range of tasks, sequence-to-sequence models were struggling with being able to capture long-range dependencies between words in text. In 2015, the concept of attention was introduced by Bahdanau et al. [9] as a way of addressing this bottleneck. In essence, attention in a neural network is a mechanism for deciding which parts of the input sequence to attend to when routing information. Various attention mechanisms have also been applied in the computer vision space for image captioning [10], which also provides a glimpse into the inner workings of the model, as is seen in Figure 5.\n", "Attention is not only restricted to the input sequence and can also be used to focus on surrounding words in a body of text   commonly referred to as self attention   to obtain more contextual meaning. This is at the heart of the current state-of-the-art transformer architecture, proposed by Vaswani et al. [11] in 2017, which is composed of multiple self-attention layers. The transformer sparked an explosion of new language model architectures (and an inside joke among AI practitioners regarding Sesame Street Muppets), the most notable being Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformers (GPT).\n", "Dai & Le [13] were the first to propose using pre-trained language models in 2015 but this notion was only recently shown to be beneficial across a broad range of NLP-related tasks. More specifically, it was shown that pre-trained language models could be fine-tuned on other data related to a specific target task [14, 15]. Additionally, language model embeddings could also be used as features in a target model leading to significant improvements over the then state-of-the-art models [16], as shown in Figure 7.\n", "Nowadays, there exists an array of initiatives aimed at open-sourcing many large state-of-the-art pre-trained models. These models can be fine-tuned to perform various NLP-related tasks like sequence classification, extractive question answering, named entity recognition and text summarization (to name a few).\n", "NLP is advancing at an incredible pace and is giving rise to global communities dedicated to solving the world s most important problems through language understanding.\n", "Stay tuned to this series to learn more about the awesome world of NLP as we share more on the latest developments, code implementations and thought-provoking perspectives on NLP s impact on the way we interact with the world. It s an extremely exciting time for anyone to get into the world of NLP!\n", "Connect with us and connect with Shane! We love engaging with academic and business communities to share knowledge and co-create valuable AI.\n"]}, {"link": "https://medium.com/@multiplatform.ai/vllm-revolutionizing-ai-with-an-open-source-library-for-efficient-llm-inference-and-serving-3816de9fa15e?source=list-e28f6edecf84--------38-------7b153c9756d3---------------------", "title": "vLLM: Revolutionizing AI with an Open-Source Library for Efficient LLM Inference and Serving", "subtitle": "false", "autorName": "Multiplatform.AI", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*DuhJ_if3FekWVLp6dtMa1Q.png", "clap": "7", "response": "1", "timeForRead": "4 min read", "dateCreate": "Jun 25", "text": ["- Large language models (LLMs) have revolutionized AI by advancing natural language understanding.- vLLM is an open-source library developed by Berkeley researchers, offering a simpler, faster, and cost-effective alternative for LLM inference and serving.- By adopting vLLM, organizations can handle peak traffic more efficiently, utilize limited computational resources, and reduce operational costs.- vLLM achieves 24x higher throughput compared to HuggingFace Transformers, without requiring modifications to the model architecture.- PagedAttention, an innovative attention algorithm introduced in vLLM, optimizes memory usage and enables parallel sampling for increased throughput.- vLLM seamlessly integrates with popular HuggingFace models and supports various decoding algorithms.- The library is easily installable and caters to both offline inference and online serving.\n", "Large language models (LLMs) have transformed the landscape of artificial intelligence (AI), representing a significant breakthrough in the realm of natural language understanding. Among these models, GPT-3 has gained widespread recognition for its unparalleled ability to comprehend vast amounts of data and generate text that mimics human-like expressions. The potential of LLMs to revolutionize human-machine interaction and communication is immense. However, the computational inefficiency of these models has posed a major hurdle, hindering their widespread adoption and real-time applicability. The sheer scale of LLMs, consisting of millions or even billions of parameters, demands substantial computational resources, memory, and processing power, which are not always readily available.\n", "Acknowledging this challenge, researchers from the University of California, Berkeley, have developed vLLM, an open-source library that offers a faster, simpler, and cost-effective alternative for LLM inference and serving. This groundbreaking library has already gained traction within the Large Model Systems Organization (LMSYS), powering their Vicuna and Chatbot Arena. By transitioning to vLLM as their backend solution, the research organization has achieved remarkable gains in peak traffic handling capability, processing five times the previous load, all while utilizing limited computational resources and significantly reducing operational costs. vLLM currently supports various HuggingFace models, including GPT-2, GPT BigCode, and LLaMA, among others. Notably, it achieves an astounding throughput that is 24 times higher than that of HuggingFace Transformers, without requiring any modifications to the underlying model architecture.\n", "In their preliminary research, the Berkeley team identified memory-related challenges as the primary bottleneck affecting LLM performance. LLMs utilize input tokens to generate attention keys and value tensors, which are then cached in GPU memory to facilitate the generation of subsequent tokens. However, the management of these dynamic key and value tensors, known as KV cache, becomes complex due to their substantial memory footprint. Addressing this issue, the researchers devised an innovative solution called PagedAttention, which introduces the concept of paging, derived from operating systems, into LLM serving.\n", "PagedAttention offers a flexible approach to managing key and value tensors by storing them in non-contiguous memory spaces, eliminating the need for continuous long memory blocks. During attention computation, these blocks can be independently retrieved using a block table, resulting in more efficient memory utilization. By adopting this ingenious technique, vLLM minimizes memory wastage to less than 4%, achieving near-optimal memory usage. Furthermore, PagedAttention enables the batching of five times more sequences, maximizing GPU utilization and enhancing overall throughput.\n", "Additionally, PagedAttention facilitates efficient memory sharing during parallel sampling, where multiple output sequences are generated simultaneously from a single prompt. By utilizing a block table, different sequences within PagedAttention can share blocks by mapping logical blocks to the same physical block. This memory-sharing mechanism not only minimizes memory usage but also ensures the secure sharing of computational resources. Experimental evaluations conducted by the Berkeley researchers demonstrated that parallel sampling reduced memory consumption by an impressive 55%, resulting in a 2.2 times increase in throughput.\n", "In summary, vLLM is a powerful solution for managing attention key and value memory, thanks to its implementation of the PagedAttention mechanism. The library exhibits exceptional throughput performance and seamlessly integrates with popular HuggingFace models. It can also be combined with various decoding algorithms, including parallel sampling. The installation of vLLM is straightforward, as it can be done with a simple pip command. The library caters to both offline inference and online serving, making it a versatile tool for leveraging the full potential of LLMs in diverse applications.\n", "The introduction of vLLM and its innovative features signifies a major leap forward in the AI market. The library s ability to enhance the efficiency of LLM inference and serving opens up new possibilities for businesses and researchers. With vLLM, organizations can leverage the power of LLMs more effectively, handling larger workloads while optimizing computational resources and reducing costs. The increased throughput and seamless integration with existing models make vLLM an attractive proposition for businesses seeking to enhance their AI capabilities. Furthermore, the availability of an open-source solution contributes to the democratization of advanced AI technologies, enabling wider accessibility and fostering innovation in the market. The emergence of vLLM sets a new standard for efficient language model deployment and is poised to shape the future of AI-powered applications across various industries.\n", "Source\n"]}, {"link": "https://medium.com/@fareedkhandev/scikit-llm-sklearn-meets-large-language-models-11fc6f30e530?source=list-9eaefa8b15cb--------5-------35122275c687---------------------", "title": "Scikit-LLM: Sklearn Meets Large Language Models", "subtitle": "false", "autorName": "Fareed Khan", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*ujdMB17AE56yPSA3zeZcNA.jpeg", "clap": "839", "response": "10", "timeForRead": "6 min read", "dateCreate": "May 25", "text": ["Scikit-LLM is a game-changer in text analysis. It combines powerful language models like ChatGPT with scikit-learn, offering an unmatched toolkit for understanding and analyzing text. With scikit-LLM, you can uncover hidden patterns, sentiment, and context in various types of textual data, such as customer feedback, social media posts, and news articles. It brings together the strengths of language models and scikit-learn, enabling you to extract valuable insights from text like never before.\n", "Official GitHub Repository   https://github.com/iryna-kondr/scikit-llm\n", "All examples are taken directly from official Repository.\n", "Start by installing Scikit-LLM, the powerful library that integrates scikit-learn with language models. You can install it using pip:\n", "As of May 2023, Scikit-LLM is currently compatible with a specific set of OpenAI models. Therefore, it requires users to provide their own OpenAI API key for successful integration.\n", "Begin by importing the SKLLMConfig module from the Scikit-LLM library and add your openAI key:\n", "As stated in their GitHub repository  \n", "One of the cool things about ChatGPT is its ability to classify text without needing to be specifically trained for it. All it requires are descriptive labels.\n", "Introducing ZeroShotGPTClassifier, a class in Scikit-LLM that lets you create such a model just like any other scikit-learn classifier.\n", "Not only that, Scikit-LLM makes sure that the response it receives actually contains a valid label. If it doesn t, Scikit-LLM will pick a label randomly, considering the probabilities based on how frequently the labels appear in the training data.\n", "In simpler terms, Scikit-LLM handles the API stuff and guarantees you get usable labels. It even fills in if a response is missing a label, choosing one for you based on how often it appeared in the training data.\n", "Here s the interesting part   you don t even need labeled data to train the model. You just need to provide a list of candidate labels:\n", "Isn t that cool? You can train a classifier without explicitly labeled data, simply by specifying the potential labels.\n", "As stated in their GitHub Repository  \n", "Performing Multi-Label Zero-Shot Text Classification is easier than you might think:\n", "The only difference you find in zeroshot an multi label zero shot is when you create an instance of the MultiLabelZeroShotGPTClassifier class, specifying the maximum number of labels you want to assign to each sample (here: max_labels=3)\n", "In the example provided above, the MultiLabelZeroShotGPTClassifier is trained with labeled data (X and y). However, you can also train the classifier without labeled data by providing a list of candidate labels instead. In this case, y should be of type List[List[str]].\n", "Here s an example of training without labeled data:\n", "Text vectorization is a process of converting text into numbers so that machines can understand and analyze it more easily. In this case, the GPTVectorizer is a module from Scikit-LLM that helps convert a piece of text, no matter how long it is, into a fixed-size set of numbers called a vector.\n", "Applying the fit_transform method of the GPTVectorizer instance to the input data X fits the model to the data and transforms the text into fixed-dimensional vectors. The resulting vectors are then assigned to the variable vectors.\n", "Let s demonstrates an example of combining the GPTVectorizer with the XGBoost Classifier in a scikit-learn pipeline. This approach allows for efficient text preprocessing and classification:\n", "GPT is really good at summarizing text. That s why they have a module in Scikit-LLM called GPTSummarizer. You can use it in two ways: on its own or as a step before doing something else (like reducing the size of the data, but with text instead of numbers):\n", "Please note that the max_words hyperparameter acts as a flexible limit for the number of words in the generated summaries. It is not strictly enforced beyond the provided prompt. This means that in certain situations, the actual number of words in the generated summaries may slightly exceed the specified limit. In simpler terms, while max_words sets a rough target for the summary length, the summarizer may occasionally produce slightly longer summaries depending on the context and content of the input text.\n"]}, {"link": "https://medium.com/@venelinvalkov/autogen-build-powerful-ai-agents-with-chatgpt-gpt-4-426cc50ef720?source=list-2eb23a991a63--------7-------0a856388a93a---------------------", "title": "AutoGen   Build Powerful AI Agents with ChatGPT/GPT-4", "subtitle": "Explore AutoGen, a Microsoft library that lets you create LLM applications with agents. These agents can communicate and help you solve complex tasks.", "autorName": "Venelin Valkov", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/2*OoQjeo1aWgiGKub_5QxwvA.jpeg", "clap": "231", "response": "1", "timeForRead": "5 min read", "dateCreate": "Oct 17", "text": ["We ll begin with an introduction to AutoGen and its benefits. Then, we ll kick off with a basic example of building a single agent for analyzing stock price trends. Afterward, we ll delve into a more advanced demonstration, using four agents to construct a cryptocurrency indicator, drawing insights from historical prices and news.\n", "AutoGen is like having a bunch of smart friends who work together to get things done, and it s made with help from top-notch researchers.\n", "You can install AutoGen with pip:\n", "Let s add the required libraries:\n", "Next, you need to enter your API key for OpenAI (get yours from https://platform.openai.com/account/api-keys(opens in a new tab)):\n"]}, {"link": "https://medium.com/@cobusgreyling/large-language-model-landscape-61d90f5ca000?source=list-2eb23a991a63--------21-------0a856388a93a---------------------", "title": "Large Language Model Landscape", "subtitle": "In the recent past I have been observing and describing current LLM-related technologies and trends. In this article I m taking a step back to present an overview of the current Large Language Model (LLM) landscape.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "78", "response": "9", "timeForRead": "5 min read", "dateCreate": "Oct 16", "text": ["The image above shows the ripples caused by the advent of LLMs which can be divided into six bands or zones. As these ripples extend, there are requirements and opportunities for products and services.\n", "Some of these opportunities have been discovered, some are yet to be discovered. I would argue that the danger of being superseded as a product is greater in Zone 5 as apposed to Zone 6.\n", "Zone 5 offers a bigger opportunity for differentiation, substantial built-in intellectual property and stellar UX enabling enterprises to leverage the power of LLMs.\n", "Considering LLMs, in essence LLMs are language bound, however, multi-modal models or multi-modality have been introduced in terms of images, audio and more. This shift gave rise to a more generic term being used, namely Foundation Models.\n", "Apart from increased modalities, there has been model diversification from the large commercial providers, offering multiple models which are more task specific. There has also been a slew of open-sourced models made available.\n", "New prompting techniques have illustrated how models performance can be enhanced and how the market are moving towards a scenario where data discovery, data design, data development and data delivery can be leveraged to achieve this level of model-autonomy.\n", "With the advent of large language models, functionality was more segmented models were trained for specific tasks. Sphere focussed on Knowledge Answering; something Meta called KI-NLP. Models like DialoGPT, GODEL and others focussed on dialog management, etc.\n", "Recent developments in LLMs follows an approach where models incorporate these traits and astounding performance can be extracted using different prompting techniques.\n", "The main implementations of LLMs are listed here, with text generation encompassing tasks like summarisation, rewriting, key-word extraction and more.\n", "Text analysis is becoming increasingly important, and embeddings are vital for these type of implementations.\n", "Speech recognition, also known as ASR is the process of converting audio speech into text. The accuracy of any ASR process can easily be measured via a method called Word Error Rate (WER). ASR opens up vast amounts of recorded language data for LLM training and use.\n", "Two notable shifts in this zone are:\n", "A few specific-use models are listed in this zone. Implementations have been split between general, powerful LLMs, and LLM-based digital/personal assistants like ChatGPT, HuggingChat and Cohere Coral.\n", "The most notable Large Language Model suppliers are listed here. Most of the LLMs have inbuilt knowledge and functionality including human language translation, capability of interpreting and writing code, dialog and contextual management via prompt engineering.\n", "This sector considers tooling to harness the power of LLMs, including vector stores, playgrounds and prompt engineering tools. Hosting like HuggingFace enables no-code interaction via model cards and simple inference APIs.\n", "Lastly, listed in this zone is the idea of data-centric tooling which focusses on repeatable, high value use of LLMs.\n", "The market opportunity in this area is creating foundation tooling which will address a future need for data discovery, data design, data development and data delivery.\n", "Further out, there is a whole host of applications which focus on flow building, idea generation, content and writing assistants. These products focus on UX and adding varying degrees of value between LLMs and the user experience.\n", "  Follow me on LinkedIn for updates on Large Language Models  \n", "I m currently the Chief Evangelist @ Kore AI. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}, {"link": "https://medium.com/@neri.vvo/topic-modelling-in-python-top-3-easy-ways-to-get-started-748726572b71?source=list-1593a492c136--------0-------ee6657477639---------------------", "title": "Topic Modelling In Python   Top 3 Easy Ways To Get Started", "subtitle": "false", "autorName": "Neri Van Otten", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*uFH-Ht_ZJEfnvIXFF-ukng.jpeg", "clap": "1", "response": "1", "timeForRead": "8 min read", "dateCreate": "Dec 15, 2022", "text": ["Topic modelling is a technique used in natural language processing (NLP) to automatically identify and group similar words or phrases in a text. This lets us figure out the central ideas or themes in a group of documents. The main benefit is that this is possible even when there are a lot of different documents.\n", "Topic modelling is one of our top 10 natural language processing techniques and is rather similar to keyword extraction, so definitely check out these articles to ensure you are using the right tools for the right problem.\n", "Topic modelling can be helpful in various applications. Some common examples are automatically organizing a large corpus of documents, understanding customer feedback, or identifying common themes in social media posts.\n", "Topic modelling can be used in various situations where it is helpful to identify the main topics discussed in a text. Here are some potential use cases for topic modelling:\n", "These are just a few examples of the many potential use cases for topic modelling. It can be a powerful tool for making sense of extensive text collections and extracting valuable insights from them.\n", "Topic modelling is a type of unsupervised machine learning that is used to discover the abstract topics that occur in a collection of documents. In topic modelling, a computer program analyses a set of documents and identifies the underlying themes or topics in the text. The program does this without being explicitly told what the topics are. It works without any supervision or guidance from a human. Instead, it relies on statistical techniques to identify patterns in the text that indicate the presence of specific topics.\n", "Topic modelling can uncover hidden structures in extensive collections of documents. It is often used in text mining and natural language processing applications. It is a valuable tool for exploring and understanding large amounts of unstructured text data. Additionally, it can identify trends and patterns that may not immediately appear to a human reader.\n", "One of the most popular topic-modelling algorithms is Latent Dirichlet Allocation (LDA). This algorithm uses a probabilistic approach to identify the underlying topics in a collection of documents. Additionally, LDA assumes that each document is a mixture of topics, and each topic is a mixture of words. As a result, the algorithm uses this assumption to identify the document s topics and related terms.\n", "One of the benefits of LDA is that it can handle large amounts of text data. This makes it well-suited for applications such as analyzing customer feedback or social media posts. Additionally, LDA can identify topics that may not be explicitly mentioned in the text. This can help uncover hidden patterns or trends.\n", "Another popular topic modelling algorithm is non-negative matrix factorization (NMF). NMF uses a linear algebra approach to identify the underlying topics in a collection of documents. Unlike LDA, NMF assumes that each document can only belong to a single topic. This can be helpful for specific applications.\n", "NMF works by decomposing a large matrix of word-document co-occurrences into two smaller matrices: one that represents the words in the documents and the other that defines the topics. As a result, this allows the algorithm to discover the underlying topics in a corpus of documents and extract them in an easily interpretable way.\n", "For example, let s say you have a corpus of 100,000 news articles and want to find the topics that are most commonly discussed in these articles. You could then use NMF to decompose the matrix of word-document co-occurrences into two matrices: one representing the words in the documents and the other defining the topics. The resulting topics would then illustrate the most common themes or topics discussed in the news articles, and you could use these topics to categorize and organize the articles.\n", "Latent Semantic Analysis (LSA) is a dimensionality reduction technique based on singular value decomposition (SVD). Its purpose is to extract the underlying structure of a corpus of documents by representing the documents and words in a low-dimensional space.\n", "In LSA, the first step is to construct a term-document matrix, which represents the frequency of each word in each document. This matrix is then decomposed using SVD, which produces a set of orthogonal latent vectors that capture the relationships between the terms and documents in the corpus. These latent vectors can then identify the underlying topics in the corpus.\n", "One advantage of LSA is that it is computationally efficient, which makes it well-suited for large datasets. Additionally, LSA can handle synonyms and polysemy (words with multiple meanings) in a way that is more robust than some other topic modelling algorithms. However, LSA has been criticized for producing less interpretable topics than those made by different algorithms.\n", "While deep learning is commonly used for a wide range of natural language processing tasks, it is not typically used for topic modelling. Instead, deep learning is often used to improve the performance of other techniques, such as Latent Dirichlet Allocation (LDA) or Non-negative Matrix Factorization (NMF), by providing them with better word embeddings or by incorporating additional context information.\n", "For example, one way deep learning can be used in topic modelling is to train a word embedding model on a large corpus of text. This model can then be used to initialize the word vectors in an LDA or NMF model, which can improve the performance of the topic modelling algorithm. Another way deep learning can be used is to incorporate additional context information, such as the overall structure of the documents in the corpus or the relationships between words, into the topic modelling algorithm. This can help the algorithm better capture the underlying structure of the corpus and produce more accurate and interpretable topics.\n", "Overall, while deep learning is not typically used as a standalone technique for topic modelling, it can help improve other algorithms  performance and provide additional context information that can help the algorithm better capture the underlying structure in the data.\n", "Here is a simple example of how Latent Dirichlet Allocation (LDA) can be implemented in Python using the Scikit-Learn library:\n", "This code uses the LatentDirichletAllocation class from the scikit-learn library to implement LDA. The n_components parameter is then used to specify the number of topics to be learned by the model. The fit method is used to fit the model to the input data, and the transform method is used to generate the topic distribution for each document.\n", "Keep in mind that this is just a simple example, and there are many different ways to implement LDA in Python. As a result, the details of the implementation can depend on the specific details of the problem at hand.\n", "In NLTK, LDA can be implemented using the ldamodel class in the gensim.models.ldamodel module. Here is an example of how you might use this class to train an LDA model on a corpus of text documents:\n", "Here, corpus is a list of documents, where each document is a list of words. The LdaModel class takes the bag-of-words representation of the corpus as input, along with the number of topics to be learned and the dictionary mapping words to unique ids. This will train the LDA model on the corpus and allow you to use the model to infer the topics of new documents or to retrieve the most likely topics for a given document.\n", "BERT is a state-of-the-art natural language processing (NLP) model developed by Google that can be used for a wide range of tasks, including topic modeling. However, it is not a specific topic modelling algorithm, so there is no  BERT topic modelling code  as such.\n", "To use BERT for topic modelling, you must combine it with a topic modelling algorithm such as Latent Dirichlet Allocation (LDA). You can then use the pre-trained Bert model to extract features from your text data, which can be used as input to the LDA algorithm to identify the topics present in the text.\n", "Here is an example of how you might use BERT for topic modelling in Python:\n", "This code uses the transformers library to load the pre-trained BERT model and then defines a function bert_features() to extract features from the input text data using BERT. The sklearn library is then used to perform LDA on the extracted features to identify the topics present in the text.\n", "At Spot Intelligence, we often use topic modelling in the exploratory stages of analysis. It allows us to quickly deep dive into the documents at hand and visually see what the documents are about without reading or browsing through them.\n", "Once we have identified topics we are interested in, we can use the results from the topic modelling to classify the documents and label them accordingly. This allows information to be found faster and further split into specific topics for analysis. This way, we can often segment the data into more manageable chunks that can then be summarised or aggregated together to get a more holistic view of the data set.\n", "Combining topic modelling with a timeline is always an excellent analysis, as topics change over time. This is especially useful when analysing social media data, and doing trend analysis.\n", "What are your favourite use cases of topic modelling? Let us know in the comments.\n", "Originally published at https://spotintelligence.com on December 15, 2022.\n"]}, {"link": "https://medium.com/@varun030403/colbert-a-complete-guide-1552468335ae?source=list-6a12672b898d--------36-------54fdf6aa16d2---------------------", "title": "ColBERT: A complete guide", "subtitle": "false", "autorName": "Varun Bhardwaj", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/0*_CzdJdHLqhdgnuwL", "clap": "309", "response": "2", "timeForRead": "7 min read", "dateCreate": "Aug 19, 2022", "text": ["Me: BERT, can you please find me a Document Retrieval Model?BERT: Yes sure, here is your State Of The Art (SOTA) ColBERT model.Me: What s so special about ColBERT?BERT: Let s understand what s so interesting about ColBERT with this blog.\n", "Since ColBERT is likely to stay around for quite some time, in this blog post, we are going to understand it by attempting to answer these 6 questions:\n", "Recent progress in Natural Language Processing (NLP) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. A document retriever model, in simple terms, is a Machine Learning model which primarily ranks documents based on some heuristic algorithm and retrieves the documents which get the best ranks in the pool of documents.\n", "There are many NLP applications such as Open-domain Question-answering models, and Web search engines, to name a few, which use Document retrievers in their end-to-end pipelines.\n", "2. Why was ColBERT needed?\n", "ColBERT proved to be a major breakthrough which enhanced the performance of document retriever models on a large scale. In prior approaches, while being effective, the increase in effectiveness came with an enormous increase in computational cost, thus making the retrieval process slow. We will see why computational cost used to be high in prior models in the later part of this blog. Some models which didn't use BERT base models to retrieve the documents, such as tf-idf based model, performed unsatisfactorily, though being computationally effective.\n", "ColBERT impressively deals with this trade-off by introducing a late interaction architecture that independently encodes the query and the document using BERT as the base model and then employs a cheap yet powerful interaction step that models their fine-grained similarity.\n", "Ugh-oh, didn t understand? Let s move ahead for now. Things will get clear.\n", "3. What is the core idea behind it?\n", "ColBERT(Contextualized Late interaction over BERT) reconciles efficiency and contextualization, hence getting this abbreviation. In ColBERT, Query and Document text are separately encoded(tokenized) into contextual embeddings using two different BERT( base model can be changed, for eg: RobBERTa, mBERT) models. Contextual embeddings are simply vectors which are being generated as outputs by the BERT models. The 2 sets of encodings (one set for query q and another set of tokens for document d) are allowed to attend each other and compute a relevance score for each query-document pair. The document achieving the highest relevance score for a query gets the lowest rank and vice-versa. In this way, we rank the pool of documents. Figure 2 illustrates other approaches to calculating relevance scores.\n", "Figure 2(a): Representation-focused rankers, which independently compute an embedding for q and another for d and estimate relevance as a single similarity score, say cosine similarity, between two vectors.\n", "Figure 2(b): Interaction-focused rankers, these rankers, instead of summarizing q and d into individual embeddings, models word-level and phrase-level relationships across q and d and match them using a deep neural network (such as CNNs).\n", "Figure 2(c): This model belongs to a more powerful interaction-based paradigm, which models the interactions between words within as well as across query and document at the same time, as in BERT s transformer architecture\n", "Figure 2(d): By isolating the encoding procedure of document and query, it s possible to pre-compute document encodings offline, thereby reducing computational load per query significantly.\n", "It s observed that the fine-grained matching of interaction-based models and the pre-computation of document representations of representation-based models can be combined by retaining yet judiciously delaying the query document interaction. This delaying procedure reduces the computational overhead by a significant margin, thus making the retrieval process swift.\n", "Talking about numbers, it delivers over 170 times speedup relative to other BERT-based retrieval models, while maintaining the overall performance.\n", "4. Architecture\n", "A pre-trained Embedding matrix is used to generate tokens for q and d. Different kinds of tokenization methods can be used, WordPiece tokenisation being the default one. We can also use SentencePiece tokenisation, Byte-level tokenisation, n-gram tokenisation etc.\n", "These tokens are separately passed into BERT-based models for generating encoded representation. Let  Eq  and  Ed  be the contextualised encoding generated by the BERT models separately. There is a  model  attribute to change the base model, by default it s  bert-base-uncased . You can check out more about different bert-based models here.\n", "Using Eq and Ed, ColBERT computes the relevance score between q and d via late interaction, which is defined as a summation of maximum similarity (MaxSim) operators. In particular, we find the maximum cosine similarity (any similarity metric can be used) of each v   Eq with vectors in Ed and combine the outputs via summation. Here, vectors are simply the contextualised encodings of the tokens given as input to the BERT model.\n", "Intuitively, the model searches for each query embedding over all the encodings of the document, thus quantifying the match between a document and query encoding. It calculates similarity scores between each document encoding and query encoding. Then it calculates the MaxSim by taking the largest similarity score between each query encoding and all document terms. Given these term scores, it then estimates the document relevance by summing the matching evidence across all query terms.\n", "If the query has fewer than a pre-defined number of tokens Nq, we pad it with BERT s special [mask] tokens up to length Nq (otherwise, we truncate it to the first Nq tokens). In the case of truncation, ColBERT returns the overflowing tokens along with the output.\n", "5. Training ColBERT- Weak Self-supervision training\n", "ColBERT is trained on triplets which are as follows : <query, positive_document, negative_document>\n", "a) query: Query for which we want to retrieve a document.\n", "b) positive_document: Document which is relevant to the query and can plausibly contain the answer to the query.\n", "c) negative_document: Document which is not relevant to the query and can t plausibly contain the answer to the query.\n", "We initially use a naive retrieval model for ranking the documents based on the heuristic algorithm of that model, we generally use the BM-25 model as the naive retrieval model. It uses tf-idf technique to rank the documents. Then this existing retrieval model is used to collect the top-k passages for every training query and, with a simple heuristic, sort these passages into positive (+ve) and negative ( ve) examples, using those to train another, more effective retriever. This process is applied thrice, resulting in a robust trained ColBERT model.\n", "We get the triplets by using a naive retriever. The top-k ranked documents are the pos_documents, and the rest of them are neg_documents.  k  is the hyperparameter, whose value can be adjusted accordingly. We use these triplets to again train the ColBERT in the same fashion. This process is repeated 3 5 times and we finally get a trained ColBERT model.\n", "6. How to build end-to-end models with ColBERT as a Retriever model?\n", "I ll try to give a glimpse of an Open-Domain Question-Answering model using ColBERT as the retriever model and XLM-RoBERTa as the Reader model.\n", "Step 1: Create a pool of documentsStep 2: Use a pre-trained retriever model and pass the pool of documents along with the query as input to the model.Step 3: Retriever will rank the pool of documents based on similarity scores.Step 4: Parse the top-k documents into paragraphs.Step 5: Pass each of these paragraphs along with the query to the Reader model, which in our case is XLM-RoBERTa.Step 6: Get the answer from the Reader model.\n", "To know more about the Reader Model checkout RoBERTa model detailed overview\n", "References:-https://arxiv.org/abs/2004.12765https://github.com/stanfordnlp/ColBERT-QAhttps://arxiv.org/abs/2007.00814\n"]}, {"link": "https://medium.com/@npolovinkin/how-to-chunk-text-into-paragraphs-using-python-8ae66be38ea6?source=list-dee72bb8661c--------36-------c25b06fd87f2---------------------", "title": "How to chunk text into paragraphs using python", "subtitle": "false", "autorName": "N Polovinkin", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*FABIUJ8mekNB_SnLeiahpg.jpeg", "clap": "259", "response": "10", "timeForRead": "8 min read", "dateCreate": "Sep 23, 2022", "text": ["In this article, I want to show the approach that we are going to use in our project of podcast summarization. To summarize text correctly we first need to split a text into meaningful parts paragraphs.\n", "  General approach  Step 1: Embedding  Step 2: Dot product/cosine similarity  Step 3: Identifying split points  Algorithm at work  Step 4: Create a paragraphed text  Final result\n", "We need to turn text into something that machines can understand   vectors. In Natural Language Processing vector representation of a text is called   Embedding. There are two ways to create it:\n", "The second option is faster and can already give us reasonable results. There number of pre-trained embeddings. In our case, we are going to choose one of these.\n", "Right now we will simply go for the best overall performance embedding    all-mpnet-base-v2 .\n", "First things first   we load all the necessary packages and then start our process.\n", "Now when we have a pre-trained model it is a pretty straightforward process. Let s say we have some random text:\n", "Sad story, but it is not the point of this article. We want to turn this text into a vector representation:\n", "Magic happened, we just turned our 4 sentences into a 768-dimensional world! How is this useful? Well, now sentences are vectors and we can check how close (i.e. similar) those vectors are in the 768-dimensions and there is a very simple way to do that   dot product.\n", "In simple words, a dot product will show how much one vector goes in the direction of another. If two vectors (sentences) point in the same direction we assume that they are similar. But let s check this in practice.\n", "This is an impressive result, giving we only used a few lines of code. We can see that the 5th sentence is going in a separate direction that the 4th one (-0.07). We successfully distinguished the meaning of the sentence about embeddings from sentences about football.\n", "But, of course, there is a much better way to see sentence similarities all at once   create a similarity matrix. Sklearn has a handy function for computing similarities with the cosine_similarity function. Why not use the dot product? Good question. Well, when vectors have the same length (magnitude) there is no difference between dot product and cosine similarity. I only showed the dot product to explain how it works under the hood.\n", "There is an interesting pattern we can spot there. The red square in the middle is a part where I talk about football. Now how would it look like if we changed topics two times? Let s build our text up and plot results.\n", "You probably already starting to get the pattern. We can see two different topics and their split points.\n", "Now when something is easy to see for humans but not necessarily easy for computers. So we need to create some pattern to help it distinguish those change points.\n", "This is a much easier-to-understand representation of the flow of our text. Once again we can see that the 4th sentence with index 3 is our splitting point. Now we do the final part\n", "6. Find relative minima of our vector.\n", "Here is the code for completing all of the steps:\n", "Now, let s change from small text to something that we are going to do in reality   chunking transcripts of long videos and podcasts. During the last project presentation one of our teachers at Le Wagon  Pato asked if we can do a summarization for one specific video:  8. The Sumerians   Fall of the First Cities . Well, I did not forget =)\n", "There was one thing I did not mention yet, but what is important   when you work with long texts you will have the problem that very short sentences create unexpected changing points. The shorter sentence is the lower similarity is possible. Generally speaking the shorter the text is   the less information it contains -> fewer possible similarities can be found.\n", "Now there are lots of smart ways to deal with this problem but for the sake of demonstration we will use the most simple solution   we will shorten very long sentences and reduce very short ones.\n", "Now we follow our steps.\n", "2. Identify splitting points;\n", "Let s zoom in on some parts so that we can really see what is happening.\n", "When we have out splitting points we are left with the easiest but most important part   implementing them into text.\n", "Vuala   We have paragraphed text of 1 thousand sentences.\n", "Let s look at some of the splits we made to check if it makes sense. I m not sure I can publish the whole text because of the rights so I took a few small parts.\n", "Each paragraph is separated with a new line, each new place in text is separated with         and the content of paragraphs is shortened by  .\n", "We can see that the first two paragraphs are nicely separated even though they follow the same thought and the second two paragraphs are precisely separated when the author starts to introduce himself. So overall I would say it's done a pretty good job.\n", "Even though it's not always perfect, sometimes it misses a splitting point by one or two sentences like here:\n", "Here in the first paragraph, we can see that the first sentence got there by an error, however, the next two paragraphs are very well separated as the second one is gratitudes to the voice actors and the third one is gratitude to Patreon supporters.\n", "Thank you all for reading! Please follow me on Medium and Linkedin, feel free to ask any questions. Star our podcast summarization project on GitHub if you liked the solution =).\n", "This is the full code of the ready solution in the Jupyter notebook.\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-9-13690a56d5bb?source=list-660438a01f7f--------7-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing (Part 9)", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "52", "response": "1", "timeForRead": "3 min read", "dateCreate": "Sep 3", "text": ["You will now get an overview of logistic regression. Previously, you learned to extract features, and now you will use those extracted features to predict whether a tweet has a positive sentiment or a negative sentiment.\n", "Logistic regression makes use of a sigmoid function that outputs a probability between zero and one. Let s take a look at the overview of logistic regression. Just a quick recap.\n", "In supervised machine learning, you have input features and a sets of labels. To make predictions based on your data, you use a function with some parameters to map your features to output labels. To get an optimum mapping from your features to labels, you minimize the cost function which works by comparing how closely your output Y hat is to the true labels Y from your data. After which the parameters are updated and you repeat the process until your cost is minimized.\n", "For logistic regression, this function F is equal to the sigmoid function. The function used to classify in logistic regression H is the sigmoid function and it depends on the parameters Theta and then the features vector X superscripts i, where i is used to denote the ith observation or data points. In the context of tweets, that s the ith tweets. Visually, the sigmoid function has this form and it approaches zero as the dot product of Theta transpose X, over here, approaches minus infinity and one as it approaches infinity.\n", "For classification, a threshold is needed. Usually, it is set to be 0.5 and this value corresponds to a dot product between Theta transpose and X equal to zero. So whenever the dot product is greater or equal than zero, the prediction is positive, and whenever the dot product is less than zero, the prediction is negative.\n", "So let s look at an example in the now familiar context of tweets and sentiment analysis. Look at the following tweet. After a preprocessing, you should end up with a list like this. Note that handles are deleted, everything is in lowercase and the word tuning is reduced to its stem, tun.\n", "Then you would be able to extract features given a frequencies dictionary and arrive at a vector similar to the following. With a bias units over here and two features that are the sum of positive and negative frequencies of all the words in your processed tweets.\n", "Now assuming that you already have an optimum sets of parameters Theta, you would be able to get the value of the sigmoid function, in this case, equal to 4.92, and finally, predict a positive sentiment. Now that you know the notation for logistic regression, you can use it to train a weight factor Theta. In the next tutorial, you will learn about the mechanics behind training such a logistic regression classifier.\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "Note:if you are a NLP export and have some good suggestions to improve this blog to share, you write comments and contribute.\n", "if you need more update about NLP and want to contribute then following and enroll in following\n", " Course: Natural Language Processing (NLP)\n", " GitHub Repository\n", "   Notebook\n", "1- Natural Language Processing with Classification and Vector Spaces\n", "2-Logistic Regression Overview\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-6-38e1219e5d0b?source=list-660438a01f7f--------10-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing (Part 6)", "subtitle": " Chapter 2: Sentiment Analysis (Logistic Regression)", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "1", "response": "2", "timeForRead": "5 min read", "dateCreate": "Aug 13", "text": ["In this article, we will explore two key techniques for text analysis: TF-IDF (Term Frequency-Inverse Document Frequency) and Term Frequency.\n", "Term Frequency measures the frequency of a term (word) within a document. Let s consider an example:\n", "Document 1:  The cat chased the mouse.  Document 2:  The mouse ran away. \n", "In Document 1, the term  cat  appears once, while  mouse  appears once as well. Hence, the Term Frequency (TF) of  cat  and  mouse  in Document 1 is 1. In Document 2,  mouse  appears once, resulting in a TF of 1 for  mouse. \n", "While Term Frequency provides valuable information, it doesn t consider the term s importance across the entire corpus. This is where TF-IDF comes into play. TF-IDF combines Term Frequency with Inverse Document Frequency (IDF). IDF measures the rarity of a term across the corpus. Let s calculate the TF-IDF scores for our example:\n", "You previously learned to encode a tweet as a vector of dimension V. You will now learn to encode a tweet or specifically represented as a vector of dimension 3. In doing so, you ll have a much faster speed for your logistic regression classifier, because instead of learning V features, you only have to learn three features.\n", "Let s take a look at how you can do this. You just saw that the frequency of a word in a class is simply the number of times that the word appears on the set of tweets belonging to that class and that this table is basically a dictionary mapping from word-class pairs, to frequencies, or it just tells us how many times each word showed up in its corresponding class.\n", "Now that you ve built your frequencies dictionary, you can use it to extract useful features for sentiment analysis. What does a feature look like? Let s look at the arbitrary tweet m. The first feature would be a bias unit equal to 1. The second is the sum of the positive frequencies for every unique word on tweet m. The third is the sum of negative frequencies for every unique word on the tweet. So to extract the features for this representation, you d only have to sum frequencies of words. Easy.\n", "For instance, take the following tweets. Now let s look at the frequencies for the positive class from the last lecture. The only words from the vocabulary that don t appear on these tweets are happy and because.\n", "Now let s take a look at the second feature from the representation that you saw on the last slide. To get this value, you need to sum the frequencies of the words from the vocabulary that appear on the tweet. At the end, you get a value equal to eight.\n", "Now let s get the value of the third feature. It is the sum of the negative frequencies of the words from the vocabulary that appear on the tweet. For this example, you should get 11 after summing up the underlined frequencies. So far, this tweets, this representation would be equal to the vector 1, 8, 11.\n", "You now know how to represent a tweet as a vector of dimension 3.\n", "Applications of TF-IDF and Term Frequency\n", "To make the most of TF-IDF and Term Frequency techniques, consider the following best practices:\n", "Please Follow coursesteach to see latest updates on this story\n", "If you want to learn more about these topics: Python, Machine Learning Data Science, Statistic For Machine learning, Linear Algebra for Machine learning Computer Vision and Research\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "1- Natural Language Processing with Classification and Vector Spaces\n", "2-Feature Extraction with Frequencies(Video)\n", "3- A Practical Guide to TF-IDF and Term Frequency in Text Analysis\n"]}, {"link": "https://medium.com/@keerthanasathish/introduction-to-machine-translation-ca6e91465467?source=list-cf9917645e65--------3-------e3327a426a29---------------------", "title": "Introduction to Machine Translation", "subtitle": "false", "autorName": "Keerthana Sathish", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*w5ougHR7_PB2wKTte48F6w.jpeg", "clap": "1", "response": "1", "timeForRead": "3 min read", "dateCreate": "Jun 13", "text": ["When we move to different place we want to learn the language for easy communication. Language connects us to every individual. To become close to an individual, it is the language which can do so. Before all the language learning apps, we had translator. We typed our words and sentences to be translated to a particular language.\n", "To perform the above action we have Machine Translation abbreviated as MT. It is a sub-field of computer linguistics which translates text or speech from one language to other.\n", "Machine Translation is the process of generating or converting text from one language to other. The translated word or sentence should have same meaning of input text and fluency.\n", "Direct translation is a method used by language models that are not trained enough to convert the source text to target text. Example: when we know a language in elementary level, we understand the target text from souce text by thinking of the meaning of the sentence and perform direct translation. The translation may or may not be accurate.\n", "Transfer Learning is the process in which the machine translate model is trained well enough to apply grammar, phonetics, dialects, antonyms, synonyms that can be used to convert the source text to target text.\n", "There are four different methods   Statistical Machine Translation (SMT), Rule Based Machine Translation (RBMT), Hybrid Machine Translation (HMT), Neural Machine Translation (NMT).\n", "It works by employing statistical models that incorporates analyzing the huge volume of bilingual content. Expects to decide the relevance between the words from the source language to the target language.\n", "The major cons of this method is that it does not calculate the context of the sentences that can impact the translation accuracy of the model. It is similar to direct translation.\n", "For example:\n", " I am with my friend  when translated to German will have two sentences such as  Ich bin mit mein Freund  or  Ich bin mit meine Freundein . It depends whether the friend is girl or boy.\n", "The output is predicted using probability. Suppose, target language sentence1 and sentence2 have probability of 0.60 and 0.40 respectively. The highest probability is taken into consideration and the output will be sentence1.\n", "2. Rule Based Machine Translation (RBMT)\n", "Words are generated on the basis of grammatical rules. Directs a grammatical representation of words in the source language and the object language to create a translated sentence.\n", "One of the major cons is that it heavily relies on dictionary for frequent translation and accuracy.\n", "3. Hybrid Machine Translation\n", "It is the combination of rule based and statistical machine translation which uses a translation memory to improve quality and reduce computational time.\n", "Several approaches like multi-engine, multi-pan and confidence based techniques are applied.\n", "Why memory element required for hybrid machine translation?\n", "4. Neural Machine Translation (NMT)\n", "It is a type oof machine translation that relies upon neural network models or methods to build statistical models. It does not rely upon regularization of parameters that influences the word-word-semantic-lexical relationship.\n", "Transformers use Encoder and Decoder. Example is BERT (Bidirectional Encoder Representations from Transformers) is a two-way translator.\n"]}, {"link": "https://medium.com/@markaherschberg/prompt-engineering-jobs-are-a-mirage-5cf9b04bd330?source=list-2eb23a991a63--------15-------0a856388a93a---------------------", "title": "Prompt Engineering Jobs are a Mirage", "subtitle": "Despite the hype these jobs do not and will not exist. Understanding why can help you avoid other dead-end career paths.", "autorName": "Mark A. Herschberg", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*VH-3Isr9SrTm8kKy-9oL8g.png", "clap": "770", "response": "17", "timeForRead": "7 min read", "dateCreate": "Oct 11", "text": ["There s a famous saying (misattributed to many),  It s hard to make predictions, especially about the future.  Ignoring his sage advice, I m going to make a bold claim, one with little upside if I m right, and nothing but downside if I m wrong (legacy evidence on the internet that I was utterly wrong).\n", "Everyone is talking about  query prompting  being the exciting job future. This is a made-up job, or rather one with made-up demand. Understanding why it s utter nonsense, yet all the rage, can help us identify other illusionary  opportunities  we may encounter in our careers.\n", "If you ve followed the news in the past year, you know that AI is the hot topic for everyone in the business world. I ve written repeatedly about it in this blog and spoken about it at events and on podcasts. Time and again I hear  experts  talk about the demand for prompt engineers. To believe the hype, tens of thousands of people will be employed in the coming years. In reality, these jobs will be as in demand as the search engineers were twenty some years ago.\n", "If you don t know what a search query engineer is, that s the point. Search engines in the 1990s allowed incredible productivity gains. For example, as a software engineer when I was stuck on some code and my colleagues couldn t help, I could search the internet and see if anyone posted about this problem and how to solve it. The catch was that search engines weren t very good back then. The thinking was that companies would need specialists to help do web searches. It wasn t crazy at the time; for example, law librarians have assisted judges and attorneys with complex research tasks. This was taking that work and just moving it online to the biggest collection of information history had ever seen. That job, if it existed at all, lasted for all of five minutes. Search engines improved, people were not so incompetent, and we ve been happily web searching ever since.\n", "Today we hear the same siren s call for prompt engineers. If you search for prompt engineering you ll find all sorts of articles and other media touting it as the future (the World Economic Forum wrote about it here). If you search the job boards, you ll find only a handful of roles (I did this search late Sept 2023). Upwork had three jobs. Indeed, when searched with no location, had eight jobs; when the same search on Indeed was done for the bay area it returned one. LinkedIn had thirteen across the US. (Note: I m only including jobs with prompt engineer as the title. There are plenty of software engineering jobs that include those words as part of a larger description; those are really software jobs not prompt engineering jobs, just as listing  document code  as part of a longer job description doesn t make the software engineering job a technical writing job.) The website promptjobs.com has a total of eighty-two jobs listed, but most are other types of engineering, and most jobs are from months ago. (Seems like someone saw a quick win opportunity at the start of the summer; we ll come back to this type of strategy at the end of this article.)\n", "Why the disconnect from the hype in the news to the reality of the job posts? Has demand just not yet materialized? In truth, it will never materialize.\n", "In the 1950s you literally needed a PhD to get a computer to do basic math. In the 1970s those with college degrees in software could use languages like COBOL or FORTRAN to do the same, no PhD required. Software and operating systems continued to advance and today a twelve-year-old can get a computer to do a math problem; in fact, today we have twelve-year-olds and younger programming computers to do much more.\n", "The world evolves faster today than it did at the dawn of computing. The clunky search engines of the late 1990s were replaced by better offerings in a matter of years. While demand for AI (and by AI we mean large language models specifically, although AI encompasses many other areas not gaining as much attention in the news) will continue to grow, the interface and usability of the tools is currently growing even faster than the core capabilities of the large language models that sit behind them. While there is some trial and error needed today, and there will always be some, the level of sophistication needed just won t be high enough to justify a full-time role. There may be some prompt engineers inside AI companies who work as a form of QA (and looking at who is hiring those jobs I saw listed that seems to be many of them), but that s about it. Prompt engineering jobs are a mirage.\n", "Maybe there will be some companies in the near term that don t know any better and will hire a handful of these roles. If you get one, and don t mind working from a company that misguided in their understanding of AI, you ve got about twelve to eighteen months to jump into a better role before they wake up and realize they don t need prompt engineers.\n", "This begs the question: if there s no demand, why is everyone so excited about the role? Understanding the answer can help us avoid similar snake oil roles in the future.\n", "Prompt engineering represents the American dream: a short cut to riches. AI is hot. VCs are throwing billions of dollars at AI companies while management consultants predict hundreds of billions in market opportunity. In turn these companies, tech and otherwise, are throwing big bucks at anyone who can help them win the AI race.\n", "Like computers decades ago, creating these tools requires advanced knowledge. Many of the most in demand people have PhDs or graduate degrees in AI and related fields. There s demand for software engineers too, but often those with AI/ML (artificial intelligence / machine learning experience). Getting a PhD requires, ugh, hard work. Even if you don t need a graduate degree, you need extensive computer programming ability, not just a quick boot camp worth of coding that just lets you regurgitate what they taught you in a narrow problem space. But a degree, or enough experience that you really understand advanced programming and basic AI/ML, ugh, that s also work.\n", "But wait! Hold the overpriced iPhone you camped out overnight at the Apple store for. There s a hot new job called  prompt engineering.  It has something to do with AI. Everyone s talking about it because, unlike search engineering back in the 1990s, today we have tens of thousands of media outlets (websites, blogs, podcasts, social media) and they each have thousands of talking heads who want to sound like they re cutting edge (he writes as he carefully avoids looking at the mirror). Now everyone can sound as wise and prophetic as Mr. McGuire,  I just want to say two words to you. Just two words. Are you listening? Prompt engineering. \n", " \n", "What makes this really great is that there s no one in the world better at prompt engineering than you. Literally, because no one has done it before. And because there s no roadmap the bar is low. Unlike the inner workings of an LLM (where even the engineers who made it can t really explain cause and effect), this is understandable. You just need to repeatedly hit a black box and record the results through trial and error to become an expert. Suddenly anyone who wants to get into AI but doesn t have time for that, ugh, burdensome training and experience, can jump into the fast-paced, high-paying world of AI. In a year s time you ll be taking a Blue Origin rocket to party on Necker Island.\n", "It s like becoming a life coach. <Goes onto LinkedIn and adds  life coach  to title.> Poof, I m a life coach. Who needs fancy degrees and training! What s that you said? It s no longer just a made-up title but one that does have training? You re right, of course. To be a real life coach I d have to spend upwards of tens of hours and maybe a few hundred dollars to get my certificate. Why look, I can just sign up for this online course through a local community college! If that sounds like too much, ugh, work, Gallop has a course only four-and-a-half days long (to be fair there are six required practice sessions after, so your training might extend into next week, ugh).\n", "While there are undoubtedly life coaches that do well, most do not. Zip Recruiter reports that the average hourly pay for a life coach in (expensive) New York City is $19.20 an hour and goes on to note that,   regardless of location, there are not many opportunities for increased pay or advancement, even with several years of experience.   A low barrier to entry creates a glut of supply. Even if I m wrong, and there do turn out to be lots of prompt engineering jobs (I still don t believe there will be), the pay will be terrible, at best. In other words, don t become a professional cello player and expect to do as well as Yo-Yo Ma.\n", "We love short cuts to riches and given the riches of AI, shortcuts in this field look like winning lottery tickets. Anytime there is a new field with high demand, there will be a bunch of faux jobs touted by people trying to sound smart, who don t understand the fundamentals of the industry. History can help us see ahead. Low hanging fruit, particularly in technology, has a shelf life about as long as actual fruit. Consequently, the value of said metaphorical fruit is, again, not unlike that of actual fruit, far lower in value compared to other options.\n", "History also teaches us that if you want to make money during a gold rush, sell shovels. The best way to make money through prompt engineering is to create and sell a certified prompt engineering online course. Caveat emptor!\n", "Originally published at https://www.thecareertoolkitbook.com.\n", "Mark A. Herschberg is a CTO, MIT instructor, speaker, author of The Career Toolkit: Essential Skills for Success That No One Taught You, and creator of the Brain Bump app.\n", "This column is about careers. He also writes on Medium about media at @cognoscomedia.\n"]}, {"link": "https://medium.com/@cobusgreyling/large-language-model-llm-stack-version-5-5a9306870e7f?source=list-e28f6edecf84--------15-------7b153c9756d3---------------------", "title": "Large Language Model (LLM) Stack   Version 5", "subtitle": "In my attempt to interpret what is taking place and where the market is moving, I created a product taxonomy defining the various LLM implementations and use cases.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "191", "response": "9", "timeForRead": "3 min read", "dateCreate": "false", "text": ["There are sure to be overlaps between some products and categories listed above. I looked into the functionality of each and every product listed, hence the categories & segmentation of the landscape is a result of that research.\n", "  Follow me on LinkedIn for updates on Large Language Models  \n", "I m currently the Chief Evangelist @ Kore AI. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}, {"link": "https://medium.com/@dash.ps/build-chatbot-with-llms-and-langchain-9cf610a156ff?source=list-2eb23a991a63--------11-------0a856388a93a---------------------", "title": "Build Chatbot with LLMs and LangChain  ", "subtitle": "false", "autorName": "Dash ICT", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*E43xWyEqzk4846fyQGzA5Q.png", "clap": "428", "response": "4", "timeForRead": "11 min read", "dateCreate": "Aug 20", "text": ["In this article I share my experience in building Chatbot through my work at Dash Company, Our goal is to delve into a comprehensive exploration of Langchain, covering a wide array of common topics. Additionally, I will share my personal experience in crafting a chatbot that seamlessly integrates with custom APIs to assist users, and provide insightful recommendations based on their webshop data.\n", "I will give you a general idea about the chatbot we built, then I will go through the details step by step.\n", "It s an AI assistant that helps users to analyze their webshop data and give them advice on how they can improve their workshop, where they can spend more money to improve their webshop income, and so on.\n", "The Chatbot reads the API swagger file, and based on the user s question it decides which endpoint needs to use it to get data from the app backend to analyze the data and give the user a perfect answer.\n", "LLMs, or Large Language Models, are advanced artificial intelligence models designed to process and generate human-like text by analyzing and learning patterns from vast amounts of textual data. These models are characterized by their ability to understand, generate, and manipulate language in a coherent and contextually relevant manner.\n", "One of the remarkable capabilities of LLMs is their adaptability to various language-related tasks, including but not limited to:\n", "There are free and paid LLMs, Examples of popular LLMs:\n", "LangChain is a powerful tool that can be used to work with Large Language Models (LLMs). LLMs are very general in nature, which means that while they can perform many tasks effectively, they may not be able to provide specific answers to questions or tasks that require deep domain knowledge or expertise. For example, imagine you want to use an LLM to answer questions about a specific field, like medicine or law. While the LLM may be able to answer general questions about the field, it may not be able to provide more detailed or nuanced answers that require specialized knowledge or expertise.\n", "We decided to use Langchain so we can avoid going low level and not use the OpenAI API directly.\n", "LangChain is a framework that enables developers to build agents that can reason about problems and break them into smaller sub-tasks. With LangChain, we can introduce context and memory into completions by creating intermediate steps and chaining commands together.\n", "LLMs have limitations; to work around this limitation, LangChain offers a useful approach where the corpus of text is preprocessed by breaking it down into chunks or summaries, embedding them in a vector space, and searching for similar chunks when a question is asked. This pattern of preprocessing, real-time collecting, and interaction with the LLM is common and can be used in other scenarios as well, such as code and semantic search.\n", "So at the end of the day, if we go with Open Ai Api directly we will need to build all of the prompts from scratch, build our solution to work around limitations, and build summarizatione and memory tools by ourselves, Why we need to do this if LangChain offers all these tools to mange prompts and limitations ?\n", "Chains are the vital core of LangChain. These logical connections between one or more LLMs are the backbone of LangChain s functionality. Chains can range from simple to complex, contingent on the necessities and the LLMs involved.\n", "Let s build a simple chain so u can get the idea of chains \n", "At first, we create the prompt template and add the variable chain. we will take it from the human question and pass it to the template then send this message to the LLM.\n", "Agents in LangChain present an innovative way to dynamically call LLMs based on user input. They not only have access to an LLM but also a suite of tools (like Google Search, Python REPL, Math Calculator, weather APIs, etc.) that can interact with the outside world.\n", "In this case, the agent leverages the pal-math tool and an OpenAI LLM to solve a math problem embedded in a natural language prompt. It demonstrates a practical case where the agent brings additional value by understanding the prompt, choosing the correct tool to solve the task, and eventually returning a meaningful response.\n", "Memory comes in handy when you want to remember items from previous inputs. For example: if you ask  Who is Albert Einstein?  and then subsequently  Who were his mentors? , then conversational memory will help the agent to remember that  his  refers to  Albert Einstein .\n", "as you notice we add a variable called chat_history this var will take the summary of the chat.\n", "2. summarize the chat\n", "3. add chat summary to ur agent\n", "Tools are functions that agents can use to interact with the world. These tools can be generic utilities (e.g. search), other chains, or even other agents.\n", "Example:\n", "we know that GPT-3 just has information until 2021, and it did not know the actual date, so we will build a tool our agent can use to know the actual date\n", "As you can see in the above example, the tool will let our agent use the tool to get the date.\n", "2. Add it to the agent\n", "Now when you ask it about the real date or a question related to knowing the real date for today it can call it.\n", "Our chatbot was able to access user data by app API, analyze this data and answer user questions.\n", "The first this comes to our mind is to use a Planner Agent\n", "It s an agent that takes the API YAML file read it and converts all the endpoints to tools, the agent can use, then make a plan that contains all APIs need to call them to give human best question answer then call these APIs to analyze these data then give the user the best answer.\n", "The limitation of this approach was:\n", "So our solution was to build a custom planner ( improvements for the original planner ), so we can pass tools for it and add memory.\n", "After we create our custom planner now, we have another problem. We need a chat agent to work with a planner agent so they can give a user-friendly chat. If they ask about analyzing their webshop data will go to the planner if another normal question chat agent will respond.\n", "Now you have two approaches to do this:\n", "The best approach based on our analysis for these two options is to go with Chat agent and use Planner as a tool but as we discussed previously planner takes time to decide which APIs can use and then call them 3 or 4 endpoints\n", "So now we have another two options:\n", "So we decided to take the benefit of using the Planner agent as a tool and edit its prompt template to improve his way of planning and analyzing the user inputs based on the app API.\n", "The final code for our chatbot:\n", "Hopefully, the learnings I have shared through this post have made you more comfortable in taking a deep dive into the LangChain. In this article we cover. how to build a Q&A chatbot based on your own datasets, and how to optimize memory for these chatbots so that you can cherry-pick/summarize conversations to send in the prompt rather than sending all previous chat history as part of the prompt.\n", "As always if there s an easier way to do/explain some of the things mentioned in this article, let me know!\n", "Until next time  \n", "I enjoy writing step-by-step beginner s guides, how-to tutorials, decoding terminology used in ML/AI, etc.\n", "Written by :\n", "Mohammed Zourob LinkedIn\n", "To know more about Dash company u can visit this link:\n", "Dash Company Website\n"]}, {"link": "https://medium.com/@power.up1163/topic-modelling-over-short-texts-with-tweetopic-2e5df368433?source=list-6a12672b898d--------30-------54fdf6aa16d2---------------------", "title": "Topic modelling over short texts with tweetopic", "subtitle": "A fast, scalable and high-quality approach to modelling short texts and Tweets.", "autorName": "M rton Kardos", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*u8Ez5oORTX0C98rL5AVjnQ.jpeg", "clap": "31", "response": "1", "timeForRead": "4 min read", "dateCreate": "Feb 23", "text": ["Topic modelling is a set of unsupervised machine learning practices for discovering underlying topics and themes in a corpus of text.\n", "The most widely available methods of topic modelling are either probabilistic generative models, such as Latent Dirichlet Allocation or dimensionality reduction methods, such as Non-negative Matrix Factorization.\n", "Most approaches to topic modelling assume, that each text is made up of a mixture of topics with certain weights. When dealing with short texts, such as tweets, however this is rarely ever the case, and it would be a more reasonable assumption that each text is largely dominated by one topic.\n", "As short-text topic modelling is a particularly useful method for scholars and data scientist, I have decided to create tweetopic, a Python library which contains highly optimized implementations of topic models for this task.\n", "A topic model, which assumes such a property is the Dirichlet-Multinomial Mixture Model or DMM.\n", "DMM can be thought of as a fuzzy clustering method, as it assumes that every document in the corpus comes from a mixture of dirichlet-multinomial distributions. In such a model topics correspond to the underlying distributions/mixture components.\n", "A DMM can be fitted using MCMC methods. For this purpose Yin and Wang (2014) have published a collapsed sampling algorithm. Their experiments have shown that DMM topics have higher consistency and coherence scores than other clustering methods.\n", "In order to use DMM on your corpus, install tweetopic:\n", "tweetopic is fully compatible with scikit-learn s topic modelling conventions, and as such a topic model consists of a vectorizer and the model itself:\n", "I recommend using scikit-learn s Pipelines as they make your job a lot easier most of the time:\n", "Then you can fit your pipeline to a corpus of texts you have:\n", "Documentation\n", "The Biterm Topic Model or BTM instead of describing the document generation process describes how pairs of words (aka. biterms) in documents are generated from underlying topic distributions.\n", "BTM can capture relations between words in short texts better than other topic models, and can also be efficiently used on corpora containing longer texts, unlike DMM.\n", "Yan et al. (2013) provide a collapsed Gibbs sampling algorithm which has been implemented in tweetopic.\n", "You can plug BTM into your projects the same way as with DMM:\n", "Documentation\n", "Just like with other scikit-learn topic models, the output of the model is the document-topic matrix, and you can access the topic-term matrix of the models on the model.components_ attribute.\n", "For investigating your results I recommend that you try out my other library, topicwizard.\n", "Install it:\n", "Then visualize your models with it:\n", "This will display a web app in a new page in your browser, where you can interactively investigate the relations between words, topics and documents, as well as generate beautiful plots and wordclouds:\n", "For more information visit the documentation.\n", "This great article also includes valuable information for interpreting your models: https://towardsdatascience.com/introduction-to-topic-modeling-using-scikit-learn-4c3f3290f5b9.\n"]}, {"link": "https://medium.com/@princekrampah/langchain-agents-c21eb84528fa?source=list-e28f6edecf84--------2-------7b153c9756d3---------------------", "title": "langLangChain   Agents", "subtitle": "false", "autorName": "Prince Krampah", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*K0tJZ-nblOhECZsmmoTuUw.jpeg", "clap": "24", "response": "1", "timeForRead": "4 min read", "dateCreate": "Jul 18", "text": ["In the previous articles we have delve into routes, we something similar to routes with an added on advantage. If you did not read the article I posted on routes and how we can have multiple prompts and then decide on which prompt to use depending on what task we are tying to complete. Agents provide us with the same functionality only in that they, use a set of tools instead of plain prompts.\n", "An agent can have multiple tools and decide on which tool to use depending on the prompt task required on it. Example there can be a tool that deal with Google searches, querying data bases and then pass on the results of the finding. This is one great advantage of using Agents in LangChain. In this article we ll go over fundamental concepts of LangChain Agents.\n", "There are mainly two types of agents in LangChain. They include the following as stated by the official documentation of LangChain.\n", "Depending on the task at hand, you might choose to use action agent or a plan-and-execute agent. In most cases, using ac compination of the two really works. Action agents are used in simple tasks while plan-and-execute agents are used for complex tasks. When combining the two, plan-and-execute agents can be used for the planning of the whole task while action agents can be used to perform individual tasks within the larget task.\n", "Tools are the actions that an agent can perform while as toolkits are the collection of actions(tools) an agent can perform. Taking this example from the official dos:\n", "An agent is primarily made of three things:\n", "Let s head to building a currency converter agent. This is a sample question I would like my agent to solve:\n", " My monthly salary is 10000 KES, if i work for 10 months. How much is my currency in USD ?\n", "In such an example, the agent will have to first calculate how much I make in KES and then get the current convertion rate and convert the KES to USD.\n", "Second approach is to get how much I make in one month in USD then multiple that value by 10(10 Months).\n", "Things we ll need include:\n", "For ability to search on the internet will need to use a serpapi agent. For ability to perform math calculation we ll need a llm-math agent.\n", "Make sure you have a serapi API key and you have added it to the environment variables as we discussed in th last articel. Not to forget have the serapi Python library installed as well. All in the last article.\n", "Let s good \n", "Now lets prompt the agent with the following\n", "Here is a screenshot of the results:\n", "Let s see what ChatGPT says about the same question. Before I even show you screenshots of ChatGPT, one power of LangChain is the ability to have agents that can connect to your application specific database or a search engine to get upto data current information. An area ChatGPT fails in. Here is a screenshot of what ChatGPT says:\n", "Congratulations for making it this far. In this article you have now seen the use of Agents in LangChain and why you should use LangChain, its advantages over ChatGPT.\n", "If you enjoy this article, and enjoy similar content in video format, kindly consider following me on YouTube @codewithprince\n", "Thank you so much and see you in the next article, do not forget to subscribe to get updates on more of such content.\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-12-563b24abc6a8?source=list-660438a01f7f--------2-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing(Part 13)", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "1", "response": "2", "timeForRead": "7 min read", "dateCreate": "Oct 1", "text": ["output\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "Note:if you are a NLP export and have some good suggestions to improve this blog to share, you write comments and contribute.\n", "if you need more update about NLP and want to contribute then following and enroll in following\n", " Course: Natural Language Processing (NLP)\n", " GitHub Repository\n", "   Notebook\n", "1- Natural Language Processing with Classification and Vector Spaces\n"]}, {"link": "https://medium.com/@dataman-ai/fine-tune-a-gpt-prefix-tuning-13c263e73141?source=list-6a12672b898d--------17-------54fdf6aa16d2---------------------", "title": "Fine-tuning a GPT   Prefix-tuning", "subtitle": "false", "autorName": "Chris Kuo/Dr. Dataman", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*RUtnaV9XF1xtfYj9Pjit-w.jpeg", "clap": "183", "response": "1", "timeForRead": "18 min read", "dateCreate": "Jun 9", "text": ["In this and the next posts, I will walk you through the fine-tuning process for a Large Language Model (LLM) or a Generative Pre-trained Transformer (GPT). There are two prominent fine-tuning methods. One is Prefix-tuning and the other is LoRA (Low-Rank Adaptation of Large Language Models). This post explains Prefix-tuning and the next post  Fine-tuning a GPT   LoRA  for LoRA. In both posts, I will cover a code example and walk you through the code line by line. In the LoRA article, I will especially cover the GPU-consuming nature of fine-tuning a Large Language Model (LLM).\n", "After completing this article, you will be able to explain\n", "Before jumping to fine-tuning a GPT, I want to even clear up some doubts about why fine-tuning is needed. Let s start!\n", "Why do we still need to fine-tune a GPT?\n", "Since GPTs are already trained with various datasets for question answering, text summarization, translation, or classification, why do we still need to fine-tune a GPT? Here is the answer. Consider GPTs as powerful  Transformer  robots (in the Transformers movies) equipped with all sorts of weaponry. The robot needs to be specialized to do certain tasks with domain data. Building a full-functioning real transformer in the Transformer movie (if they ever exist!) is incredibly expensive   likewise building a GPT. Customizing a GPT, or called fine-tuning, will be far less costly.\n", "Are there any challenges in fine-tuning a GPT?\n", "In a very basic form, customizing a GPT means updating all its parameters iteratively to new values so it can do the specialized work. However, most of the LLMs have billions of parameters so the task to update all the parameters is still prohibitively expensive. For example, Google s flan-t5-XXL has 11 billion parameters and the physical file size is more than 100 GB.\n", "Since fine-tuning a GPT is challenging, how can we develop efficient fine-tuning methods? The primary idea of fine-tuning is NOT to touch the billions of pre-trained \n"]}, {"link": "https://medium.com/@ayselaydin/2-stemming-lemmatization-in-nlp-text-preprocessing-techniques-adfe4d84ceee?source=list-9eaefa8b15cb--------1-------35122275c687---------------------", "title": "2  Stemming & Lemmatization in NLP: Text Preprocessing Techniques", "subtitle": "false", "autorName": "Aysel Aydin", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/2*VXqAYEAugqZRy4j-plhkLQ.jpeg", "clap": "202", "response": "2", "timeForRead": "4 min read", "dateCreate": "Oct 11", "text": ["In the previous article, we explained the importance of text preprocessing and explained some of the text preprocessing techniques. Click to read\n", "In this article, we will cover the Stemming & Lemmatization topics.\n", "Stemming and lemmatization are two text preprocessing techniques used to reduce words to their base or root form. The primary goal of these techniques is to reduce the number of unique words in a text document, making it easier to analyze and understand.\n", "They are widely used for Search engines and tagging. Search engines use stemming for indexing the words. Therefore, instead of storing all forms of a word, a search engine may only store its roots. In this way, stemming reduces the size of the index and increases retrieval accuracy.\n", "Let s learn them deeply!\n", "Stemming involves removing suffixes from words to obtain their base form while lemmatization involves converting words to their morphological base form.\n", "Stemming is a simpler and faster technique compared to lemmatization. It uses a set of rules or algorithms to remove suffixes and obtain the base form of a word. However, stemming can sometimes produce a base form that is not valid, in which case it can also lead to ambiguity.\n", "On the other hand, lemmatization is a more sophisticated technique that uses vocabulary and morphological analysis to determine the base form of a word. Lemmatization is slower and more complex than stemming. It produces a valid base form that can be found in a dictionary, making it more accurate than stemming.\n", "Stemming is preferred when the meaning of the word is not important for analysis. for example: Spam Detection\n", "Lemmatization would be recommended when the meaning of the word is important for analysis. for example: Question Answer\n", "Porter & ZemberekPorter stemming algorithm is one of the most common stemming algorithms which is basically designed to remove and replace well-known suffixes of English words.\n", "If you want to do your operations in Turkish, the most common algorithm to find word roots in Turkish is known as  Zemberek . Zemberek is a natural language processing library that can separate word roots and suffixes in accordance with the language structure and morphology of Turkish.\n", "Although the Porter Stemming Algorithm was developed for English texts, it can be adapted to different languages. However, it is more effective to use natural language processing tools and algorithms specifically designed for different languages such as Turkish, as they are not fully adapted to the characteristics of the language.\n", "Zemberek is more successful in understanding and processing the rich morphological structure of Turkish and therefore gives better results on Turkish texts. Therefore, it is more common to choose language-specific tools such as Zemberek for language processing and root-finding tasks for Turkish.\n", "Let s see how it works Porter stemming algorithm:\n", "Output:\n", "Now let s consider the topic of  Lemmatization \n", "In our lemmatization example, we will be using a popular lemmatizer called WordNet lemmatizer.\n", "WordNet is a word association database for English and a useful resource for English lemmatization. However, there is no direct equivalent of this source in Turkish, and language-specific tools such as Zemberek are more suitable for the lemmatization of Turkish texts.\n", "As I mentioned above, I will discuss the subject of  Zemberek  in more detail in another article.\n", "Let s code and apply Lemmatization.\n", "Output:\n", "To summarize, stemming and lemmatization are methods that help us in text preprocessing for Natural Language Processing. They both aim to reduce inflections down to common base root words, but each takes a different approach to doing so.\n", "In some cases, stemming may produce better results than lemmatization, while in other cases, lemmatization may be more accurate. Therefore, it is essential to weigh the trade-offs between simplicity, speed, and accuracy when selecting a text normalization technique.\n", "I hope it will be a useful article for you. Happy coding  \n", "Contact Accounts: Twitter, LinkedIn\n"]}, {"link": "https://medium.com/@cees-roele/a-term-score-matrix-for-bertopic-821e78e198ee?source=list-6a12672b898d--------29-------54fdf6aa16d2---------------------", "title": "A Term Score Matrix for BERTopic", "subtitle": "false", "autorName": "Cees Roele", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*TEwhJUNB4hkwxEv8Wx4EVg.jpeg", "clap": "124", "response": "1", "timeForRead": "6 min read", "dateCreate": "Oct 25, 2022", "text": ["An improved visualisation of term scores for topics with BERTopic\n", "This article demonstrates a Term Score Matrix, a visualisation of pairs of terms and scores characterising topics in BERTopic. Focus is on functionality. You can find a link to the notebook containing the implementation of the stylised dataframe for the Term Score Matrix at the bottom of this article.\n", "The used dataset is based on the over 14,000 reports on  pro-Kremlin disinformation cases  at the website EU vs Disinformation. This reflects political contention between the EU and Russia over the past seven years. You can find more information on the dataset and on topic modelling with BERTopic of this dataset in the following article:\n", "Based on a notion of distance calculation, the process of clustering defines a number of clusters and assigns each sample of the dataset to a cluster.\n", "Assuming we have assigned our dataset to a variable docs, we can model topics with BERTopic by:\n", "The result is a set of thirty clusters with a size of at least 100 samples. Here are the resulting topics with the largest frequencies:\n", "Topic modelling applies clustering to linguistic samples and provides a characterisation of the resulting clusters as a ranked list of terms with their c-TF-IDF scores. We see an example in the table below. Note that the name of the topic   above the table   is generated on the basis of the topic id   here 2  and the highest ranking terms.\n", "Having names for topics based on relevant terms helps identifying and understanding the different topics. But what are we otherwise to do with the list? Just considering the sequence of terms gives us limited understanding of the relevance of these terms to the topic.\n", "The Term score decline diagram gives us insight in how the scores of terms decrease to a level where their influence is hardly distinguishable from that of other terms.\n", "For the illustration below we have configured BERTopic to create term-scores for twenty terms by initialising it with top_n_words=20. For most topics, we see a steep decline of the c-TF-IDF score from the term ranked first to the term ranked third. After the third rank the scores are gradually flattening with increasing rank.\n", "To get this diagram, we run: topic_model.visualize_term_rank() The annotations in red are manually added.\n", "Only by using tooltips in BERTopics plotly-based diagram can we see what line in the diagram corresponds to what topic (see the little arrow on the left of the tooltip in the image). At the eleventh ranked term we see that topics 29_kerch_strait and 12_MH17_JIT stand out by having higher c-TF-IDF values than the others. Their values at this term are even higher than the scores for the first items of several other topics.\n", "The Term Score Decline diagram helps us decide whether we can cut off the number of terms we want to distinguish. E.g. we might consider that only the top 11 terms are important enough for us to consider. But how to deal with the outliers, like the two topics having relatively high c-TF-IDF values even at the eleventh rank?\n", "Let s take a closer look at these topics and their terms. We use BERTopic s visualize_barchart() to take a closer look at these topics and their terms. In the following diagram we look at the 15 highest ranked terms and include two other topics as reference.\n", "Ignoring arguments for setting the title, height, and custom labels, we run: topic_model.visualize_barchart(topics=[0,29,12,30], n_words=12)\n", "We see   with added annotation and a tooltip   the ranked terms with their relative scores for four topics, including the two we saw standing out in the Term decline score diagram. Looking at the terms we can understand why the scores for these topics decline relatively little, that is, they keep standing out compared to other topics: both involve specific events.\n", "Topic 29 is about the waterway Kerch Strait and we see terms like sea, waters, vessels, boats, crews, and ships. Topic 12 is about the downing of the airplane MH17 and we see terms like BUK (anti-aircraft missile), missile, JIT (Joint Investigation Team), flight, crash, downing.\n", "Above we saw that the bar chart can help us answer the question:  What are the highest ranking terms of any topic and how do their scores decline? \n", "But it doesn t show us the other direction: terms for any topic by rank. First, because the scales of the different bar charts are different, as indicated by the annotation in the image. Second, because the length of bars in sequence   rather than in parallel   is hard to compare visually. Comparison in the multiple bar chart layout works only in one dimension: between the scores of terms of one topic in one bar chart.\n", "Additionally, the bar chart uses space to represent a scalar value. If we would represent all thirty topics in a layout of multiple bar charts, it would take us eight rows of up to four bar charts.\n", "We can address these drawbacks of the bar chart representation by having colour represent the score, similar to a heatmap.\n", "We can use a pandas DataFrame to do this for the same topics as we displayed in the bar chart above:\n", "We see that now our ability to compare is two-dimensional: we can horizontally compare relative value of ranked terms for one topic   as in the bar chart   and vertically the relative value for any topic of a specific rank. As with BERTopic s plotly-based diagrams, we display a tooltip with the c-TF-IDF value   in the image with the example for boat.\n", "Note that the bar chart allows for infinite extension of the number of terms as it stacks them vertically, and here we are restricted to the width of the screen. We are displaying only eight ranks instead of fifteen.\n", "Let s look at the Term Score Matrix for all topics now. Colours may vary as long as they indicate the scale. In the next image we use a colour scheme of yellow, orange, and red. As the whole image is zoomed out, we can now include fifteen ranks.\n", "Much more information than we get from the multiple bar chart layout fits into this single matrix and it is much easier to compare values in all directions.\n", "The Term Score Matrix is a space-efficient diagram representing c-TF-IDF scores for terms along the two dimensions of topics and ranks. It enables us to quickly identify term decline for topics and what terms are prevalent in topics.\n", "A Jupyter notebook with the python code used in this article is available at github.\n", "You can find a simple explanation of the styling technique behind the Term Score Matrix in another article of mine:\n"]}, {"link": "https://medium.com/@cobusgreyling/the-conversational-ai-technology-landscape-version-5-0-01f91c66af6d?source=list-2eb23a991a63--------13-------0a856388a93a---------------------", "title": "The Conversational AI Technology Landscape: Version 5.0", "subtitle": "This market map originated as chatbot development framework focussed research. Subsequently it started following the expansion of all related technology into areas like Voicebots, testing, NLU tooling and more ", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "110", "response": "1", "timeForRead": "3 min read", "dateCreate": "false", "text": ["This is most probably the last time I will be updating this chart due to a number of market shifts and developments and the chatbot / voicebot market being fragmented.\n", "Traditionally the chatbot architecture and tooling were very much settled around four pillars; intents, entities, dialog flows and response messages.\n", "This has been disrupted by the advent of voice and large language models.\n", "The market requirement to automate voice calls originating from a telephone call placed to a contact centre, necessitated chatbot vendors to branch out into voice.\n", "Voice as a medium demands two pieces of highly specialised technology; Automatic Speech Recognition (ASR, Speech To Text, STT) and Speech Synthesis (Text To Speech, TTS). The voice focus also shifted away from dedicated devices like Google Home and Alexa, to automated telephone calls.\n", "This change in focus added complexity in the form of higher value conversations, calls with higher consequences, longer conversations with more dialog turns and complex conversation elements like self-correction, background noise and more.\n", "Large Language Models also disrupted the ecosystem in two stages.\n", "The first stage was during chatbot/voicebot development; starting with adding efficiency to the NLU development process in terms of generating training data for intents, detecting named entities, etc.\n", "An obvious step was using LLMs for copy writing and generating and vetting responses.\n", "This developed into the process of describing a flow, and the framework generating a flow, with variables, responses and more.\n", "The introduction of LLMs at design time was a safe avenue in terms of the risk of customer facing aberrations or UX failures. It was also a way to mitigate cost and spending and not face the challenges of customer and PII data being sent into the cloud.\n", "The second stage of LLM disruption was at run time, where LLMs were used for Search assistants, and highly contextual conversations via RAG implementations.\n", "Read more on LLM disruption and other factors in a follow-up post \n", "  Follow me on LinkedIn for updates on Large Language Models  \n", "I m currently the Chief Evangelist @ Kore AI. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-4-1ba37434f33b?source=list-660438a01f7f--------13-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing (Part 4)", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "1", "response": "2", "timeForRead": "4 min read", "dateCreate": "Jul 30", "text": ["Python Notbook\n", "In this Tutorial, you re going to learn how to represent a text as a vector. In order for you to do so, you first have to build vocabulary and that will allow you to encode any text or any tweet as an array of numbers.\n", "def:In the context of natural language processing (NLP), vocabulary refers to the set of unique words that appear in a text corpus.The vocabulary is used to represent text in a machine-readable format. For example, if the vocabulary contains 10,000 words, then each text document can be represented as a vector of 10,000 numbers, where each number represents the frequency of a particular word in the document.\n", "The vocabulary is an important part of NLP because it allows us to represent text in a way that can be processed by computers. Without a vocabulary, it would be very difficult to develop NLP algorithms that can understand and process natural language.\n", "There are a few different ways to create a vocabulary for NLP. One common approach is to use a statistical method to select the most frequent words in a text corpus. Another approach is to use a dictionary of words that are considered to be important for a particular task.\n", "So let s dive in and see how you can do this. Picture a list of tweets, visually it would look like this. Then your vocabulary, V, would be the list of unique words from your list of tweets. To get that list, you ll have to go through all the words from all your tweets and save every new word that appears in your search. So in this example, you d have the word I, then the word, am and happy, because, and so forth. But note that the word I and the word am would not be repeatedin the vocabulary.\n", "Let s take these tweets and extract features using your vocabulary. To do so, you d have to check if every word from your vocabulary appears in the tweet. If it does like in the case of the word I, you would assign a value of 1 to that feature, like this. If it doesn t appear, you d assign a value of 0, like that.\n", "In this example, the representation of your tweet would have six ones and many zeros. These correspond to every unique word from your vocabulary that isn t in the tweet.\n", "Now, this type of representation with a small relative number of non-zero values is called a sparse representation. Now let s take a closer look at this representation of these tweets. In the last slides, I walked you through extracting features to represent the tweet based on a vocabulary and I arrived at this vector.\n", "This representation would have a number of features equal to the size of your entire vocabulary. This would have a lot of features equal to 0 for every tweet. With the sparse representation, a logistic regression model would have to learn n plus 1 parameters, where n would be equal to the size of your vocabulary and you can imagine that for large vocabulary sizes, this would be problematic. It would take an excessive amount of time to train your model and much more time than necessary to make predictions.\n", "Given a text, you learned how to represent this text as a vector of dimension V. Specifically, you did this for a tweet and you were able to build a vocabulary of dimension V. Now as V gets larger and larger, you will face certain problems. In the next video, you will learn to identify these problems.\n", "Please Follow coursesteach to see latest updates on this story\n", "If you want to learn more about these topics: Python, Machine Learning Data Science, Statistic For Machine learning, Linear Algebra for Machine learning Computer Vision and Research\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "1- Natural Language Processing with Classification and Vector Spaces\n", "2-Vocabulary & Feature Extraction\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-5-c42cb5265534?source=list-660438a01f7f--------11-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing (Part 5)", "subtitle": " Chapter 2: Sentiment Analysis (Logistic Regression):Negative and Positive Frequencies", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "1", "response": "2", "timeForRead": "4 min read", "dateCreate": "Aug 6", "text": ["What is the frequency dictionary (Bar)\n", "A frequency dictionary in NLP is a list of all the unique words occurring in a corpus, along with their frequencies. The frequency of a word is the number of times it appears in the corpus. Frequency dictionaries are used in a variety of NLP tasks, such as:\n", "Frequency dictionaries can be created manually or automatically. Manually created frequency dictionaries are created by counting the frequency of each word in a corpus. Automatic frequency dictionaries are created using statistical techniques.\n", "We ll now learn to generate counts, which you can then use as features in your logistic regression classifier. Specifically, given a word, you want to keep track of the number of times, that s where it shows up as the positive class. Given another word you want to keep track of the number of times that word showed up in the negative class. Using both those counts, you can then extract features and use those features into your logistic regression classifier.\n", "So let s take a look at how you can do that. It is helpful to first imagine how these two classes would look.\n", "Here for instance, you could have a corpus consisting of four tweets. Associated with that corpus, you would have a set of unique words, your vocabulary. In this example, your vocabulary would have eight unique words.\n", "For this particular example of sentiment analysis, you have two classes. One class is associated with positive sentiment and the other with negative sentiment. So taking your corpus, you d have a set of two tweets that belong to the positive class,and the sets of two tweets that belong to the negative class.\n", "Let s take the sets of positive tweets. Now, take a look at your vocabulary. To get the positive frequency in any word in your vocabulary, you will have to count the times as it appears in the positive tweets. For instance, the word happy appears one time in the first positive tweet, and another time in the second positive tweet. So it s positive frequency is two. The complete table looks like this. Feel free to take a pause and check any of its entries.\n", "The same logic applies for getting the negative frequency. However, for the sake of clarity, look at some examples, the word am appears two times in the first tweet and another time in the second one. So it s negative frequency is three. Take a look at the entire table for negative frequencies and feel free to check its values.\n", "So this is the entire table with the positive and negative frequencies for your corpus. In practice when coding, this table is a dictionary mapping from a word class there to its frequency. So it maps the word and its corresponding class to the frequency or the number of times that s where it showed up in the class. You now know how to create a frequency dictionary, which maps a word and the class to the number of times that word showed up in the corresponding class.\n", "Please Follow coursesteach to see latest updates on this story\n", "If you want to learn more about these topics: Python, Machine Learning Data Science, Statistic For Machine learning, Linear Algebra for Machine learning Computer Vision and Research\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "1- Natural Language Processing with Classification and Vector Spaces\n", "2-Negative and Positive Frequencies\n"]}, {"link": "https://medium.com/@Coursesteach/best-free-resources-to-learn-nlp-d7b6be97ba10?source=list-660438a01f7f--------12-------dbbdeca8bd9e---------------------", "title": "Best Free Resources to Learn NLP", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "137", "response": "2", "timeForRead": "2 min read", "dateCreate": "Jul 30", "text": ["In this blog post, we will explore the best free resources available online to learn Natural Language Processing (NLP). Natural Language Processing is a subfield of artificial intelligence that focuses on the interaction between computers and human language. Whether you are a beginner looking to get started with NLP or an experienced practitioner searching for advanced learning materials, this comprehensive list of free resources will help you gain the knowledge and skills needed to excel in NLP.\n", "1-Gluon-NLP:GluonNLP is a toolkit for natural language processing (NLP) built on top of MXNet. It provides a number of features that make it easy to build and deploy NLP models.\n", "2-Coursesteach: it is free resource share platform\n", "3-The Super Duper NLP Repo: it contains colab notebook about different nlp implementationimplementation\n", "1-Natural Language Processing Specialization:it is a four-course specialization offered by Coursera. It is designed to teach you the fundamentals of natural language processing (NLP) using Python\n", "1-awesome-nlp: A curated list of resources dedicated to Natural Language Processing\n", "2- DataSpoof:Learn Data Science, Big data, Machine learning, Deep learning, NLP and many more\n", "3-SkalskiP/courses\n", "4-ML-University:Machine Learning Open Source University is an IDEA of free-learning of a ML enthusiast for all other ML enthusiast.\n", "1-LLM University :provides a great course and resources for learning about Natural Language Processing and Large Language Models\n", "1-Datawolrd\n", "Please Follow coursesteach to see latest updates on this story\n", "If you want to learn more about these topics: Python, Machine Learning Data Science, Statistic For Machine learning, Linear Algebra for Machine learning Computer Vision and Research\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.Stay tuned for our this blog , you will find new website, Course where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "Note:if you are a NLP export and have some good suggestions to improve this blog to share, you write comments and contribute.\n"]}, {"link": "https://medium.com/@cobusgreyling/automatic-prompt-engineer-ui-aa7dd0770dd1?source=list-e28f6edecf84--------5-------7b153c9756d3---------------------", "title": "Automatic Prompt Engineer UI", "subtitle": "The Automatic Prompt Engineer (APE) UI demonstrates how prompts can be optimised for text generation.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "33", "response": "1", "timeForRead": "4 min read", "dateCreate": "Sep 29", "text": ["I m currently the Chief Evangelist @ HumanFirst. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n", "APE takes as input a dataset with a list of inputs and outputs and a prompt template.\n", "Then APE uses a language model to generate a set of possible prompts. APE also has a prompt evaluation function to evaluate the quality of each generated prompt.\n", "APE shows promise and the concept is novel, and can expand, but the following needs to be noted:\n", "Considering the image below, when you click on tasks, different datasets are loaded for prompt generation and scoring.\n", "After the dataset is loaded, an estimate of cost can be generated prior to committing the evaluation.\n", "Under configuration there are basic and advanced configuration tabs.\n", "After running the APE functionality, the results are shown below, with the scores.\n", "In conclusion, the APE GUI is a convenient way to play around with datasets to create and tweak prompts.\n", "However, in a follow-up post I will be considering a recent study, which highlighted the challenges and complexities of prompt engineering. And how an AI accelerated latent-space can assist with that.\n", "  Follow me on LinkedIn for updates on Large Language Models  \n", "I m currently the Chief Evangelist @ HumanFirst. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}]