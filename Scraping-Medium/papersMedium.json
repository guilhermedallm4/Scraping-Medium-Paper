[{"link": "https://medium.com/@indusnet/what-to-know-about-semantic-search-using-nlp-be387c688ec9?source=list-2c27d980d3f3--------24-------338c7da11cbf---------------------", "title": "What To Know About Semantic Search Using NLP", "subtitle": "false", "autorName": "Indus Net Technologies", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*xfG8KptyoIUcr_1DOl2BcQ.png", "clap": "6", "response": "1", "timeForRead": "4 min read", "dateCreate": "Jan 12, 2022", "text": ["Have you used your application or search engine to understand the underlying meaning behind your query? If yes, the solution to this requirement is through Semantic Search. A couple of years ago, a simple keyword search would have yielded search results matching just the keywords. We call it  lexical search . Today, we can have machines and applications understand the semantics behind a query through Natural Language Processing (NLP). The credit goes to the Artificial Intelligence revolution.\n", "Let s say you search the nursery rhyme,  Hey Diddle Diddle  on Google. And the search results will return both lexical and semantic instances of it. The former is an example of computational information retrieval below semantic search. So, we can say that  Semantic search describes a search engine s attempt to generate the most accurate Search Engine Results Page (SERP) results possible by understanding based on searcher intent, query context, and the relationship between words. \n", "Through the superset of machine learning, we have the following abilities today:\n", "Natural Language Processing (NLP): It is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language. It can be further divided into 3 fields:\n", "The above machines help to  comprehend  both intent and context of human communication. Imagine the positive impact that this emerging technological paradigm has had on global trade, academics, scientific research, and culture. After all, there are over 6,500 human natural languages all over the world!\n", "The best part of this technology is that both speech and text can use it. However, we would stick to the dynamics of semantic search alone. It involves a pre-processing data stage called text processing. This allows the understanding and processing of large amounts of text data. It is the process of analyzing textual data into a computer-readable format for machine learning algorithms.\n", "A language model is a tool to incorporate concise and abundant information reusable in an out-of-sample context by calculating a probability distribution over words or sequences of words.\n", "The problem of NLP cannot be explained without citing BERT (Bidirectional Encoder Representations from Transformers) as an example of a state-of-the-art pre-trained language model. The bidirectional encoder representations from transformers can answer more accurate and relevant results for semantic search using NLP. Jacob Devlin created a well-known state-of-the-art language model in 2018. And Google leveraged in 2019 to understand user searches.\n", "There are many open-source frameworks for solving NLP problems such as NLTK, GPT3, and spaCey. We at INT. use those frameworks for engineering NLP-driven software.\n", "GPT3 (Generative Pre-trained Transformer- think GAN of NLP) was a wonder framework released in 2020 by OpenAI. It has the power to thrill and scare people due to its accuracy in mimicking human natural language. It used a transformer, which is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. NLP and computer vision (CV) primarily use the GPT3 framework. Its ability to differentially weight features works out terrifically for us as the model can discern different words in a sample. Also, it can assign probabilities of them occurring in the past, present, and future.\n", "Language models such as BERT need a truly humongous amount of data in the targeted language to fine-tune its general understanding of the language. Data engineering is an absolute need for the accuracy of a language model. Crowdsourcing is one such strategy to get abundant data.\n", "The other way is to have an application/algorithm crawl through targetted or available resources on the internet.\n", "Lastly, companies specializing in the required data for NLP can provide data for purchasing.\n", "Source: https://www.indusnet.co.in/what-to-know-about-semantic-search-using-nlp/\n"]}, {"link": "https://medium.com/@cees-roele/detecting-persuasion-with-spacy-6b6beba51076?source=list-a13ace4f182c--------37-------f7e9b3597071---------------------", "title": "Detecting Persuasion with spaCy", "subtitle": "false", "autorName": "Cees Roele", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*TEwhJUNB4hkwxEv8Wx4EVg.jpeg", "clap": "124", "response": "1", "timeForRead": "10 min read", "dateCreate": "Jun 7, 2022", "text": ["Persuasion techniques express shortcuts in the argumentation process, e.g., by leveraging on the emotions of the audience or by using logical fallacies to influence it. In this article we will create a spaCy pipeline with a SpanCategorizer to detect and classify spans in which persuasion techniques are used in a text.\n", "Our training data identifies 20 categories. Spans may overlap, that is, a word can be part of different spans.\n", "Here is a partial example:\n", "We will train different models with the dataset, use different spaCy configurations, and compare the results.\n", "Table of Contents:\n", "Unwarranted reasoning goes by different names. Philosophers talk of fallacies, psychologists focus on manipulation, political scientists speak of propaganda, and linguists interested in the venerable tradition of rhetorics address persuasion. Each domain has its own focus on what the relevant impact of unwarranted reasoning is.\n", "Detecting and explaining unwarranted reasoning might require epistemology, logic, estimation of intention, psychological biases, knowledge of pre-existing narrative, and even physical context. As all this doesn t fit a feasible machine learning problem description, we convert unwarranted reasoning into a problem of classification: given a set of categories and a dataset of texts with marked spans belonging to categories, we train a model to detect such spans and classify them. We call these categories persuasion techniques.\n", "Different studies have come up with different sets of persuasion techniques, e.g. ranging from a single classification of a whole text as  propaganda  to distinguishing 69 different techniques.[4] Presently, we will not discuss whether some categorisation is  better  than another or whether any categorisation is fit for purpose. Here we adopt one set of twenty techniques, with the understanding that a different set would be possible.\n", "Let s look at the description of some of the techniques described in [1]:\n", "These techniques are described in the context of subtask 2 of SemEval-2021 Task 6:  Detection of Persuasion Techniques in Texts and Images  [1].\n", "The present article focuses on the implementation of a system detecting and classifying spans using the spaCy SpanCategorizer and will pay only cursory attention to the meaning of techniques and to the used dataset. For detailed information on the dataset and on the meaning of the different techniques into which we classify spans, please check out the mentioned article.\n", "The dataset we use was created for SemEval-2021 task 6  Detection of Persuasion Techniques in Texts and Images . It can be found on github [3].\n", "The dataset consists of a total of 951  memes , short texts taken from social media posts, in which 2083 spans with persuasion techniques are identified by a team of annotators. The texts are overlaid on an image   hence they contain numerous line breaks  and many are written in uppercase. We ignore the images.\n", "Here is an example:\n", "Persuasion spans in this text:\n", "Below the overview of persuasion techniques distinguished in the dataset with for each the number of occurrences in the dataset and the average number of tokens in a span. To determine the number of words in a span the standard spaCy tokeniser was used. Note that interpunction and whitespace tokens are included in the counting.\n", "In total there are 2083 spans.\n", "Looking at the table you see:\n", "Generally:\n", "Defining spans is like taking a coloured marker and highlighting a fragment of the original text. As we need exact fragments, we will not modify the original text by pre-processing it in any way.\n", "Our dataset is already divided into train, dev, and test parts. We convert each of these files separately into a binary .spacy file which is used as input for training.\n", "As it is an open question what base model would best fit our requirements we will try small, large, and transformer models and compare the results.\n", "Once our corpus is defined we can start a training using the spacy train command. For readability and repeatability we define this in a spaCy project. You can find an example of a regular pipeline consisting of corpus, train, and evaluate steps in the project.yml of my article  Detecting Toxic Spans with Spacy .\n", "Such a pipeline makes fine-tuning training of one model easy. Here we will not tune, but instead use three basis models and see how they perform. To define their configuration we use the  Quickstart  dialog in the spaCy documentation.\n", "We select the spancat component and generate three configurations:\n", "This will result in small, large, and transformer models. The default spaCy transformer model is RoBERTa-base.\n", "Using default configuration values, we will train each of these models and compare the results.\n", "To detect spans, spaCy first generates a set of possible spans for a document. This is done by a component named Suggester.\n", "SpaCy 3.3 comes with two implementations of Suggesters, both based on generating n-grams, that is, spans of n tokens. The ngram_suggester is configured with a list of lengths of n-grams, e.g. [1, 2, 3, 4]. The ngram_range_suggester is configured with a minimum and maximum of a range of lengths, e.g. min_size=1, max_size=4.\n", "Named entities typically consist of only a few tokens. With a token-length of 5 the named entity  Berlin Brandenburg Airport Willy Brandt  is relatively long. In our current dataset, however, we deal with spans that might even range across sentences. Here is an example fragment from our dataset of the category Causal Simplification:  Childish Trump Won t Meet With Pelosi On Coronavirus\\nBecause He Doesn t Like Her . SpaCy tokenises this fragment into 15 tokens, including one for the newline, where  won t  is broken up into [ wo ,  n t ].\n", "As training, evaluation, and prediction of any span can only succeed if it doesn t contain more tokens than generated by the Suggester, we must look into our dataset and see how many samples are cut off.\n", "In the table below, we take 8-grams, 16-grams, and 32-grams as maximums for the ngram-range-suggester.\n", "We apply some markup based on arbitrary limits:\n", "What we see:\n", "We will train with suggesters with maximums of 16-grams and 32-grams. We will ignore a suggester with a maximum of 8-grams as this looks unpromising given that so many techniques are not covered satisfactorily by that suggester.\n", "Looking at F1 scores for the n-gram suggester set to maximums of 16 and of 32 tokens we find:\n", "As expected, the large (lg) model does better than the small (sm) model and the transformer (trf) model does better than the large model. Surprisingly, however, the small and large models do worse with 32-grams than with 16-grams. Also we find that the transformer model is doing significantly better with 32-grams than with 16-grams.\n", "In the table below the data for the F1 score used in the diagram above plus precision and recall measure. Values that have decreased for the 32-grams suggester are marked in red.\n", "What we see is that for the small and large models the recall decreases for the 32-gram suggester, but the precision increases.\n", "This means that the small and large models label too many tokens that shouldn t be labelled for the 32-gram suggester, although they do cover more of those that should be labelled. Let s call is over-optimistic labelling. The transformer models don t suffer from that defect.\n", "It would be interesting to determine why this is happening, but that would require further research.\n", "Let s look at the F1 scores for each individual persuasion technique:\n", "We see:\n", "Created models should be able to predict spans for different classes that overlap each other. However, what happens   and what should not happen   is that there are predictions of spans with the same label that overlap each other. The following image illustrates the prediction of a 32-grams transformer-based model.\n", "I consider this an error of the SpanCategorizer.\n", "Metrics are merely numbers unless serving to compare different systems producing them. As we effectively implemented subtask 2 of Task 6 of SemEval 2021, we can compare our outcome with other systems on the leaderboard for that task. SpaCy uses token-based metrics, but the mentioned contest uses character-based metrics.\n", "Using the best model to produce a prediction for the test dataset and having the result evaluated by the scorer method provided for the contest leads to a character-based F1 of 0.449. That would place it second in the Task 6 ranking published in [1]!\n", "This encourages future research comparing the architecture of the spaCy model-with-suggester with the models participating in Task 6.\n", "We have seen that spaCy s SpanCategorizer can be used to detect spans and classify them. As the dataset used for the present article contains spans of widely varying length, we needed to take the functionality and configuration of the spaCy Suggester into account, which is the function generating spans. For this dataset transformer models proved significantly more accurate than spaCy s small and large models. The resulting model ranked well within alternative systems for detecting and classifying spans.\n", "[1]  SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and Images  (2021) D. Dimitrov et al\n", "[2]  WVOQ at SemEval-2021 Task 6: BART for Span Detection andClassification  (2021) Cees Roele\n", "[3]  Data for SemEval-2021 Task 6: Detection of Persuasive Techniques in Texts and Images , github\n", "[4]  Fine-Grained Analysis of Propaganda in News Articles  (2019) G. Da San Martino et al\n"]}, {"link": "https://medium.com/@cobusgreyling/llms-contextual-demonstration-af99de936cf0?source=list-e28f6edecf84--------13-------7b153c9756d3---------------------", "title": "LLMs & Contextual Demonstration", "subtitle": "Large Language Models are able to learn in-context via a number of examples which acts as demonstrations, also referred to as few-shot learning. In the recent past, there has been little understanding on how models learn from few-shot, in-context demonstrations & what part of the demonstration is the most important to performance.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "65", "response": "9", "timeForRead": "4 min read", "dateCreate": "false", "text": ["With human language in general and LLMs in specific, context is of utmost importance. When a few-shot learning approach is followed via Prompt Engineering, a contextual reference is established for the LLM to serve as an input specific contextual reference.\n", "A recent study explored how models learn and which aspects contribute most to the tasks and performance. The study found that the key drivers of few-shot training are:\n", "The analysis creates a new understanding of how in-context learning works and challenges notions of what can be achieved at inference alone.\n", "Considering the image below, the performance of two use-cases are shown across three models. The model performance where no demonstration is given, varies quite a bit.\n", "Considering the no demonstration performance of GPT-J for a moment the GPT-J model can be access via a few playgrounds; generally casual users are disappointed with the model s performance.\n", "However, consider the boost in performance with gold and random labels are used at inference. This goes to show that apart from fine-tuning, implementing an accurate and succinct prompt engineering pipeline can boost the performance of LLMs.\n", "This finding has implications for local installations of smaller models which are open-sourced; models which are often deemed not good enough when being casually inspected.\n", "However, when the principles detailed here are followed and as seen in the graph below, exceptional performance can be extracted from models.\n", "This approach can solve for cost, data privacy, corporate governance requirements and more; considering smaller models can be made use of.\n", "Below the impact of the distribution of the inputs are shown, notice the disparity between Direct & Channel. The direct model exploits the label space better than the input distribution, and the channel model exploits the input distribution better than the label space.\n", "Below is a good breakdown of practical examples where format, input distribution, label space and input-label mapping are experimented with.\n", "It needs to be noted these experiments were limited to classification and multi-choice tasks.\n", "Any study on how to optimise inference data for other tasks like completion, editing and chat will immensely useful.\n", "For few-shot text classification. Instead of predicting the label from input; like intent detection works; channel models compute the conditional probability of the input given the label.\n", "Direct prompting is shown very clearly in the image below. And I do get the sense that direct inference is more widely used as opposed to channel; while there are studies showing that channel outperforms direct.\n", "  Follow me on LinkedIn for updates on Large Language Models  \n", "I m currently the Chief Evangelist @ Kore AI. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}, {"link": "https://medium.com/@cobusgreyling/react-synergy-between-reasoning-acting-in-llms-36fc050ae8c7?source=list-e28f6edecf84--------3-------7b153c9756d3---------------------", "title": "ReAct: Synergy Between Reasoning & Acting In LLMs", "subtitle": "An element of human intelligence is the ability to seamlessly combine task-oriented actions with verbal or inner speech. This inner speech plays an important role in human cognition and enables self-regulation and strategising.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "12", "response": "1", "timeForRead": "5 min read", "dateCreate": "Jun 28", "text": ["I m currently the Chief Evangelist @ HumanFirst. I explore and write about all things at the intersection of AI and language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces and more.\n", "With humans the tight synergy between reasoning & acting allows for humans to learn new tasks quickly and perform robust reasoning and decision making. We can perform this even when unforeseen circumstances, information or uncertainties are faced.\n", "LLMs have demonstrated impressive results in chain-of-thought reasoning (CoT) and prompting, and acting (generation of action plans).\n", "The idea of ReAct is to combine reasoning and taking action.\n", "Reasoning enables the model to induce, track and update action plans, while actions allow for gathering additional information from external sources.\n", "Combining these to ideas are named ReAct, and it was applied to a diverse set of language and decision making tasks to demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness.\n", "  Please follow me on LinkedIn for updates on Conversational AI  \n", "According to the study, ReAct overcomes issues of hallucination and error cascading of CoT reasoning by interacting with a knowledge source like Wikipedia. Human-like task-solving trajectories are generated.\n", "As seen below in the sequence of events of a ReAct based Agent, reasoning traces makes the final result of the LLM more interpretable with various references along the thought process.\n", "Below is an example of a ReAct agent implemented via LangChain. Consider the following complex question:\n", "Author David Chanoff has collaborated with a U.S. Navy admiral who served as the ambassador to the United Kingdom under which President?\n", "The first thought of the LLM Agent is to search David Chanoff and determine the U.S. Navy admiral he collaborated with. Followed by determining the U.S. President:\n", "Here is the complete code to run the ReAct agent based on OpenAI, Wikipedia and LangChain:\n", "And the output from the agent:\n", "A lot has been said about chain of thought reasoning and promting and reasoning.\n", "The fact that many LLMs have a set time stamp and time cutoff in terms of general knowledge is also impacts LLMs negatively.\n", "Having an external data source like Wikipedia plays a big role in the LLM agent being able to take action.\n", "  Please follow me on LinkedIn for updates on Conversational AI  \n", "I m currently the Chief Evangelist @ HumanFirst. I explore and write about all things at the intersection of AI and language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces and more.\n"]}, {"link": "https://medium.com/@albertoromgar/openai-could-lose-its-ai-lead-to-google-for-the-first-time-in-4-years-c611fe5d85d4?source=list-2eb23a991a63--------5-------0a856388a93a---------------------", "title": "OpenAI Could Lose Its AI Lead to Google For the First Time in 4 Years", "subtitle": "The AI race is now at a tipping point", "autorName": "Alberto Romero", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/2*oMdIZBsnK8EFhQLUaAB5ZA.jpeg", "clap": "82", "response": "3", "timeForRead": "2 min read", "dateCreate": "false", "text": ["It was in 2019 that OpenAI released GPT-2, surpassing Google in the race to create better generative AI models.\n", "Will Google retake the AI throne with Gemini before 2023 ends?\n", "The Information scooped yesterday that in mid-2023 OpenAI had to stop working on a new model, codenamed Arrakis, that would presumably make ChatGPT run more efficiently.\n", "This was the company s main ongoing development after finishing GPT-4 in the summer of 2022.\n", "Why did they stop? Because the model didn t work as expected. In a space where a gap of a few months is the difference between being the leader or not, this was an important setback for OpenAI.\n", "The young startup is more than fine   this isn t a life-or-death situation. It s making $1.3 billion in ARR, has been releasing new juicy models like GPT-4 vision, DALL-E 3, etc., and has more aces up the sleeve for the DevDay conference on November 6th.\n", "But this  failure  (I guess we can say that if we compare it with the flawless 4-year run it s had so far!) may allow Google to surpass OpenAI with Gemini, which is posited to beat GPT-4.\n", "If that happens, we can take away two insights from all this:\n", "No one knows which steps are safer or more promising on our way toward AGI. OpenAI s  aura of invincibility  as The Information puts it, was an outlier feature that lasted much more than anyone expected.\n", "AGI is somewhere ahead of us, awaiting patiently, but the path there is full of obstacles we must overcome. OpenAI couldn t get its next release on time and that s both illuminating and humbling.\n", "OpenAI is seen by everyone as the favorite candidate. If it doesn t manage to overcome this hurdle and maintain its leadership or release soon something much better than GPT-4, people will cease to praise it so fervently.\n", "They claimed Google dead after ChatGPT was announced   who knows if now they will predict OpenAI s death. OpenAI will have to solve its technical problems with the model, its business pressures with Microsoft, and its social complaints with the general public.\n", "What do you think, will OpenAI keep or lose its leadership before 2023 ends?\n", "This article is a selection from The Algorithmic Bridge, an educational newsletter to bridge the gap between AI, algorithms, and people. It will help you understand the impact AI has in your life and develop the tools to better navigate the future.\n", "You can also become a Medium member and support my work here.\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-8-5435d573d660?source=list-660438a01f7f--------8-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing (Part 8)", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "1", "response": "2", "timeForRead": "3 min read", "dateCreate": "Aug 27", "text": ["You will now use everything that you learned to create a matrix that corresponds to all the features of your training example. Specifically, I will walk you through an algorithm that allows you to generate this x matrix.Let s take a look at how you can build it.\n", "Previously, you saw how to preprocess a tweet like this one to get a list of words that contain all the relevant information for the sentiment analysis tasks in NLP. With that list of words, you would be able to get a nice representation using a frequency dictionary mapping. And finally, get a vector with a bias unit and two additional features that store the sum of the number of times that every word on your process tweets appears in positive tweets and the sum of the number of times they appear in negative ones.\n", "In practice, you would have to perform this process on a set of m tweets. So given a set of multiple raw tweets, you would have to preprocess them one by one to get these sets of lists of words one for each of your tweets. And finally, you d be able to extract features using a frequencies dictionary mapping. At the end, you would have a matrix, X with m rows and three columns where every row would containthe features for each one of your tweets.\n", "The general implementation of this process is rather easy. First, you build the frequencies dictionary, then initialize the matrix X to match your number of tweets. After that, you ll want to go over through your sets of tweets carefully deleting stop words, stemming, deleting URLs, and handles and lower casing. And finally, extract the features by summing up the positive and negative frequencies of the tweets. For this week s assignment, you ve been provided some helper functions, build_freqs and process_tweet. However, you ll have to implement the function to extract the features of a single tweet. That was a lot of code, but at least now you have your X matrix. And in the next video, we will show youhow you can feed in that X matrix into your logistic regression classifier. Let s take a look at how you can do that.\n", "Please Follow coursesteach to see latest updates on this story\n", "If you want to learn more about these topics: Python, Machine Learning Data Science, Statistic For Machine learning, Linear Algebra for Machine learning Computer Vision and Research\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "Note:if you are a NLP export and have some good suggestions to improve this blog to share, you write comments and contribute.\n", "if you need more update about NLP and want to contribute then following and enroll in following\n", " Course: Natural Language Processing (NLP)\n", " GitHub Repository\n", "   Notebook\n", "1- Natural Language Processing with Classification and Vector Spaces\n", "2-Putting it All Together\n"]}, {"link": "https://medium.com/@cobusgreyling/a-new-prompt-engineering-technique-has-been-introduced-called-step-back-prompting-b00e8954cacb?source=list-e28f6edecf84--------30-------7b153c9756d3---------------------", "title": "A New Prompt Engineering Technique Has Been Introduced Called Step-Back Prompting", "subtitle": "Step-Back Prompting is a prompting technique enabling LLMs to perform abstractions, derive high-level concepts & first principles from which accurate answers can be derived.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "823", "response": "9", "timeForRead": "5 min read", "dateCreate": "Oct 12", "text": ["As we have seen with most prompting techniques published, Large Language Models (LLMs) need guidance when intricate, multi-step reasoning is demanded from a query, and decomposition is a key component when solving complex request.\n", "A process of supervision with step-by-step verification is a promising remedy to improve the correctness of intermediate reasoning step\n", "The most well known prompting technique when it comes to decomposition is chain-of-thought reasoning. In this study Step-Back Prompting is compared to COT prompting.\n", "The text below shows a complete example of STP with the original question, the stepback question, principles, and the prompt for the final answer to be generated by the LLM.\n", "This chart shows the strong performance of Step-Back Prompting which follows an abstraction and reasoning scheme. Evidently this approach leads to significant improvements in a wide range of more complex tasks.\n", "The chart below shows the Step-Back Prompting approach on the TimeQA dataset. Step-Back combined with RAG compared to baseline predictions.\n", "On the left is Step-Back & RAG vs baseline predictions.\n", "On the right, Step-Back RAG vs RAG predictions.\n", "Step-Back Prompting fixed 39.9% of the predictions where the baseline prediction is wrong, while causing 5.6% errors.\n", "Step-Back Prompting + RAG fixes 21.6% errors coming from RAG. While introducing 6.3% errors.\n", "This study again illustrates the versatility of Large Language Models and how new ways of interacting with LLMs can be invented to leverage LLMs even further.\n", "This technique also shows the ambit of static prompting and clearly shows that as complexity grows, more augmented tools like prompt-chaining and autonomous agents need to be employed.\n", "  Follow me on LinkedIn for updates on Large Language Models  \n", "I m currently the Chief Evangelist @ Kore AI. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}, {"link": "https://medium.com/@cobusgreyling/langsmith-hub-by-the-numbers-55e962ba5cf5?source=list-2eb23a991a63--------23-------0a856388a93a---------------------", "title": "LangSmith Hub By The Numbers", "subtitle": "LangSmith can be divided into four sub-products named Projects, Data, Testing & Hub. The first three of these sub-products are focussed on improving production implementations while Hub focusses more on pre-launch testing and refinement.", "autorName": "Cobus Greyling", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*nzfAEuujMN0s-aK6R7RcNg.jpeg", "clap": "104", "response": "1", "timeForRead": "3 min read", "dateCreate": "Oct 13", "text": ["Below is a matrix of all the models available in the LangSmith Hub access to models is a significant drawcard and allowing users to experiment with different models while tweaking prompts and comparing model output.\n", "Considering the table below, the 14 use cases are listed according to the number of prompts in the LangSmith Hub. The biggest use case is chatbots, followed by summarisation and QnA over documents. The top rated use cases include extraction and agents.\n", "This is a good indication of how Large Language Models are being used in implementations.\n", "Considering the table below, chat based prompts are almost on par with string (completion) based templates. This almost 50/50 split is interesting considering the push from OpenAI to deprecate complete and edit modes and favour the chat mode.\n", "Something I found curious is how high the Chinese language is ranked in the number of prompts.\n", "Lastly, prompt count according to models is dominated by OpenAI, followed by Anthropic and Google.\n", "In closing, there is a need for a LLM focused workspace where experimentation is possible referencing different LLMs. There are a few prompt hubs, most notably that of Haystack.\n", "In upcoming articles I will be focussing on data, and the four fundamental pillars of data in terms of discovery, design, development and delivery.\n", "  Follow me on LinkedIn for updates on Large Language Models  \n", "I m currently the Chief Evangelist @ Kore AI. I explore & write about all things at the intersection of AI & language; ranging from LLMs, Chatbots, Voicebots, Development Frameworks, Data-Centric latent spaces & more.\n"]}, {"link": "https://medium.com/@rajistics/explaining-predictions-from-transformer-models-55ab9c6cab24?source=list-a13ace4f182c--------34-------f7e9b3597071---------------------", "title": "Explaining predictions from   transformer models", "subtitle": "false", "autorName": "Rajiv Shah", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*_FB_9GE68trweRsuptPUFQ.png", "clap": "51", "response": "1", "timeForRead": "3 min read", "dateCreate": "Aug 15, 2022", "text": ["This post covers 3 easy-to-use   packages to get started. You can also check out the Colab   companion notebook at https://bit.ly/raj_explain and the Youtube   video for a deeper treatment.\n", "Explanations are useful for explaining predictions. In the case of text, they highlight how the text influenced the prediction. They are helpful for   diagnosing model issues,   showing stakeholders understand how a model is working, and   meeting regulatory requirements.\n", "Here is an explanation   using shap. For more on explanations, check out the explanations in machine learning video.\n", "Let s review 3 packages you can use to get explanations. All of these work with transformers, provide visualizations, and only require a few lines of code.\n", "2. Transformers Interpret uses Integrated Gradients from Captum to calculate the explanations. This approach is   quicker than SHAP! Check out this space to see a demo.\n", "3. Ferret is built for benchmarking interpretability techniques and includes multiple explanation methodologies (including Partition Shap and Integrated Gradients). A spaces demo for ferret is here along with a paper that explains the various metrics incorporated in ferret.\n", "You can see below how explanations can differ when using different explanation methods. A great reminder that explanations for text are complicated and need to be appropriately caveated.\n", "Ready to dive in?  \n", "For a longer walkthrough of all the   packages with code snippets, web-based demos, and links to documentation/papers, check out:\n", "  Colab notebook: https://bit.ly/raj_explain\n", "  https://youtu.be/j6WbCS0GLuY\n", "Originally published at http://projects.rajivshah.com on Aug. 14, 2022.\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-7-6e73b81ecc7c?source=list-660438a01f7f--------9-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing (Part 7)", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "1", "response": "2", "timeForRead": "14 min read", "dateCreate": "Aug 20", "text": ["What is text-processing\n", "Text pre-processing is the process of transforming unstructured text to structured text to prepare it for analysis. When you pre-process text before feeding it to algorithms, you increase the accuracy and efficiency of said algorithms by removing noise and other inconsistencies in the text that can make it hard for the computer to understand. Making the text easier to understand also helps to reduce the time and resources required for the computer to pre-process data.These words need to then be encoded as integers, or floating-point values, for use as inputs in machine learning algorithms. This process is called feature extraction (or vectorizations).\n", "Scikit-learn s CountVectorizer is used to convert a collection of text documents to a vector of term/token counts. It also enables the  pre-processing of text data prior to generating the vector representation. This functionality makes it a highly flexible feature representation module for text.\n", "In this section, we will discuss the steps involved in preparing the data for sentiment analysis using logistic regression.\n", "1- Data Collection:\n", "The first step in any sentiment analysis project is to collect a suitable dataset. This can be done by scraping data from social media platforms, online reviews, or any other relevant sources.\n", "2- Data Cleaning\n", "Once the data is collected, it needs to be cleaned by removing unnecessary characters, punctuation marks, and stopwords. Stop-words are words that do not carry much meaning and can be safely ignored. When it comes to low-level text processing problems  it is advisable to remove all punctuations and special characters ( including emojis ) for several significant reasons, which are Dimensionality issues, Computational Efficiency, Noise Reduction, and Generalization you can state other issues as well \n", "Dimensionality Reduction: Keeping every punctuation mark and special character as a separate feature can significantly increase the dimensionality of the data, making it computationally expensive and potentially leading to overfitting. By removing them, you reduce the dimensionality of the feature space.\n", "Computational Efficiency: Some NLP algorithms and models, especially those based on neural networks, are computationally more efficient when trained on preprocessed text. Removing punctuations and special characters can help speed up the training and inference processes.\n", "Noise Reduction: Punctuation and special characters often don t carry significant semantic meaning on their own. Removing them can help reduce the noise in the text and make it easier for NLP models to focus on the meaningful words and phrases.\n", "Generalization: Ignoring punctuations and special characters helps NLP models generalize better. For instance, if you remove the period from the end of a sentence, the model can better learn the relationship between words without being overly influenced by sentence boundaries.\n", "However, it s important to note that there are cases where punctuations and special characters might convey valuable information, such as in sentiment analysis (e.g.,  I love it  vs.  I love it! , in the second case the speaker seems to be more excited). In such cases, you may choose to retain certain punctuation marks or handle them differently in your preprocessing pipeline. The choice of whether to remove or retain punctuations depends on the specific NLP task and the goals of your analysis.\n", "3- Tokenization:\n", "Tokenization is the process of splitting text into individual words or tokens. This step helps in creating a structured format for further analysis.\n", "4- Feature Extraction: After tokenization, relevant features need to be extracted from the text. This can be done using techniques such as bag-of-words, TF-IDF (Term Frequency-Inverse Document Frequency), or word embeddings.\n", "Labeling: Each data point in the dataset needs to be labeled with the corresponding sentiment category (positive or negative). This can be done manually or by using pre-labeled datasets for training purposes.\n", "During human conversations, punctuation marks like  , ! , [, }, *, #, /, ?, and   are incredibly relevant and necessary to have a proper conversation. Thelp to fully convey the message of the writer. Since we have a Twitter dataset, we'd like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks. We'll use the re library to perform regular expression operations on our tweet. We'll define our search pattern and use the sub() method to remove matches by substituting with an empty character (i.e. '\n", "By removing punctuation marks from our text we allow the model to focus on the text alone rather than distracting it with symbols. This makes it easier for the text to be analysed.\n", "Tokenization is the process of transforming a string or document into smaller chunks, which we call tokens. When a sentence breakup into small individual words or Phrases, these pieces of words are known as tokens, and the process is known as tokenization.This is usually one step in the process of preparing a text for natural language processing. This allows the computer to work on your text token by token rather than working on the entire text in the following stage. There are many theories and rules regarding tokenization, and you can create your own tokenization rules using regular expressions, but normally tokenization will do things like break out words or sentences, often separate punctuation or you can even just tokenize parts of a string like separating all hashtags in a Tweet [1].The NLTK library includes a range of tokenizers for different languages and use cases.\n", "Why bother with tokenization? Because it can help us with some simple text processing tasks like mapping part of speech, matching common words and perhaps removing unwanted tokens like common words or repeated words. Here, we have a good example. The sentence is: I don t like Sam s shoes. When we tokenize it we can clearly see the negation in the not and we can see possession with the  s. These indicators can help us determine meaning from simple text [1].\n", "The two main types of tokenization are word and sentence tokenization.\n", "1- Word tokenization is the most common kind of tokenization. Here, each token is a word, meaning the algorithm breaks down the entire text into individual words\n", "2- sentence tokenization: On the other hand, sentence tokenization breaks down text into sentences instead of words. It is a less common type of tokenisation only used in few Natural Language Processing (NLP) tasks.\n", "In normalization, your text is converted to standard form. An example of this is converting all text to lowercase, removing numbers, or removing punctuations. Normalization helps to make the text more consistent. There are a couple of different normalization techniques, but I ll give you an explanation of some of the most commonly employed normalisation techniques below.\n", "This technique converts all the letters in your text to a single case, either uppercase or lowercase. Case normalisation ensures that your data is stored in a consistent format and makes it easier to work with the data. An example would be looking for all the instances of a word and searching for it in your text. Without case normalisation, the result of searching for the word  Boy  would be different from the result of searching for  boy .\n", "Stop word is used to filter some words which are repeat often and not giving information about the text. In Spacy, there is a built-in list of some stop words.One of the important preprocessing steps in NLP is to remove stop words from text. Stop words are basically connector words such as  to ,  with ,  is , etc. which provide minimal context. spaCy allows easy identification of stop words with an attribute of the  doc  object called  is_stop . We iterate over all the tokens and apply the  is_stop  method [2].Stop words, such as  the,   and,   is,  and  an,  are common words that appear frequently in a language. These terms are frequently irrelevant to the analysis and can be removed to reduce the noise in the data. The NLTK library includes a list of English stop words for this purpose.\n", "These are the most common words which do not add much value to the meaning of the document.[1]\n", "Let s take a look at how you can do this. Let s process this tweet. First, I remove all the words that don t add significant meaning to the tweets, aka stop words and punctuation marks. In practice, you would have to compare your tweet against two lists. One with stop words in English and another with punctuation. These lists are usually much larger, but for the purpose of this example, they will do just fine. Every word from the tweet that also appears on the list of stop words should be eliminated. So you d have to eliminate the word and, the word are, the word a, and the word at. The tweet without stop words looks like this.\n", "Note that the overall meaning of the sentence could be inferred without any effort. Now, let s eliminate every punctuation mark. In this example, there are only exclamation points. The tweet without stop words and punctuation looks like this.\n", "However, note that in some contexts you won t have to eliminate punctuation. So you should think carefully about whether punctuation adds important information to your specific NLP task or not. Tweets and other types of texts often have handles and URLs, but these don t add any value for the task of sentiment analysis. Let s eliminate these two handles and this URL. At the end of this process, the resulting tweets contains all the important information related to its sentiment.\n", "Tuning the GREAT AI model is clearly a positive tweet and a sufficiently good model should be able to classify it.\n", "Lemmatization is better than stemming and informative to find beyond the word to its stem also determine part of speech around a word. That s why spacy has lemmatization, not stemming. So we will do lemmatization with spacy.Lemmatization is another important preprocessing step for NLP pipelines. It helps to remove different versions of a single word to reduce redundancy of same-meaning words as it converts the words to their root lemmas. For example, it will convert  is  ->  be ,  eating  ->  eat , and  N.Y.  ->  n.y. . With spaCy, the words can be easily converted to their lemmas using a  .lemma_  attribute of the  doc  object.[2]. We iterate over all the tokens and apply the  .lemma_  method.[2].Lemmatization is the process of reducing a word to its base or root form, called a lemma. Stemming is a similar process, but it often results in words that are not actual words.\n", "For example, the words  walked ,  walking , and  walk  would all be lemmatized to the word  walk . This is because they all have the same lemma, which is the dictionary form of the word.\n", "Lemmatization can be done using a variety of tools and techniques. Some popular lemmatizers include the Porter stemmer, the Snowball stemmer, and the WordNet lemmatizer.\n", "Lemmatization is a similar process to stemming, but it reduces words to their base form by using a dictionary or knowledge of the language. This can result in more accurate base forms than stemming [6].\n", "Another text preprocessing technique using which we reduce the words down to their root forms.[1]\n", "A basic example demonstrating how a lemmatizer works\n", "In the following example, we are taking the PoS tag as  verb,  and when we apply the lemmatization rules, it gives us dictionary words instead of truncating the original word:[4]\n", "What is stemming: Stemming is a process in which words are reduced to their root meaning.It s a technique to get to the root form of a word by removing the prefix and suffix of a word.[1].We use Stemming to normalize words. In English and many other languages, a single word can take multiple forms depending upon the context used. For instance, the verb  study  can take many forms like  studies,   studying,   studied,  and others, depending on its context. When we tokenize words, an interpreter considers these input words as different words even though their underlying meaning is the same. Moreover, as we know that NLP is about analyzing the meaning of content, to resolve this problem, we use stemming [4].\n", "Stemming normalizes the word by truncating the word to its stem word. For example, the words  studies,   studied,   studying  will be reduced to  studi,  making all these word forms to refer to only one token. Notice that stemming may not give us a dictionary, grammatical word for a particular set of words [4].In Natural Language Processing (NLP),  steaming  refers to the process of reducing a word to its base or root form. This is often done to group together different forms of a word so they can be analyzed together as a single item [6].\n", "Stemming is the process of reducing words to their base or stem form, by removing any prefixes or suffixes. This is a common technique for reducing the dimensionality of the data, as it groups similar words together.\n", "Now that the tweet from the example has only the necessary information, I will perform stemming for every word.\n", "Stemming in NLP is simply transforming any word to its base stem, which you could define as the set of characters that are used to construct the word and its derivatives. Let s take the firstword from the example. Its stem is tun, because adding the letter e, it forms the word tune. Adding the suffix ed, forms the word tuned, and adding the suffix ing, it forms the word tuning. After you performstemming on your corpus, the word tune, tuned, and tuning will be reduced to the stem tun. So your vocabulary would be significantly reduced when you perform this process for every word in the corpus.\n", "To reduce your vocabulary even further without losing valuable information, you d have to lowercase every one of your words. So the word GREAT, Great and great would be treated as the same exact word. This is the final preprocess tweet as a list of words. Now that you re familiar with stemming and stop words, you know the basics of texts processing.\n", "Types of stemmer\n", "Porter stemmer was developed in 1980. It is used for the reduction of a word to its stem or root word.one thing is noticed that the porter stemmer is not giving many good results. So, that s why the Snowball stemmer is used for a more improved method.\n", "Is a commonly used model that allows you to count all words in a piece of text. Basically, it creates an occurrence matrix for the sentence or document, disregarding grammar and word order. These word frequencies or occurrences are then used as features for training a classifier.\n", "Bag of Words is a text-processing methodology that extracts features from textual data. It uses a pre-defined dictionary of words to measure the presence of known words in your data and doesn t consider the order of word appearance.\n", "The algorithm uses this dictionary to loop through all the documents in the data and can use a simple scoring method to create the vectors. For example, it can mark the presence of a word in a vocabulary as 1 or 0 if absent. Additional scoring methods include looking at the frequency of each word appearing in the document.\n", "Here is an example of a bag-of-words representation of the sentence  John likes to watch movies. Mary likes movies too :\n", "This representation tells us that the words  john ,  likes ,  movies , and  mary  appear in the sentence, and that the word  likes  appears twice. It does not tell us anything about the order of the words in the sentence, or about the grammatical relationships between the words.\n", "The bag-of-words model is a simple and efficient way to represent text for use in machine learning algorithms. It is often used in tasks such as document classification, sentiment analysis, and topic modeling.\n", "Please Follow coursesteach to see latest updates on this story\n", "If you want to learn more about these topics: Python, Machine Learning Data Science, Statistic For Machine learning, Linear Algebra for Machine learning Computer Vision and Research\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "Note:if you are a NLP export and have some good suggestions to improve this blog to share, you write comments and contribute.\n", "if you need more update about NLP and want to contribute then following and enroll in following\n", " Course: Natural Language Processing (NLP)\n", " GitHub Repository\n", "   Notebook\n", "1-Day 2: 30 Days of Natural Language Processing Series with Projects\n", "3-Fully Explained Regular Expression with Python (Unread)\n", "4-Natural Language Processing (NLP) with Python   Tutorial( Unread)\n", "5-Python for Natural Language Processing: A Beginner s Guide\n", "6-Every Beginner NLP Engineer must know these Techniques (Unread)\n", "7-A Guide to Text Preprocessing Techniques in NLP\n", "8- Natural Language Processing with Classification and Vector Spaces\n", "9 6-How to Convert Text Into Vectors\n", "10-Text Preprocessing For NLP Part   1\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-10-80b392750ef4?source=list-660438a01f7f--------6-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing(Part 10)", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "1", "response": "2", "timeForRead": "3 min read", "dateCreate": "Sep 10", "text": ["In the previous tutorial, you learned how to classify whether a tweet has a positive sentiment or negative sentiment, using a theta that I have given you. In this tutorial, you will learn your own theta from scratch, and specifically, I ll walk you through an algorithm that allows you to get your theta variable.\n", "Let s see how you can do this. To train your logistic regression classifier, iterate until you find the set of parameters theta, that minimizes your cost function. Let us suppose that your loss only depends on the parameters theta1 and theta2, you would have a cost function that looks like this contour plots on the left. On the right, you can see the evolution of the cost function as you iterate. First, you would have to initialize your parameters theta. Then you will update your theta in the direction of the gradient of your cost function. After a 100 iterations, you would be at this point, after 200 here, and so on. After many iterations, you derive to a point near your optimum costs and you d end your training here.\n", "Let s look at this process in more detail. First, you d have to initialize your parameters vector theta. Then you d use the logistic function to get values for each of your observations. After that, you d be able to calculate the gradients of your cost function and update your parameters. Finally, you d be able to compute your cost J and determine if more iterations are needed according to a stop-parameter or maximum number of iterations. As you might have seen in the other courses, this algorithm is known as gradient descent. Now, that you have your theta variable, you want to evaluate your theta, meaning you want to evaluate your classifier. Once you put in your theta into your sigmoid function, do get a good classifier or do you get a bad classifier? In the next tutorial, we will show you how you can do this.\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "Note:if you are a NLP export and have some good suggestions to improve this blog to share, you write comments and contribute.\n", "if you need more update about NLP and want to contribute then following and enroll in following\n", " Course: Natural Language Processing (NLP)\n", " GitHub Repository\n", "   Notebook\n", "1- Natural Language Processing with Classification and Vector Spaces\n", "2-Logistic Regression: Training\n"]}, {"link": "https://medium.com/@avra42/summarizing-scientific-articles-with-openai-and-streamlit-fdee12aa1a2b?source=list-dee72bb8661c--------17-------c25b06fd87f2---------------------", "title": "Summarizing Scientific Articles with OpenAI   and Streamlit  ", "subtitle": "false", "autorName": "Avra", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*NzYk5R6Pl1fYyHJPkIl2rA.png", "clap": "97", "response": "1", "timeForRead": "6 min read", "dateCreate": "Dec 19, 2022", "text": ["  GitHub  |   Twitter |   YouTube |   BuyMeaCoffee | Ko-fi \n", "TL;DR: This blog post uses the OpenAI API and the Streamlit library to create a simple web application that allows a user to input a scientific article and generate a summary of the article. The user can choose the size of the output summary and save the generated summary to their device.\n", "To use the OpenAI API, you will need to obtain an API key by signing up for a free account on the OpenAI website and creating a new API key. You can then use the API key in your code by setting it as the value of the openai.api_key variable (refer to the code block). You can then use the OpenAI API by making requests to the API's endpoints using the openai library. This will allow you to use the AI-powered text generation capabilities of the OpenAI API in your projects. (Note: I've made a demo tutorial video that will guide you through this process)\n", "The st.secrets object is a special Streamlit object that allows you to store sensitive information in a secure way, as the secrets are not visible in the source code of your app while testing locally. This is useful for protecting API keys and other sensitive information from being exposed publicly. The st.secrets object is accessed like a dictionary, with the keys being the names of the secrets and the values being the secret values themselves.\n", "First, let s start by importing the required libraries and setting up our OpenAI API key:\n"]}, {"link": "https://medium.com/@siddiquimubasheer/text-summarization-using-bert-and-t5-e05dbbc757c6?source=list-2c27d980d3f3--------16-------338c7da11cbf---------------------", "title": "Text Summarization using BERT and T5", "subtitle": "false", "autorName": "Mubasheer Siddiqui", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*AiEM6FHS0zuDxpsL0feKUw.jpeg", "clap": "418", "response": "4", "timeForRead": "7 min read", "dateCreate": "Jan 7, 2022", "text": ["Many times we find ourselves in a situation where we need the summary of the details and not a full report of the same, then often we go through the whole text and markup important direct or indirect details and then rewrite. This is definitively a time consuming approach and when the no of documents increases, we realize the importance of automatic text summarization.\n", "The difficulty of producing a concise, accurate, and fluent summary of a lengthy text document is known as text summarization.\n", "Automatic text summarization methods are desperately needed to deal with the ever-increasing amount of text data available online, in order to improve both the discovery and consumption of relevant information.\n", "Automatic Text Summarization can be used to summarize research papers, long reports, full books, online pages, and news, among other things. We have seen new highs in this discipline as a result of recent breakthroughs in Machine Learning, particularly Deep Learning.\n", "Deep learning technologies have proven to be particularly promising for this endeavor since they attempt to replicate the way the human brain functions by managing multiple levels of abstraction and nonlinearly translating a given input into a given output (in this process the output of one layer becomes the input of the other layer and so on). Obviously, the deeper the layers, the deeper the depth. Deep neural networks are commonly utilized in NLP difficulties because their architecture fits well with the language s complicated structure; for example, each layer can handle a particular task before passing the output to the next.\n", "In Natural Language Processing (NLP), there are two main ways to summarize text :1. Extractive Summarization2. Abstractive Summarization\n", "Extracting essential words from a source document and combining them to make a summary is what extractive text summarization is all about. The extraction is done according to the predefined measure without making any changes to the texts.\n", "Methods for extractive summarization are :\n", "Parts of the source document are interpreted and trimmed as part of the abstraction approach. When deep learning is applied for text summarization, abstraction can overcome the grammar mistakes of the extractive method.\n", "The abstractive text summarization algorithms, like humans, produce new phrases and sentences that convey the most relevant information from the original text.\n", "Due to these reasons, abstraction outperforms extraction. The text summarization algorithms required for abstraction, on the other side, are more challenging to build, which is why extraction is still widely used.\n", "Methods for abstractive summarization are :\n", "Text : Joseph and Mary rode on a donkey to attend the annual event in Jerusalem. In the city, Mary gave birth to a child named Jesus.\n", "Extractive Summary : Joseph and Mary attend event Jerusalem. Mary birth Jesus.\n", "Abstractive Summary : Joseph and Mary came to Jerusalem where Jesus was born.\n", "It s not an exaggeration to mention that BERT has considerably altered the Natural Language Processing scene. Consider using a single model trained on a huge unlabeled dataset to obtain best-in-class results on eleven different NLP tasks. and every one of this with very little fine-tuning. That s BERT! It s a tectonic shift in how design we models.\n", "a lot of latest NLP architectures, training approaches, and language models, such as OpenAI s GPT-2, Google s TransformerXL, RoBERTa, ERNIE2.0, XLNet, etc. have been inspired byBERT.\n", "You ve probably heard about BERT and read about how amazing it is and how it may change the NLP landscape. But, first and foremost, what is BERT?\n", "The NLP framework is described as follows by the BERT research team:\n", " BERT stands for Bidirectional Encoder Representations from Transformers. It is intended to condition both left and right context to pre-train deep bidirectional representations from unlabeled text. As a result, with just one additional output layer, the pre-trained BERT model may be fine-tuned to generate state-of-the-art models for a wide range of NLP tasks. \n", "To begin, BERT stands for Bidirectional Encoder Representations from Transformers, which is simple to grasp. Each word has a significance, which we will discover one by one throughout this article. For now, the most important takeaway from this section is that BERT is built on the Transformer architecture.\n", "Second, BERT is pre-trained on a vast corpus of unlabeled text, which comprises the whole Wikipedia, which has 2,500 million words, and the Book Corpus, which contains around 800 million words.This pre-training step is responsible for half of BERT s success. This is due to the fact that when a model is trained on a large text corpus, it learns to pick up on deeper and more intimate understandings of how language works. This data may be used as a swiss army knife in almost any NLP project.\n", "Third, BERT is a  deeply bidirectional  model. During the training phase, BERT learns information from both the left and right sides of the context of a token.\n", "In this section we will be looking at Extractive Text Summarization using BERT. As we know, in Extractive Summarization we select sentences from the text as summary. Hence it can be considered as a classification problem where we classify if a sentence is part of a summary or not.\n", "The challenge here is that the model will have to interpret the entire text, choose the correct keywords and ensure that there is no loss. Hence to ensure that there is no compromise of speed and accuracy, we use BERTSUM which is able enough to parse meaning from language and do other preprocessing steps like stop word removal, lemmatization, etc. on its own.\n", "The BERTSUM model consists of 2 parts:\n", "1. BERT encoder.2. Summarization Classifier.\n", "The Encoder provides us with a vector representation of each sentence which is then used by the Summarization Classifier to assign a label to each sentence indicating whether or not it will be incorporated into the final report.\n", "The input of BERTSUM is a little bit different as compared to the BERT model. Here we add the [CLS] token before each sentence in order to separate each sentence and collect the features of its preceding sentence. Each sentence is also assigned and embedding i.e. it is given Ea if the sentence is of even and Eb if it is of odd length. It also gives a score to each sentence, depending on how important it is, and based on these scores the sentences are decided whether or not to be included in the summary.\n", "T5, built by one of the tech giants Google, is one of the most powerful tools for text summarizing. T5, or text to text transfer transformer, a transformer model allows fine tuning for any simple text to task.\n", "To the current study, T5 adds the following:1. It builds Colossal Cleaned Common Crawl (C4), a clean version of the enormous common crawl data collection . This data set dwarfs Wikipedia by two orders of magnitude.2. It proposes that all NLP jobs be reframed as an input text to output text formulation.3. It exhibits that fine tuning on various tasks   summarization, QnA, reading comprehension with the pretrained T5, and text-text formulation   produces state-of-the-art outcomes.4. The T5 team also conducted a thorough investigation into the best procedures for pre-training and fine-tuning\n", "T5 is one of the most qualified for Abstractive summarization for the reasons listed above. Abstractive summarization is a Natural Language Processing (NLP) job that seeks to produce a short summary of a source text. As aforementioned, abstractive summarization, unlike extractive summarization, does not merely reproduce essential phrases from the original text but also has the capacity to generate new relevant phrases, which is referred to as paraphrase.\n", "For abstractive summarization, T5 can be used so easily as follows :\n"]}, {"link": "https://medium.com/@Coursesteach/natural-language-processing-part-15-bayes-rule-b87f9dff4a90?source=list-660438a01f7f--------0-------dbbdeca8bd9e---------------------", "title": "Natural Language Processing(Part 15)-Bayes  Rule", "subtitle": "false", "autorName": "Coursesteach", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ckRR-s5P7VXOnnKG32SjAg.jpeg", "clap": "137", "response": "2", "timeForRead": "5 min read", "dateCreate": "false", "text": ["We will be looking at conditional probabilities to help us understand Bayes rule. In other words, if I tell you, you can guess what the weather is like given that we are in California and it is winter, then you ll have a much better guess than if I just asked you to guess what the weather is like. In order to derive Bayes rule, let s first take a look at the conditional probabilities.\n", "Now think about what happens if, instead of the entire corpus, you only consider tweets that contain the word happy. This is the same as saying, given that a tweet contains the word happy with that, you would be considering only the tweets inside the blue circle, where many of the positive tweets are now excluded. In this case, the probability that a tweet is positive, given that it contains the word happy, simply becomes the number of tweets that are positive and also contain the word happy. We divide that by the number that contains the word happy. As you can see by this calculation, your tweet has a 75 percent likelihood of being positive if it contains the word happy.\n", "You could make the same case for positive tweets. The purple area denotes the probability that a positive tweet contains the word happy. In this case, the probability is 3 over 13, which is 0.231.\n", "With all of this discussion of the probability of missing certain conditions, we are talking about conditional probabilities. Conditional probabilities could be interpreted as the probability of an outcome B knowing that event A already happened, or given that I m looking at an element from set A, the probability that it s also belongs to set B.\n", "Here s another way of looking at this with a Venn diagram you saw before. Using the previous example, the probability of a tweet being positive, given that it has the word happy, is equal to the probability of the intersection between the tweets that are positive and the tweets that have the word happy divided by the probability of a tweet given from the corpus having the word happy.\n", "Let s take a closer look at the equation from the previous slide. You could write a similar equation by simply swapping the position of the two conditions. Now, you have the conditional probability of a tweet containing the word happy, given that it is a positive tweet. Armed with both of these equations, you re now ready to derive Bayes rule.\n", "To combine these equations, note that the intersection represents the same quantity, no matter which way it s written. Knowing that, you can remove it from the equation, with a little algebraic manipulation, you are able to arriveat this equation.\n", "This is now an expression of Bayes rule in the context of the previous sentiment analysis problem. More generally, Bayes rule states that the probability of x given y is equal to the probability of y given x times the ratio of the probability of x over the probability of y. That s it. You just arrived at the basic formulation ofBayes rule, nicely done.\n", "To wrap up, you just derive Bayes rule from expressions of conditional probability. Throughout the rest of this course, you ll be using Bayes rule for various applications in NLP. The main takeaway for now is that, Bayes rule is based on the mathematical formulation of conditional probabilities. That s with Bayes rule, you can calculate the probability of x given y if you already know the probability of y given x and the ratio of the probabilities of x and y. That s great work. I ll see you later. Congratulations. You now have a good understanding of Bayes rule. In the next video, you ll see how you can start applying Bayes rule to a model known as Naive Bayes. This will allow you to start building your sentiment analysis classifier using just probabilities.\n", "Please Follow coursesteach to see latest updates on this story\n", "If you want to learn more about these topics: Python, Machine Learning Data Science, Statistic For Machine learning, Linear Algebra for Machine learning Computer Vision and Research\n", "Then Login and Enroll in Coursesteach to get fantastic content in the data field.\n", "Stay tuned for our upcoming articles where we will explore specific topics related to NLP in more detail!\n", "Remember, learning is a continuous process. So keep learning and keep creating and sharing with others! \n", "Note:if you are a NLP export and have some good suggestions to improve this blog to share, you write comments and contribute.\n", "if you need more update about NLP and want to contribute then following and enroll in following\n", " Course: Natural Language Processing (NLP)\n", " GitHub Repository\n", "   Notebook\n", "Do you want to get into data science and AI and need help figuring out how? I can offer you research supervision and long-term career mentoring.Skype: themushtaq48, email:mushtaqmsit@gmail.com\n", "Contribution: We would love your help in making coursesteach community even better! If you want to contribute in some courses , or if you have any suggestions for improvement in any coursesteach content, feel free to contact and follow.\n", "Together, let s make this the best AI learning Community!  \n", " WhatsApp\n", "  Facebook\n", " Github\n", " LinkedIn\n", " Youtube\n", " Twitter\n", "1- Natural Language Processing with Classification and Vector Spaces\n"]}, {"link": "https://medium.com/@kedion/getting-started-with-hugging-face-5efae4984dee?source=list-a13ace4f182c--------12-------f7e9b3597071---------------------", "title": "Fine-Tuning NLP Models With Hugging Face", "subtitle": "Part 1: Getting Started", "autorName": "Kedion", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*NO-81irJRQMGyKcT4yvluw.png", "clap": "18", "response": "1", "timeForRead": "14 min read", "dateCreate": "Sep 21, 2021", "text": ["Written by Tigran Avetisyan\n", "This is Part 1 of our 3 PART SERIES on Hugging Face.\n", "See Part 2 here.\n", "Natural language processing, or NLP, is an exciting field that has seen dramatic development in recent years. Today, language models like GPT-3 (Generative Pre-trained Transformer 3) can produce text that is indistinguishable from that written by a human, which is sensational from a technological standpoint but controversial when we are talking about ethics.\n", "You can easily build and train quality NLP models right on your computer as well (though you probably won t be reaching GPT-3 levels of convincingness). However, starting with NLP can be tricky because of data requirements and preprocessing rules for text sequences.\n", "If you want to make use of natural language processing right now, you could leverage an NLP framework like Hugging Face, which is an API that facilitates the use of language models for large-scale inference.\n", "In this guide, we are going to introduce you to the features and capabilities of Hugging Face, and we will also showcase the basics of inference with pretrained Hugging Face models.\n", "Let s get started!\n", "Hugging Face is a community and NLP platform that provides users with access to a wealth of tooling to help them accelerate language-related workflows. The framework contains thousands of models and datasets to enable data scientists and machine learning engineers alike to tackle tasks such as text classification, text translation, text summarization, question answering, or automatic speech recognition.\n", "In a nutshell, the framework contains the following important components (there are more):\n", "The Inference API is the key component of Hugging Face and is the one that will interest most potential users of the framework. Hugging Face has built the API for those who are not willing or are not technically adept enough to delve into code and those who want to get started with their projects in a short timeframe. Additionally, since the Inference API is hosted in the cloud, you don t have to deploy any models in your local environment.\n", "To start using the Inference API, you need to sign up with Hugging Face. The platform offers a number of subscription plans   a free plan with limited features and several paid plans with increased API request limits and access to accelerated inference.\n", "The free plan is perfectly sufficient for testing out the Inference API. If you end up liking the framework, you may upgrade to a paid plan in accordance with your project s needs.\n", "Performing Inference with the Inference API\n", "    Building a query function    \n", "To leverage the Inference API, you simply need to craft an HTTP request as follows:\n", "Above, we defined a function to perform a query to the Inference API. The Inference API requires that you pass the following arguments:\n", "The function returns a response in JSON format (though you may freely manipulate the results in whichever way you see fit for your needs).\n", "The model_id argument determines which model will be used to carry out your requests. To locate model_id, you should choose a model from the Hugging Face model directory and copy the endpoint at the very top of the webpage. As an example, if we take the RoBERTa base SQuAD model, here s what the endpoint looks like:\n", "RoBERTa base SQuAD is a model built to answer questions based on an input question and provided context for the answer. The model choice in this example is arbitrary   for real-world applications, you would select a model based on your goals.\n", "Next, we need to define our payload. The payload needs to conform to the input format of the selected model, which you can find under Usage down the webpage of your respective model.\n", "In the case of RoBERTa base SQuAD (and other question answering models), input data is a dictionary with two keys and associated sequences:\n", "Lastly, we have the API key. Assuming you ve set up an account on Hugging Face, you will find your API key at https://huggingface.co/settings/token.\n", "Single-question requests with the Inference API\n", "Here s how we put the code together and make a request to the Inference API:\n", "Our response is a dictionary with the following contents:\n", "Here:\n", "You don t have to feed input sequences one-by-one   you can perform batch prediction by just stuffing your input dictionaries in a Python list (or any other container of your choice):\n", "For this query, the response would be as follows:\n", "From this example, we not only got to see the Inference API in action, but we also saw that the RoBERTa base SQuAD model can accurately answer our questions based on context!\n", "Performing Invalid Requests With The Inference API\n", "To conclude this section, let s demonstrate what happens if your query request fails (e.g. if your input data format is wrong). Note the key  text  instead of  context  in the input dictionaries.\n", "The response to this query would be as follows:\n", "The Inference API doesn t throw any exceptions   instead, whenever anything goes wrong, error messages will be delivered to you in the response.\n", "These have been the basics of using the Inference API. Play around with the code yourself to find out what it can do for you!\n", "Pipeline VS Direct Model Use In Inference\n", "The Inference API completely abstracts the  behind the scenes  aspects of inference, which is great if you want to start using NLP models quickly. But if you would like to get direct access to the models for fine-tuning or just for self-learning purposes, you should use the Transformers library.\n", "Transformers can be installed with pip:\n", "Or with Conda:\n", "There are more ways to install Transformers   check out the library s installation guide to learn more.\n", "Transformers allows you to run inference and training on your local machine. Once you get Transformers installed on your machine, you will get access to two primary ways to do inference and/or training:\n", "Let s have a look at each of these methods below!\n", "Similar to the Inference API, pipelines hide away the process of inference, allowing you to get predictions with just a few lines of code.\n", "Pipeline instantiation is done as follows:\n", "The pipeline method has only one mandatory parameter   task. The value passed to this parameter determines which pipeline will be returned. The following pipelines are available for selection:\n", "Optionally, you may also specify a pre-trained model and tokenizer to be used. If you don t provide these arguments, the pipeline will load the default model for the specified task and the default tokenizer for the specified model.\n", "In our case, the default model is DistilBert base cased distilled SQuAD.\n", "To keep things simple, we ve again selected  question-answering  for our pipeline.\n", "Note that if you request a model and tokenizer pair for the first time, Transformers will need to download the files onto your machine. Downloaded files are cached for reuse.\n", "You may also set up Transformers to work in a completely offline environment, but this is beyond the scope of this post. The installation guide of Transformers contains more information about offline use.\n", "Anyway, once we ve got our pipeline loaded, we may directly proceed to inference. To do this, we just pass a query to the pipeline, like so:\n", "As you can see, carrying out inference with pipelines is very similar to how you use the Inference API. However, pipelines are hosted on your local machine (rather than in the cloud), and their setup process is somewhat different.\n", "If you want to customize your code even further, you could access pre-trained Transformers models directly.\n", "Transformers offers models in TensorFlow and/or PyTorch. The model usage process is very similar in either of the libraries, but there are some subtle differences in the code. You should consult Transformers   Summary of the tasks  for more information about implementation details.\n", "We will be using TensorFlow to showcase direct model use. And once again, we will stick to the task of question answering to keep this section consistent with the previous ones.\n", "To get started with direct model use, we need to import two classes:\n", "You can find more information about available auto classes (for tokenizers and model loaders) in the Transformers documentation.\n", "And here s how we instantiate our tokenizer and model:\n", "To obtain a pretrained tokenizer and model, we use the from_pretrained method with both classes, supplying the ID of the associated model. Instead of the ID, you may also supply a path or URL to a saved vocabulary file or model weights.\n", "Here, note the use of the from_pt parameter. The RoBERTa base SQuAD model we have been using throughout this guide is built with PyTorch, and no  native  TensorFlow models are available (as of this post s writing). However, the from_pt parameter allows us to convert PyTorch models to TensorFlow. This works the other way around as well through the from_tf parameter in PyTorch auto models.\n", "With all that said, keep in mind that conversion may not always be smooth. In the case of RoBERTa base SQuAD, we ve got the following warning:\n", "So not all of the weights of the pre-trained PyTorch model were carried over to the TensorFlow model. This won t matter now since we are only going to show how to use Transformers models rather than how to achieve great results with them.\n", "Tokenizing Question-Context Pairs\n", "To perform inference with the loaded model, we need to tokenize our question and its corresponding context as follows:\n", "When calling the tokenizer, we do the following:\n", "Note that depending on your model, you may need to make use of the other parameters of the tokenizer (such as maximum input length). You can find out more about some tokenizer parameters here.\n", "Here s what the inputs variable contains, if you are curious:\n", "As we can see, the inputs variable contains inputs_ids, which are the tokens assigned to our input question and context. Additionally, inputs contains an attention mask that, in our case, assigns equal importance to our inputs.\n", "In the code block above, we ve extracted the input IDs from the tokenized object and assigned them to the input_ids variable. Let s see what it contains:\n", "If we converted the input_ids back to a string, we would get the following:\n", "This shows us that the tokenizer combined our question and context and indicated their beginning and end with a special token to let the model know which is which. Without these tokens, the model would not be able to accurately answer our questions.\n", "Now, let s feed our inputs into the model and examine the outputs:\n", "The model output contains two TF Tensors   start_logits and end_logits. Essentially, these Tensors show the positions in the input at which the model thinks the answer to the input question starts and ends.\n", "To be able to pinpoint the exact position of the answer, we need to locate the indices with the maximum scores, like so:\n", "Notice that with answer_end, we add 1 to the index with the maximum score. This is to ensure that we don t lose the last word in the answer after we slice the input sequence.\n", "Let s see the values for answer_start and answer_end:\n", "And to retrieve our answer, we need to convert the IDs between the start and end indices back to a string:\n", "Notice that the output contains a space before  1980 . We won t bother removing it in this guide, but you could easily do so if necessary.\n", "In this particular example, the model picked one word   1980   as the answer to our question. But it can output answers with several consecutive words as well, which we can see if we rephrase the question to  Where was I born? \n", "The examples above showed how to process a single-sample query. For batches of questions, one option would be to iterate over the questions & contexts and predict an answer for each of them separately:\n", "Alternatively (and more optimally), we can vectorize the inference of question answers. Let s see this in action using the questions and contexts from the previous example:\n", "Notice the new parameter padding. We need to use this parameter whenever our input sequences have unequal length. When padding is True, the tokenizer will pad all sequences to make them as long as the longest sequence in the input.\n", "Next, we pass our inputs to the model and extract the input IDs from our inputs Tensor:\n", "Let s inspect input_ids to figure out how they are different from the IDs with single-sample inference:\n", "inputs_ids contains two NumPy arrays   one for each input question-context pair. Let s convert the second array back to a string to see its contents:\n", "This is indeed our second question-context pair! We can also see how the tokenizer handled its padding.\n", "Next, let s have a look at the output of our model:\n", "The start_logits and end_logits Tensors are each composed of two arrays that correspond to our question-context pairs.\n", "Next, let s retrieve the indices with the highest scores:\n", "Here s what we get:\n", "Now, let s use the indices to retrieve our answers:\n", "And here s the result:\n", "And to put it all together and make sure that the answers correspond to our questions, we can do the following:\n", "The answers indeed match our questions, so we ve done everything correctly!\n", "And this concludes our guide for inference via direct model use with Hugging Face   for now!\n", "The usage of other Hugging Face models in other tasks will be similar   however, you will need to check:\n", "The basics of direct model use with Hugging Face are described in the documentation of Transformers, so make sure to check it out!\n", "Note that the documentation doesn t go into much detail about certain aspects of Transformers. Often, you will need to figure out model input formats on your own, and you ll also need to figure out how to interpret outputs. But this is part of the learning experience!\n", "That s it!\n", "We covered a lot in this article. Now check out Part 2, where we learn to fine-tune NLP models with Hugging Face.\n", "You can find all code for this article in the Jupyter notebook here.\n"]}, {"link": "https://medium.com/@skillcate/detecting-fake-news-with-a-bert-model-9c666e3cdd9b?source=list-a0aae78aa81b--------18-------5fb2bbebc495---------------------", "title": "Detecting Fake News   with a BERT Model", "subtitle": "false", "autorName": "Skillcate AI", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*elWG64moGeBZ-lir056cBA.png", "clap": "90", "response": "1", "timeForRead": "9 min read", "dateCreate": "Oct 2, 2022", "text": ["In the last couple of decades, the emergence of social media (Facebook, Instagram, Twitter, etc.) & messaging platforms (WhatsApp, Telegram, etc.) have brought us all closer than ever. Just imagine, how easy it is to voice an opinion today, on topics that matter to us.\n", "But on the contrary, this very ease of information dissemination has also made social media platforms tools for spreading falsehood, popularly called as fake news. It hardly takes a few hours for propaganda drivers to put something online and get it circulated, leading to conflicts and defamations. So, it s quite pertinent to build sophisticated fake news detection algorithms, that could flag online content spreading misinformation, with adequate reliability.\n", "Well, in this tutorial we shall build a powerful Fake News Detection Model, using the pre-trained BERT, with the help of Transfer Learning.\n", "Well, this article is actually the third & last instalment of my three part learning series, where we are:\n", "Now, let s continue further on this third part, where we build a sophisticated fake news detection model.\n", "If you are more of a video person, go ahead and watch it on YouTube, instead. Make sure to subscribe to my channel to get access to all of my latest content.\n", "This is the snapshot of the dataset we are using. Here s the source for this dataset.\n", "We have two separate .csv files, one having the real news, called true.csv and another one, having the fake news, called fake.csv. Both files have the exact same data form.\n", "We have the title of the news, the entire news article text, the subject, which is basically the category of news and the date on which it was published. For our use case, we shall merge these files into a single large dataset, and add a new column  Label , that will have  true  mentioned against all observations from the true.csv and  fake  mentioned against the observations from fake.csv\n", "Moving on, this is our step-by-step plan on building this project..\n", "BERT is a big neural network architecture, with a huge number of parameters, that can range from 100 million to over 300 million. And, training a BERT model from scratch on a small dataset would result in overfitting. So, it is better to use a pre-trained BERT model that was trained on a huge dataset, as a starting point. We can then further train the model on our relatively smaller dataset and this process is known as model fine-tuning. To do this, there are these three approaches:\n", "In this tutorial, we will use the third approach. We will freeze all the layers of BERT during fine-tuning and append a dense layer and a softmax layer to the architecture.\n", "Now, let s get started with our Fake New Detection Model building using Python. This is our project folder on Google Drive, having all the project related files in one place. I ll share a link to this in the description part below. Here, b2_FakeNewsDetection is our Jupyter notebook. Let s fire it up, to do a quick code walkthrough.\n", "By the way, to proceed with this tutorial, a Jupyter Notebook environment with a GPU is recommended. The same can be accessed through Google Colaboratory which provides a cloud-based Jupyter Notebook environment with a free GPU. For this tutorial, we shall be working on Colab. Once you are on Colab, activate the GPU runtime by clicking on Runtime -> Change runtime type -> Select GPU.\n", "Alright, now let s get coding. As first step, let s set up our working environment.\n", "Here, we install Huggingface s transformers library, which allows us to import a wide range of transformer-based pre-trained models. Additionally, we are installing pycaret. We also set up our working directory.\n", "Next up, let s load the dataset.\n", "Here, first up we load true and fake csv files as pandas dataframe. Then, we create a column  Target , where we put the labels as True / Fake. Finally, we merge the two dataframes into one data, by random mixing.\n", "Next up, the target column has string values, which a computer won t understand. So, we need to transform them into numeric form. To do this, we use Pandas get_dummies to create a new column called label, where we put all Fake labels as 1 and True as 0. Towards the end, to check if our data is balanced across the two labels, we may plot a pie chart. As you would see, our data is fairly well balanced.\n", "Next up, we split up our data into training validation and test set, in 70:15:15 ratio.\n", "Now we come to the BERT fine-tuning stage, where we shall perform transfer learning.\n", "This is what we are doing here:\n", "With this understanding, now let s go ahead to tokenize our sequences, that is titles in our training, test and validation sets.\n", "We also convert the integer sequences to tensors. And finally, we define data loaders for both train and validation set. These data loaders will pass batches of train data and validation data as input to the model during the training phase.\n", "Moving on, we freeze pre-trained model weights. If you can recall, earlier I mentioned in this tutorial, that we would freeze all the layers of the model before fine-tuning it. So, let s do it now. This will prevent updating of model weights during fine-tuning.\n", "If you wish to fine-tune even the pre-trained weights of the BERT model then you may not execute this code.\n", "Moving on, we define our model architecture.\n", "We are using PyTorch for defining, training, & evaluating our deep learning model. Post our BERT network, we are adding dense layers 1 & 2 followed by softmax activation. Then, we define our hyperparameters; we are using AdamW as our optimizer.\n", "Then we define our loss function. And lastly, we are keeping number of epochs to 2. With Colab s free GPU, one epoch might take upto 20 mins. So, I m taking this low numbers to not keep waiting on forever. Haha!\n", "So, just to summarise:\n", "Now, we need to define functions to train (or fine-tune) and evaluate our fake news detection model. Let s do it:\n", "And finally, now we can start fine-tuning our BERT Model to learn fake news detection:\n", "Now let s build a classification report on the test set using our fake news model:\n", "As you would see, we are getting a strong 88% accuracy.\n", "Both precision & recall for class 1 are quite high which means that the model predicts this class pretty well. If you look at the recall for class 1, it is 0.85 which means that the model was able to correctly classify 85% of the fake news as fake. Precision is 0.92, which means that 92% of the fake news classifications by the model, are actually fake news.\n", "Let s also run predictions on these sample news titles. First two are fake and the next two are real.\n", "Quite rightly, our model classifies all four of these reviews correctly.\n", "Guys, congratulations to you for making it to this point. Do give yourself a pat on the back for completing this Transfer Learning Fake News Detection Project all by yourself.  \n", "To summarize, in this tutorial we fine-tuned a pre-trained BERT model to perform text classification on a small dataset. I urge you to fine-tune BERT on a different dataset and see how it performs. For example, you may do a sentiment classification or a spam detection model. NLP use cases are endless, really.\n", "You can even perform multiclass or multi-label classification with the help of BERT. In addition to that, you can even train the entire BERT architecture as well if you have a bigger dataset.\n", "In case you have any doubts or got stuck somewhere, leave a comment below, and I ll help you out.\n", "Guys, if you need further guidance on building a career in data science or any help related to this vast domain, you may go to my website www.skillcate.com and set up a free 1:1 mentoring session with me, by filling out this small form.\n", "You may write to me over email and WhatsApp.\n", "Good luck to you, bye!!\n"]}, {"link": "https://medium.com/@pranik-chainani/transformers-in-nlp-de1db51ef08?source=list-2c27d980d3f3--------33-------338c7da11cbf---------------------", "title": "Transformers in NLP", "subtitle": "false", "autorName": "Pranik Chainani", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/0*ir3Rers95UsDkR8P.jpg", "clap": "69", "response": "7", "timeForRead": "7 min read", "dateCreate": "Dec 19, 2021", "text": ["First introduced in the renowned Attention is All You Need by Vaswani et al, Transformers have become the state-of-the art for many tasks in natural language processing and sequential models as a whole. In fact, there have also been recent experiments that have shown Transformers to generalize well to even computer vision tasks (consider An Image is Worth 16x16 words). As such, it is important to explore attention-based models as a robust framework in detail, given how well they can extend to numerous domains in Machine Learning.\n", "To start with, as starkly proposed in  Attention is All You Need,  Transformers are grounded in attention mechanisms. That is, attention, best put by Vaswani et al, is described as   an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences. \n", "In other words, attention mechanisms have the ability to generate concrete relationships between data points within sequences.\n", "In fact, Transformers use a specific type of attention mechanism, referred to as multi-head attention. However, to understand what this type of attention mechanism is, we must first introduce a simpler scaled dot-product attention scheme.\n", "Let us start with scaled dot-product attention. Simply, we can express this form of attention as:\n", "wherein which Q, K, and V are batches of matrices, each with shape (batch_size, length_of_sequence, num_of_features). We further observe that the inner product between the query Q and the key K results in a matrix of size (batch_size, length_of_sequence, length_of_sequence). We can, thus, interpret this new matrix as telling us roughly how important each element in the given sequence is. As such, we identify this multiplication as the core attention of the current layer, as it essentially determines which elements we  pay attention  to.\n", "This attention matrix is then normalized by the softmax nonlinearity, so that all the weights sum to one. Finally, we simply apply the value V to our attention matrix to observe our desired output.\n", "We can observe how simple it is to implement this form of attention below:\n", "Now that we have a decent idea of how scaled dot-product attention, we simply incorporate this dot-product attention scheme as shown in the diagram below to construct our multi-head attention layer.\n", "Namely, we observe that the multi-head attention is composed of several identical attention heads, where each so-called attention head contains 3 linear layers, followed by the scaled dot-product attention we know. We can simply implement this using a class structure as follows:\n", "Thus, to recap, we observe that each attention head in our multi-head attention scheme computes its own query, key, and value matrices, and then simply applies the scaled dot-product attention.\n", "We can interpret this intuitively as each head can attend to a different part of the given input sequence, independent of the others. Thus, if we increase the number of attention heads, we are able to  pay attention  to more parts of the given input sequence at once, which makes our model even more robust.\n", "Interestingly, it is important to note that our multi-head attention framework really has no trainable components that operates over the sequence in_dim. In fact, everything instead operates over the feature k-dim, and is thus independent of sequence length. As such, we must then provide positional information to our model, so as to ensure that our model knows about the relative position of our data points in the given input sequence.\n", "The way to go about this is as follows:\n", "We see that the usage of seemingly unusual sinusoidal encodings in turn allows for us to better extrapolate to longer sequence lengths. This is because the trigonometric position encodings are periodic, with a range of [0, 1], and thus behave nicely. We can observe this by supposing that, during model inference, we provide an input sequence longer than any used during training. By doing so, the positional encoding for the last elements in that given sequence might be different than anything the model as encountered during training. As such, the sinusoidal positional embeddings then allow for the learned model to extrapolate smoothly to sequence lengths longer than the ones seen before.\n", "Now, we can move on to construct our Transformer model. We start by observing a diagram of the full scheme:\n", "Upon first glance, we see that the transformer uses an encoder-decoder model architecture. The encoder (left) thus processes a given input sequence and returns a feature/latent vector. In turn, the decoder (right) then processes the target sequence, and incorporates information learned from the encoder memory. The output then from our decoder model is our model s prediction.\n", "We will first start by writing up our encoder layer before we move on to the decoder.\n", "Above, we implemented a simple feed forward network and a residual block that we will utilize in our Transformer model (consider reading more on residual blocks from ResNet for more background).\n", "Now to create our encoder, we simply incorporate these utility methods above as follows (following the diagram we introduce above):\n", "The decoder class follows in a similar manner. It is important to note, however, that the decoder accepts two arguments (target and memory/from encoder). Furthermore, the scheme introduce by Vaswani uses two multi-head attention modules per layer, instead of one.\n", "Observe our implementation below:\n", "Finally, we combine everything into a single Transformer class as follows:\n", "To conclude with, we have demonstrated a simply, intuitive explanation that sheds light on a powerful framework of neural networks known as Transformers. Particularly in NLP, transformers do not rely on past hidden states to capture dependencies with previous words, as they are able to process a sentence as a whole, with no risk of loosing (or  forgetting ) past information, as is the case with many RNN models. Moreover, since we incorporate multi-head attention scheme and positional embeddings, we are able to provide information about the intrinsic relationships between different words that aren t easily captured in standard recurrent or markov-based models.\n"]}, {"link": "https://medium.com/@antoine.louis/a-brief-history-of-natural-language-processing-part-2-f5e575e8e37?source=list-a0aae78aa81b--------34-------5fb2bbebc495---------------------", "title": "A Brief History of Natural Language Processing   Part 2", "subtitle": "false", "autorName": "Antoine Louis", "imageAutor": "https://miro.medium.com/v2/resize:fill:88:88/1*4Z6gATiHqzYJEOAfIEPPiA@2x.jpeg", "clap": "98", "response": "1", "timeForRead": "6 min read", "dateCreate": "Jul 7, 2020", "text": ["Natural language processing (NLP) is a theoretically motivated range of computational techniques for analyzing and representing naturally occurring texts at one or more levels of linguistic analysis (Liddy, 2001). The purpose of these techniques is to achieve human-like language processing for a range of tasks or applications. Although it has gained enormous interest in recent years, research in NLP has been going on for several decades dating back to the late 1940s. This review divides its history into two main periods: NLP before (part 1) and during (part 2) the deep learning era.\n", "(If you missed part 1, check NLP before the Deep Learning Era.)\n", "Starting in the 2000s, neural networks begin to be used for language modeling, a task which aims at predicting the next word in a text given the previous words. In 2003, Bengio et al. proposed the first neural language model, that consists of a one-hidden layer feed-forward neural network. They were also one of the first to introduce what is now referred as word embedding, a real-valued word feature vector in R^d. More precisely, their model took as input vector representations of the n previous words, which were looked up in a table learned together with the model. The vectors were fed into a hidden layer, whose output was then provided to a softmax layer that predicted the next word of the sequence. Although classic feed-forward neural networks have been progressively replaced with recurrent neural networks (Mikolov et al., 2010) and long short-term memory networks (Graves, 2013) for language modeling, they remain in some settings competitive with recurrent architectures, the latter being impacted by  catas- trophic forgetting  (Daniluk et al., 2017). Furthermore, the general building blocks of Bengio et al. s network are still found in most neural language and word embedding models nowadays.\n", "In 2008, Collobert and Weston applied multi-task learning, a sub-field of machine learning in which multiple learning tasks are solved at the same time, to neural networks for NLP. They used a single convolutional neural network architecture (CNN; LeCun et al., 1999) that, given a sentence, was able to output many language processing predictions such as part-of-speech tags, named entity tags and semantic roles. The entire network was trained jointly on all the tasks using weight-sharing of the look-up tables, which enabled the different models to collaborate and share general low-level information in the word embedding matrix. As models are being increasingly evaluated on multiple tasks to gauge their generalization ability, multi-task learning has gained in importance and is now used across a wide range of NLP tasks. Also, their paper turned out to be a discovery that went beyond multi-task learning. It spearheaded ideas such as pre-training word embeddings and using CNNs for text, that have only been widely adopted in the last years.\n", "In 2013, Mikolov et al. introduced arguably the most popular word embedding model: Word2Vec. Although dense vector representations of words have been used as early as 2003 (Bengio et al.), the main innovation proposed in their paper was an efficient improvement of the training procedure, by removing the hidden layer and approximating the objective. Together with the efficient model implementation, these simple changes enabled large-scale training of word embeddings on huge corpora of unstructured text. Later that year, they improved the Word2Vec model by employing additional strategies to enhance training speed and accuracy. While these embeddings are not different conceptually than the ones learned with a feed-forward neural network, training on a very large corpus enables them to capture certain relationships between words such as gender, verb tense, and country-capital relations, which initiated a lot of interest in word embeddings as well as in the origin of these linear relationships (Mimno and Thompson, 2017; Arora et al., 2018; Antoniak and Mimno, 2018; Wendlandt et al., 2018). But what made word embeddings a mainstay in current NLP was the evidence that using pre-trained embeddings as initialization improved performance across a wide range of downstream tasks. Since then, a lot of work has gone into exploring different facets of word embeddings (as indicated by the staggering number of citations of the original paper, i.e. 19,071 citations at the time of writing). Despite many more recent developments, Word2Vec is still a popular choice and widely used today.\n", "The year 2013 also marked the adoption of neural network models in NLP, in particular three well-defined types of neural networks: recurrent neural networks (RNNs; Elman, 1990), convolutional neural networks (CNNs), and recursive neural networks (Socher et al., 2013). Because of their architecture, RNNs became popular for dealing with the dynamic input sequences ubiquitous in NLP. But Vanilla RNNs were quickly replaced with the classic long-short term memory networks (LSTMs; Hochreiter and Schmidhuber, 1997), as they proved to be more resilient to the vanishing and exploding gradient problem. At the same time, convolutional neural networks, that were then beginning to be widely adopted by the computer vision community, started to get applied to natural language (Kalchbrenner et al., 2014; Kim, 2014). The advantage of using CNNs for dealing with text sequences is that they are more parallelizable than RNNs, as the state at every time step only depends on the local context (via the convolution operation) rather than all past states as in the RNNs. Finally, recursive neural networks were inspired by the principle that human language is inherently hierarchical: words are composed into higher-order sentences, which can themselves be recursively combined according to a set of production rules. Based on this linguistic perspective, recursive neural networks treated sentences as trees rather than as a sequences. Some research (Tai et al., 2015) also extended RNNs and LSTMs to work with hierarchical structures.\n", "In 2014, Sutskever et al. proposed sequence-to-sequence learning, a general end-to-end approach for mapping one sequence to another using a neural network. In their method, an encoder neural network processes a sentence symbol by symbol, and compresses it into a vector representation. Then, a decoder neural network predicts the output sequence symbol by symbol based on the encoder state and the previously predicted symbols that are taken as input at every step. Encoders and decoders for sequences are typically based on RNNs, but other architectures have also emerged. Recent models include deep-LSTMs (Wu et al., 2016), convolutional encoders (Kalchbrenner et al., 2016; Gehring et al., 2017), the Transformer (Vaswani et al., 2017), and a combination of an LSTM and a Transformer (Chen et al., 2018). Machine translation turned out to be the perfect application for sequence-to-sequence learning. The progress was so significant that Google announced in 2016 that it was officially replacing its monolithic phrase-based machine translation models in Google Translate with a neural sequence-to-sequence model.\n", "In 2015, Bahdanau et al. introduced the principle of attention, which is one of the core innovations in neural machine translation (NMT) and the key idea that enabled NMT models to outperform classic sentence-based MT systems. It basically alleviates the main bottleneck of sequence-to-sequence learning, which is its requirement to compress the entire content of the source sequence into a fixed-size vector. Indeed, attention allows the decoder to look back at the source sequence hidden states, that are then combined through a weighted average and provided as additional input to the decoder. Attention is potentially useful for any task that requires making decisions based on certain parts of the input. For now, it has been applied to constituency parsing (Vinyals et al., 2015), reading comprehension (Hermann et al., 2015), and one-shot learning (Vinyals et al., 2016). More recently, a new form of attention has appeared, called self-attention, being at the core of the Transformer architecture. In short, it is used to look at the surrounding words in a sentence or paragraph to obtain more contextually sensitive word representations.\n", "The latest major innovation in the world of NLP is undoubtedly large pretrained language models. While first proposed in 2015 (Dai and Le), only recently were they shown to give a large improvement over the state-of-the-art methods across a diverse range of tasks. Pre-trained language model embeddings can be used as features in a target model (Peters et al., 2018), or a pre-trained language model can be fine-tuned on target task data (Devlin et al., 2018; Howard and Ruder, 2018; Radford et al., 2019; Yang et al., 2019), which have shown to enable efficient learning with significantly less data. The main advantage of these pre-trained language models comes from their ability to learn word representations from large unannotated text corpora, which is particularly beneficial for low-resource languages where labelled data is scarce.\n"]}, {"link": "https://medium.com/@fareedkhandev/exciting-news-claude-ai-is-now-available-in-95-countries-90047ebf1606?source=list-2eb23a991a63--------24-------0a856388a93a---------------------", "title": "Exciting News   Claude.ai is Now Available in 95 Countries!", "subtitle": "false", "autorName": "Fareed Khan", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*ujdMB17AE56yPSA3zeZcNA.jpeg", "clap": "53", "response": "3", "timeForRead": "5 min read", "dateCreate": "Oct 17", "text": ["The world of artificial intelligence is evolving at a pace that can sometimes feel like science fiction come to life. From AI-powered chatbots to robotic helpers, technology is progressing faster than ever. In the midst of this AI revolution, there s one company that s taking a unique approach to ensure that the future remains safe and sound. That company is Anthropic, and they ve just made a groundbreaking move: Claude.ai, their AI chatbot, is now available in a whopping 95 countries.\n", "Check out the availability here: Claude.ai in 95 Countries\n", "Anthropic s CEO, Dario Amodei, has been vocal about the potential risks of AI becoming too autonomous, especially as it gains the ability to access the internet and control robots. And he s not alone in this concern. Many other AI leaders and scientists share his view. To address these concerns, Anthropic has taken an unusual approach by developing their own Large Language Model (LLM). Their latest iteration, Claude 2, is already being hailed as a potential  ChatGPT killer. \n", "But what makes Claude.ai and Claude 2 so special? Let s dive into it.\n", "Claude.ai is an AI chatbot powered by Anthropic s LLM, Claude 2. If you ve ever used ChatGPT or Google Bard, you ll feel right at home with Claude. It s a powerful and flexible chatbot that collaborates with you, writes for you, and answers your questions.\n", "Anthropic, the brains behind Claude, was founded in 2021 by a group of former OpenAI employees who were instrumental in developing GPT-2 and GPT-3. Their primary focus? AI research with an unwavering commitment to safety.\n", "After a successful closed alpha phase with select commercial partners in early 2023, Claude was integrated into products like Notion AI, Quora s Poe, and DuckDuckGo s DuckAssist. In March 2023, Claude opened up its API to a broader range of businesses and finally released its chatbot to the public in July 2023, alongside the launch of Claude 2.\n", "If you re curious about Claude and want to give it a spin, you re in luck. The Claude chatbot, powered by the latest Claude 2 model, is currently available through an open beta in the U.S. and U.K. Anthropic has plans to expand access globally in the future. To get started, simply sign up at Claude.ai. You can initiate a conversation or use one of Claude s default prompts to begin your journey.\n", "And if you re eager for more, Anthropic recently introduced Claude Pro, which offers high-traffic access and access to upcoming features.\n", "Now, you might wonder what sets Claude apart from other AI models. All AI models have their strengths and limitations, and Claude is no exception. One significant concern with AI is bias and inaccuracy, and hallucinations often occur when an AI doesn t know the answer. Claude s mission is to be  helpful, harmless, and honest. \n", "While most AI companies rely on human contractors to fine-tune their models, Anthropic took a different path. In addition to human fine-tuning, they developed a second AI model known as Constitutional AI. This model incorporates rules inspired by the United Nations  Declaration of Human Rights and Apple s terms of service. It ensures Claude s behavior aligns with values that prioritize safety and ethical conduct. These rules are easy to understand and adjust, allowing for transparency and adaptability.\n", "Anthropic takes red teaming to a whole new level. They intentionally provoke Claude to respond in ways that breach its benevolent guardrails. This process helps identify areas for safety improvements. Additionally, Anthropic collaborates with the Alignment Research Center (ARC) for third-party safety assessments, ensuring Claude s safety is rigorously evaluated.\n", "Unlike many AI companies, Anthropic operates as a public benefit corporation. This means that their decisions aren t solely driven by financial gains. While they do partner with big names like Google and Zoom and aim to secure investments, their unique structure enables them to prioritize safety over profits.\n", "One of the most compelling aspects of Claude 2 is its ability to handle up to 100K tokens per prompt. This is equivalent to about 75,000 words, which is twelve times more than GPT-4. Claude 2 performs admirably on standardized tests, though it excels in creative writing while lagging behind in coding and quantitative reasoning. It s also noteworthy that Claude 2 s knowledge extends up to early 2023, surpassing GPT-4 s September 2021 cutoff.\n", "To truly appreciate Claude s capabilities, I decided to put it to the test. I gave it various tasks and compared its performance with other chatbots.\n", "In a test to practice Spanish, Claude, ChatGPT, Llama 2, and Bard each had their moments, but ChatGPT emerged as the victor.\n", "For generating ideas for a dystopian young adult novel, Claude, ChatGPT, and Llama 2 performed similarly. Bard, however, missed the mark entirely.\n", "But where Claude truly shines is in its 100K context window. Although it declined my request to write a 30,000-word novel based on a plot outline, when I accessed the Claude 2 model through Poe, it effortlessly generated the first five chapters of a compelling young adult novel. This was a testament to its creative writing prowess.\n", "Anthropic s unique approach to AI safety doesn t end with Claude s development. They firmly believe that to advocate for AI safety, they need to compete commercially. This influences competitors to prioritize safety and accountability. While it s too early to assess the full impact of Claude s release on the AI industry, Anthropic s leaders were invited to brief the U.S. president and are actively cooperating with organizations like the U.K. s AI Safety Taskforce.\n", "In a surprising twist, a group of researchers who feared the existential threat of AI decided to take matters into their own hands and create a powerful AI model. Thus far, Anthropic s approach appears to be a promising step forward for AI safety.\n", "With Claude.ai now accessible in 95 countries, it s exciting to see how this unique endeavor will shape the future of AI and ensure its responsible and ethical use. Anthropic s dedication to safety is a breath of fresh air in the ever-evolving world of artificial intelligence.\n"]}, {"link": "https://medium.com/@davidsweenor/synthetic-data-and-surveys-64d95fcd429?source=list-e28f6edecf84--------32-------7b153c9756d3---------------------", "title": "Synthetic Data and Surveys", "subtitle": "Exploring ChatGPT s advanced data-analysis capabilities", "autorName": "David Sweenor", "imageAutor": "https://miro.medium.com/v2/resize:fill:44:44/1*vw4eCCf1HQOPrYXqmaOFow.png", "clap": "53", "response": "6", "timeForRead": "10 min read", "dateCreate": "Oct 14", "text": ["Recently, a colleague of mine asked if I d be interested in analyzing survey data that was recently collected. I must say, there was a little trepidation since I haven t analyzed raw survey data in quite some time. For the past several years, survey tools like Qualtrics, SurveyMonkey, Pollfish, and Alchemer have embedded analytics and visualizations directly into their web apps, making it easy for anyone to analyze survey data at lightning speed. However, these options were not available for this request.\n", "The other wrinkle in this little story is that my Python skills are quite rusty, and I didn t have an analytics software platform available. So, what were my options? As I pondered this, I downloaded KNIME and thought it would be an excellent opportunity to explore ChatGPT s Advanced Data Analysis capability. I will share my KNIME experience in a separate post and focus this discussion on ChatGPT.\n", "Quite simply, it s artificially fabricated data usually with machine learning (ML) technologies. Synthetic data can be quite complex it can capture complex mathematical relationships between different variables. The synthetic data vault describes synthetic data:  Although the synthetic data is entirely machine generated, it maintains the original format and mathematical properties. This makes synthetic data versatile. It can completely replace the existing data in a workflow, or it can supplement the data to enhance its utility. [1]\n", "Synthetic data can undoubtedly be structured data (i.e. numbers) but can also be text, images, or other formats. In fact, images were one of the first types of synthetic data to take off. Many builders of computer vision applications utilize synthetic data to create different colors, shadows, angles, and other properties to train the algorithm to recognize objects under various conditions and perspectives. It s also relied on quite heavily to train autonomous vehicles.\n", "The need for synthetic data continues to grow with all of the data privacy regulations and restrictions. In fact, the analyst firm Gartner predicts that by \n"]}]